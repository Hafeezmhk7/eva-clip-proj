#!/usr/bin/env python3
"""
UPDATED Multi-GPU Training Script for BLIP3-o DiT with Dual Supervision
Key Features:
1. Dual supervision architecture (patch + global)
2. Custom MLP layers for domain adaptation
3. Frozen CLIP visual projection
4. Enhanced recall performance training
"""

import os
import sys
import argparse
import logging
import torch
import torch.distributed as dist
from pathlib import Path
import json
from datetime import datetime
import traceback

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

def setup_logging():
    """Setup logging for all ranks"""
    local_rank = int(os.environ.get("LOCAL_RANK", 0))
    
    if local_rank == 0:
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
    else:
        logging.basicConfig(level=logging.WARNING)
    
    return logging.getLogger(__name__)

def parse_arguments():
    """Parse command line arguments for dual supervision multi-GPU training."""
    parser = argparse.ArgumentParser(
        description="Dual Supervision Multi-GPU BLIP3-o DiT training",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # Data configuration
    parser.add_argument(
        "--chunked_embeddings_dir", type=str, required=True,
        help="Path to directory containing chunked embedding files"
    )
    
    # Output configuration
    parser.add_argument(
        "--output_dir", type=str, required=True,
        help="Output directory for checkpoints"
    )
    
    # Model configuration
    model_group = parser.add_argument_group("Model Configuration")
    model_group.add_argument("--model_dim", type=int, default=768,
                           help="Model hidden dimension")
    model_group.add_argument("--num_layers", type=int, default=16,
                           help="Number of transformer layers")
    model_group.add_argument("--num_heads", type=int, default=12,
                           help="Number of attention heads")
    
    # NEW: MLP configuration for dual supervision
    mlp_group = parser.add_argument_group("MLP Configuration")
    mlp_group.add_argument("--mlp_hidden_dim", type=int, default=2048,
                          help="Hidden dimension for adaptation MLP")
    mlp_group.add_argument("--mlp_num_layers", type=int, default=3,
                          help="Number of layers in adaptation MLP")
    mlp_group.add_argument("--mlp_dropout", type=float, default=0.1,
                          help="Dropout rate for adaptation MLP")
    
    # Training configuration
    train_group = parser.add_argument_group("Training Configuration")
    train_group.add_argument("--num_epochs", type=int, default=8,
                           help="Number of training epochs")
    train_group.add_argument("--batch_size", type=int, default=6,  # Smaller for dual supervision
                           help="Training batch size per GPU")
    train_group.add_argument("--eval_batch_size", type=int, default=4,
                           help="Evaluation batch size per GPU")
    train_group.add_argument("--learning_rate", type=float, default=5e-5,  # Lower for stability
                           help="Learning rate")
    train_group.add_argument("--weight_decay", type=float, default=0.01,
                           help="Weight decay")
    train_group.add_argument("--warmup_steps", type=int, default=100,
                           help="Number of warmup steps")
    train_group.add_argument("--gradient_accumulation_steps", type=int, default=6,  # Higher for smaller batches
                           help="Gradient accumulation steps")
    train_group.add_argument("--lr_scheduler_type", type=str, default="cosine",
                        choices=["linear", "cosine", "constant", "cosine_with_restarts"],
                        help="Learning rate scheduler type")
    train_group.add_argument("--warmup_ratio", type=float, default=0.05,
                        help="Warmup steps as ratio of total steps")
    
    # NEW: Dual supervision loss weights
    loss_group = parser.add_argument_group("Loss Configuration")
    loss_group.add_argument("--patch_loss_weight", type=float, default=1.0,
                          help="Weight for patch reconstruction loss")
    loss_group.add_argument("--global_loss_weight", type=float, default=2.0,  # Higher for retrieval
                          help="Weight for global alignment loss")
    loss_group.add_argument("--flow_matching_loss_weight", type=float, default=1.0,
                          help="Weight for flow matching loss")
    loss_group.add_argument("--use_cosine_similarity", action="store_true",
                          help="Use cosine similarity instead of MSE for losses")
    
    # Hardware configuration
    hw_group = parser.add_argument_group("Hardware Configuration")
    hw_group.add_argument("--fp16", action="store_true",
                        help="Use mixed precision training (fp16)")
    hw_group.add_argument("--dataloader_num_workers", type=int, default=4,
                        help="Number of dataloader workers per GPU")
    
    # CLIP model configuration
    clip_group = parser.add_argument_group("CLIP Configuration")
    clip_group.add_argument("--clip_model_name", type=str, default="openai/clip-vit-large-patch14",
                          help="CLIP model name for frozen projection")
    
    return parser.parse_args()

def setup_ddp_environment():
    """Setup DDP environment variables for better performance."""
    os.environ.setdefault("NCCL_ASYNC_ERROR_HANDLING", "1")
    os.environ.setdefault("TORCH_DISTRIBUTED_DEBUG", "OFF")
    os.environ.setdefault("PYTORCH_CUDA_ALLOC_CONF", "max_split_size_mb:128")

def main():
    """Main dual supervision training function"""
    logger = setup_logging()
    
    # Setup DDP environment
    setup_ddp_environment()
    
    # Get distributed info
    local_rank = int(os.environ.get("LOCAL_RANK", 0))
    global_rank = int(os.environ.get("RANK", 0))
    world_size = int(os.environ.get("WORLD_SIZE", 1))
    
    # Only print from rank 0
    if local_rank == 0:
        print("üöÄ DUAL SUPERVISION Multi-GPU BLIP3-o DiT Training")
        print("=" * 60)
        print("üéØ NEW ARCHITECTURE:")
        print("   EVA [B,256,4096] ‚Üí DiT ‚Üí [B,256,1024] ‚Üí {")
        print("     Patch Output: [B,256,1024] (patch loss)")
        print("     Global Path: Avg Pool ‚Üí MLP ‚Üí Frozen CLIP Proj ‚Üí [B,768]")
        print("   }")
        print("üîó DUAL LOSS:")
        print("   L1: MSE(dit_patches, clip_patches) - patch fidelity")
        print("   L2: MSE(dit_global, clip_global) - retrieval capability")
        print("   L3: Flow Matching Loss - velocity prediction")
        print("=" * 60)
        print(f"World size: {world_size}")
        print(f"Local rank: {local_rank}")
        print(f"Global rank: {global_rank}")
    
    # Parse arguments
    args = parse_arguments()
    
    if local_rank == 0:
        print(f"üìä Dual Supervision Training Configuration:")
        print(f"   Total GPUs: {world_size}")
        print(f"   Batch size per GPU: {args.batch_size}")
        print(f"   Total effective batch: {args.batch_size * world_size * args.gradient_accumulation_steps}")
        print(f"   Model dimension: {args.model_dim}")
        print(f"   MLP hidden dim: {args.mlp_hidden_dim}")
        print(f"   MLP layers: {args.mlp_num_layers}")
        print(f"   Loss weights: Patch={args.patch_loss_weight}, Global={args.global_loss_weight}, Flow={args.flow_matching_loss_weight}")
        print(f"   Use cosine similarity: {args.use_cosine_similarity}")
        print(f"   CLIP model: {args.clip_model_name}")
    
    try:
        # Import modules
        from src.modules.config.blip3o_config import BLIP3oDiTConfig
        from src.modules.models.blip3o_dit import create_blip3o_dit_model
        from src.modules.losses.flow_matching_loss import create_dual_supervision_loss
        from src.modules.trainers.blip3o_trainer import BLIP3oTrainer, create_blip3o_training_args
        from src.modules.datasets.blip3o_dataset import create_chunked_dataloaders
        
        # Load manifest
        manifest_path = Path(args.chunked_embeddings_dir) / "embeddings_manifest.json"
        with open(manifest_path, 'r') as f:
            manifest = json.load(f)
        
        if local_rank == 0:
            print(f"üìä Dataset info:")
            print(f"   Total shards: {manifest['total_shards']}")
            print(f"   Total samples: {manifest['total_samples']:,}")
            print(f"   Samples per GPU: {manifest['total_samples'] // world_size:,}")
        
        # Create model config for dual supervision
        model_config = BLIP3oDiTConfig(
            input_size=16,  # 16x16 = 256 tokens
            patch_size=1,
            in_channels=1024,  # CLIP dimension
            dim=args.model_dim,
            eva_embedding_size=4096,  # EVA-CLIP dimension
            n_layers=args.num_layers,
            n_heads=args.num_heads,
            norm_eps=1e-5,
            qk_norm=True,
            learn_sigma=False,
            _gradient_checkpointing=True,
            # NEW: MLP configuration
            mlp_hidden_dim=args.mlp_hidden_dim,
            mlp_num_layers=args.mlp_num_layers,
            mlp_dropout=args.mlp_dropout,
        )
        
        if local_rank == 0:
            print(f"üèóÔ∏è  Creating dual supervision model...")
        
        # Create model with dual supervision
        model = create_blip3o_dit_model(
            config=model_config,
            load_clip_projection=True,  # Load frozen CLIP projection
            clip_model_name=args.clip_model_name,
        )
        
        # Enable gradient checkpointing for memory efficiency
        if hasattr(model, 'enable_gradient_checkpointing'):
            model.enable_gradient_checkpointing()
        
        # Calculate parameters
        if local_rank == 0:
            total_params = sum(p.numel() for p in model.parameters())
            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            frozen_params = total_params - trainable_params
            
            print(f"üìä Model parameters:")
            print(f"   Total: {total_params:,}")
            print(f"   Trainable: {trainable_params:,}")
            print(f"   Frozen (CLIP): {frozen_params:,}")
            print(f"üíæ Memory per GPU: ~{trainable_params * 4 / (1024**3):.1f} GB")
        
        # Create dual supervision flow matching loss
        flow_matching_loss = create_dual_supervision_loss(
            patch_loss_weight=args.patch_loss_weight,
            global_loss_weight=args.global_loss_weight,
            flow_matching_loss_weight=args.flow_matching_loss_weight,
            use_cosine_similarity=args.use_cosine_similarity,
            clip_model_name=args.clip_model_name,
        )
        
        # Create dataloaders
        if local_rank == 0:
            print(f"üîÑ Creating dataloaders...")
        
        train_dataloader, eval_dataloader = create_chunked_dataloaders(
            chunked_embeddings_dir=args.chunked_embeddings_dir,
            batch_size=args.batch_size,
            eval_batch_size=args.eval_batch_size,
            eval_split_ratio=0.1,
            normalize_embeddings=True,
            delete_after_use=False,  # Keep files for multi-GPU access
            num_workers=args.dataloader_num_workers,
            pin_memory=True,
            drop_last=True,  # Important for DDP
        )
        
        # Safe dataloader checking
        has_eval_dataloader = eval_dataloader is not None
        
        if local_rank == 0:
            print(f"‚úÖ Training dataloader created")
            if has_eval_dataloader:
                print(f"‚úÖ Evaluation dataloader created")
            else:
                print("‚ö†Ô∏è  No evaluation dataloader")
        
        # Create dummy datasets for Trainer
        class LengthEstimateDataset:
            def __init__(self, estimated_samples: int):
                self.estimated_samples = estimated_samples
            
            def __len__(self):
                return self.estimated_samples
            
            def __getitem__(self, idx):
                raise NotImplementedError("Use custom dataloader")
        
        # Calculate estimated samples per GPU
        total_samples = manifest['total_samples']
        samples_per_gpu = total_samples // world_size
        eval_samples_per_gpu = int(samples_per_gpu * 0.1) if has_eval_dataloader else 0
        
        # Create dummy datasets
        train_dataset = LengthEstimateDataset(samples_per_gpu)
        eval_dataset = LengthEstimateDataset(eval_samples_per_gpu) if has_eval_dataloader else None
        
        # Calculate training steps
        steps_per_epoch = max(1, samples_per_gpu // args.batch_size)
        max_steps = (steps_per_epoch * args.num_epochs) // args.gradient_accumulation_steps
        
        if local_rank == 0:
            print(f"üìà Training schedule:")
            print(f"   Steps per epoch per GPU: {steps_per_epoch}")
            print(f"   Max steps: {max_steps}")
            print(f"   Total epochs: {args.num_epochs}")
            print(f"   Scheduler: {args.lr_scheduler_type}")
        
        # Create TrainingArguments for dual supervision
        training_args = create_blip3o_training_args(
            output_dir=args.output_dir,
            num_train_epochs=args.num_epochs,
            per_device_train_batch_size=args.batch_size,
            per_device_eval_batch_size=args.eval_batch_size,
            learning_rate=args.learning_rate,
            lr_scheduler_type=args.lr_scheduler_type,
            warmup_ratio=args.warmup_ratio,
            weight_decay=args.weight_decay,
            warmup_steps=args.warmup_steps,
            logging_steps=50,  # More frequent for dual supervision
            save_steps=max(100, max_steps // 5),
            eval_steps=max(50, max_steps // 10) if has_eval_dataloader else 0,
            gradient_accumulation_steps=args.gradient_accumulation_steps,
            fp16=args.fp16,
            dataloader_num_workers=args.dataloader_num_workers,
            remove_unused_columns=False,
            load_best_model_at_end=has_eval_dataloader,
            metric_for_best_model="eval_global_cosine_similarity",  # Focus on global alignment
            greater_is_better=True,
            
            # DDP settings
            ddp_find_unused_parameters=False,
            dataloader_pin_memory=True,
            save_on_each_node=False,
            local_rank=local_rank,
            
            # Memory optimizations
            dataloader_persistent_workers=True,
            save_total_limit=3,
            prediction_loss_only=False,
            
            # Disable features that can cause issues
            push_to_hub=False,
            report_to=[],  # Disable wandb for now
            
            # Additional DDP optimizations
            ddp_timeout=1800,
            ddp_backend="nccl",
        )
        
        if local_rank == 0:
            print("üîß Creating dual supervision trainer...")
            print(f"‚úÖ Loss weights: Patch={args.patch_loss_weight}, Global={args.global_loss_weight}")
            print(f"‚úÖ Cosine similarity: {args.use_cosine_similarity}")
            print(f"‚úÖ Frozen CLIP projection loaded")
        
        # Create trainer with dual supervision
        trainer = BLIP3oTrainer(
            model=model,
            args=training_args,
            flow_matching_loss=flow_matching_loss,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            clip_model_name=args.clip_model_name,
        )
        
        # Override dataloader methods to use our chunked dataloaders
        def get_train_dataloader_override():
            if local_rank == 0:
                logger.info("Using dual supervision train dataloader")
            return train_dataloader
        
        def get_eval_dataloader_override(eval_dataset=None):
            if has_eval_dataloader:
                if local_rank == 0:
                    logger.info("Using dual supervision eval dataloader")
                return eval_dataloader
            else:
                return None
        
        # Apply overrides
        trainer.get_train_dataloader = get_train_dataloader_override
        trainer.get_eval_dataloader = get_eval_dataloader_override
        
        if local_rank == 0:
            print("üöÄ Starting DUAL SUPERVISION multi-GPU training...")
            print("‚úÖ Architecture: DiT ‚Üí Avg Pool ‚Üí MLP ‚Üí Frozen CLIP Proj")
            print("‚úÖ Dual loss: Patch + Global + Flow Matching")
            print("‚úÖ Expected recall improvement: 0% ‚Üí 60%+")
            print("‚úÖ All model parameters properly handled by DDP")
        
        # Start training - Trainer handles all DDP automatically
        trainer.train()
        
        # Save final model (only on main process)
        if local_rank == 0:
            print("üíæ Saving final dual supervision model...")
            trainer.save_model()
            
            # Save additional dual supervision info
            final_info = {
                'architecture': 'dual_supervision_blip3o',
                'training_completed': True,
                'final_step': trainer.training_step_count,
                'model_components': {
                    'dit_backbone': True,
                    'global_adaptation_mlp': True,
                    'frozen_clip_projection': True,
                    'dual_outputs': True,
                },
                'loss_components': {
                    'patch_reconstruction': True,
                    'global_alignment': True,
                    'flow_matching': True,
                },
                'expected_improvements': {
                    'recall_performance': 'significant_improvement_expected',
                    'patch_fidelity': 'maintained',
                    'global_retrieval': 'enhanced',
                },
                'timestamp': datetime.now().isoformat(),
            }
            
            final_info_path = Path(args.output_dir) / "dual_supervision_completion.json"
            with open(final_info_path, 'w') as f:
                json.dump(final_info, f, indent=2)
            
            print("‚úÖ Dual supervision multi-GPU training completed successfully!")
            print(f"üìÅ Model saved to: {args.output_dir}")
            print("üéØ Next steps:")
            print("   1. Run evaluation script to test recall performance")
            print("   2. Compare with baseline recall results")
            print("   3. Expected improvement: 0% ‚Üí 60%+ recall")
        
        return 0
        
    except Exception as e:
        print(f"‚ùå Dual supervision training failed on rank {global_rank}: {e}")
        print("Full traceback:")
        traceback.print_exc()
        
        if "Expected to have finished reduction" in str(e):
            print("\nüîç DDP Debugging Information:")
            print("This indicates a DDP parameter usage issue.")
            print("The dual supervision model ensures all parameters are used.")
        
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)