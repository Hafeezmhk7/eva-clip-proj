#!/usr/bin/env python3
"""
FIXED COCO Embedding Extraction - Proper Alignment

This script fixes the critical alignment bug in the extraction loop that was causing
most image-caption pairs to be misaligned, leading to poor evaluation results.

CRITICAL FIX:
- Proper index handling in batch processing loop
- Explicit alignment verification for each sample
- Debug output to catch alignment issues early

Usage:
    python extract_coco_embeddings_FIXED.py --blip3o_model_path <path> --coco_root <path> [options]
"""

import sys
import argparse
import logging
from pathlib import Path
import json
import torch
import torch.nn.functional as F
import numpy as np
import pickle
from typing import Dict, List, Optional, Union, Tuple
from tqdm import tqdm
import datetime
import time

def setup_paths():
    """Setup paths for project structure"""
    script_dir = Path(__file__).parent
    project_root = script_dir.parent.parent if 'src' in str(script_dir) else script_dir.parent
    
    # Add import paths
    sys.path.insert(0, str(project_root))
    sys.path.insert(0, str(project_root / "src"))
    sys.path.insert(0, str(project_root / "src" / "modules"))
    sys.path.insert(0, str(project_root / "src" / "modules" / "utils"))
    sys.path.insert(0, str(project_root / "src" / "modules" / "evaluation"))
    
    return project_root

def setup_temp_manager():
    """Setup temp manager for structured directory management."""
    try:
        from src.modules.utils.temp_manager import setup_snellius_environment
        manager = setup_snellius_environment("blip3o_workspace")
        print("‚úÖ Temp manager initialized successfully")
        return manager
    except ImportError as e:
        print(f"‚ö†Ô∏è  Could not import temp_manager: {e}")
        print("   Falling back to simple directory creation")
        return None

def setup_logging(verbose: bool = False):
    """Setup logging configuration."""
    level = logging.INFO if verbose else logging.WARNING
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    return logging.getLogger(__name__)

def parse_arguments():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="FIXED COCO Embedding Extraction - Proper Alignment",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # Required arguments
    parser.add_argument(
        "--blip3o_model_path", type=str, required=True,
        help="Path to trained BLIP3-o DiT model directory"
    )
    parser.add_argument(
        "--coco_root", type=str, required=True,
        help="Path to MS-COCO dataset root directory"
    )
    
    # Optional arguments
    parser.add_argument(
        "--num_samples", type=int, default=1000,
        help="Number of image-caption pairs to extract"
    )
    parser.add_argument(
        "--batch_size", type=int, default=8,
        help="Batch size for processing"
    )
    parser.add_argument(
        "--device", type=str, default="auto",
        choices=["auto", "cuda", "cpu"],
        help="Device to use for computation"
    )
    parser.add_argument(
        "--random_seed", type=int, default=42,
        help="Random seed for reproducible sampling"
    )
    parser.add_argument(
        "--save_raw_embeddings", action="store_true",
        help="Also save raw embeddings before projection"
    )
    parser.add_argument(
        "--force_reextract", action="store_true",
        help="Force re-extraction even if embeddings already exist"
    )
    parser.add_argument(
        "--debug_alignment", action="store_true",
        help="Enable detailed alignment debugging"
    )
    parser.add_argument(
        "--verbose", "-v", action="store_true",
        help="Enable verbose logging"
    )
    
    return parser.parse_args()

def validate_paths(args):
    """Validate input paths."""
    # Check BLIP3-o model path
    blip3o_path = Path(args.blip3o_model_path)
    if not blip3o_path.exists():
        raise FileNotFoundError(f"BLIP3-o model path not found: {blip3o_path}")
    
    # Check COCO path
    coco_path = Path(args.coco_root)
    if not coco_path.exists():
        raise FileNotFoundError(f"COCO dataset path not found: {coco_path}")
    
    # Check for COCO structure
    images_dir = coco_path / "images" / "val2017"
    annotations_file = coco_path / "annotations" / "captions_val2017.json"
    
    if not images_dir.exists():
        raise FileNotFoundError(f"COCO images directory not found: {images_dir}")
    
    if not annotations_file.exists():
        raise FileNotFoundError(f"COCO annotations file not found: {annotations_file}")
    
    # Count available data
    num_images = len(list(images_dir.glob("*.jpg")))
    with open(annotations_file, 'r') as f:
        coco_data = json.load(f)
    num_captions = len(coco_data['annotations'])
    
    print(f"‚úÖ Paths validated:")
    print(f"   BLIP3-o model: {blip3o_path}")
    print(f"   COCO dataset: {coco_path}")
    print(f"   Available images: {num_images:,}")
    print(f"   Available captions: {num_captions:,}")

def verify_sample_alignment(image_id: int, caption: str, image_path: str, 
                          coco_annotations: Dict, sample_idx: int, debug: bool = False) -> bool:
    """Verify that a single sample is properly aligned."""
    # Check filename consistency
    expected_filename = f"{image_id:012d}.jpg"
    actual_filename = Path(image_path).name
    
    if actual_filename != expected_filename:
        if debug:
            print(f"‚ùå Sample {sample_idx}: filename mismatch")
            print(f"   Expected: {expected_filename}, Got: {actual_filename}")
        return False
    
    # Check caption consistency
    if image_id in coco_annotations:
        coco_captions = coco_annotations[image_id]
        if caption not in coco_captions:
            if debug:
                print(f"‚ùå Sample {sample_idx}: caption not found for image {image_id}")
                print(f"   Caption: '{caption[:50]}...'")
            return False
    else:
        if debug:
            print(f"‚ùå Sample {sample_idx}: image_id {image_id} not in COCO")
        return False
    
    if debug and sample_idx < 5:
        print(f"‚úÖ Sample {sample_idx}: ID={image_id}, caption='{caption[:40]}...'")
    
    return True

def load_coco_annotations(coco_root: str) -> Dict:
    """Load COCO annotations for alignment verification."""
    annotations_file = Path(coco_root) / "annotations" / "captions_val2017.json"
    
    with open(annotations_file, 'r') as f:
        coco_data = json.load(f)
    
    # Create image_id to captions mapping
    id_to_captions = {}
    for ann in coco_data['annotations']:
        image_id = ann['image_id']
        if image_id not in id_to_captions:
            id_to_captions[image_id] = []
        id_to_captions[image_id].append(ann['caption'])
    
    return id_to_captions

def extract_embeddings_FIXED(args, logger, temp_manager):
    """FIXED embedding extraction with proper alignment."""
    
    # Set random seed for reproducibility
    torch.manual_seed(args.random_seed)
    np.random.seed(args.random_seed)
    
    print("\nüöÄ FIXED COCO EMBEDDING EXTRACTION")
    print("=" * 50)
    print("üîß CRITICAL FIXES:")
    print("   ‚Ä¢ Fixed batch index alignment bug")
    print("   ‚Ä¢ Added per-sample alignment verification") 
    print("   ‚Ä¢ Explicit index handling in extraction loop")
    print("   ‚Ä¢ Debug output for alignment tracking")
    print("=" * 50)
    
    # Load COCO annotations for alignment verification
    if args.debug_alignment:
        print("üìã Loading COCO annotations for alignment verification...")
        coco_annotations = load_coco_annotations(args.coco_root)
        print(f"‚úÖ Loaded annotations for {len(coco_annotations)} images")
    else:
        coco_annotations = None
    
    # Setup output directory using temp manager
    if temp_manager:
        # Create structured embeddings directory
        embeddings_base_dir = temp_manager.get_embeddings_dir()
        coco_eval_dir = temp_manager.create_embeddings_subdirectory("coco_val_evaluation_FIXED")
        
        # Setup model cache
        temp_manager.setup_model_cache()
        
        print(f"‚úÖ Using temp manager structured storage:")
        print(f"   Base embeddings dir: {embeddings_base_dir}")
        print(f"   COCO eval dir: {coco_eval_dir}")
    else:
        # Fallback to simple directory creation
        coco_eval_dir = Path("./results/coco_embeddings_FIXED")
        coco_eval_dir.mkdir(parents=True, exist_ok=True)
        print(f"‚ö†Ô∏è  Using fallback directory: {coco_eval_dir}")
    
    # Generate output filename
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    output_name = f"coco_val_embeddings_FIXED_{args.num_samples}samples_{timestamp}"
    
    # Initialize evaluator
    logger.info("Initializing BLIP3-o evaluator...")
    from src.modules.evaluation.evaluator import BLIP3oEvaluator
    
    evaluator = BLIP3oEvaluator(
        blip3o_model_path=args.blip3o_model_path,
        device=args.device,
    )
    
    # Create COCO dataloader - CRITICAL: shuffle=False
    logger.info(f"Creating COCO dataloader for {args.num_samples} samples...")
    from src.modules.evaluation.coco_dataset import create_coco_dataloader
    
    dataloader = create_coco_dataloader(
        coco_root=args.coco_root,
        batch_size=args.batch_size,
        max_samples=args.num_samples,
        shuffle=False,  # üîß CRITICAL: No shuffling!
        num_workers=4,
    )
    
    print(f"\nüìä Dataloader Configuration:")
    print(f"   Batch size: {args.batch_size}")
    print(f"   Max samples: {args.num_samples}")
    print(f"   Shuffle: False")
    print(f"   Number of batches: {len(dataloader)}")
    
    # Storage for embeddings
    embeddings_data = {
        # Main embeddings (768-dim, projected and normalized)
        'clip_vision_embeddings': [],      # CLIP vision + visual projection [N, 768]
        'generated_clip_embeddings': [],   # EVA‚ÜíBLIP3o + visual projection [N, 768]
        'text_embeddings': [],             # CLIP text embeddings [N, 768]
        
        # Metadata
        'image_ids': [],                   # Image IDs
        'captions': [],                    # Corresponding captions (first caption per image)
        'image_paths': [],                 # Image file paths
        
        # Extraction info
        'metadata': {
            'extraction_date': str(datetime.datetime.now()),
            'num_samples': 0,
            'model_path': str(args.blip3o_model_path),
            'coco_root': str(args.coco_root),
            'embedding_dim': 768,
            'random_seed': args.random_seed,
            'shuffle_used': False,
            'alignment_verified': True,  # Will be updated
            'device': str(evaluator.device),
            'batch_size': args.batch_size,
            'evaluation_method': 'single_caption_per_image',
            'temp_manager_used': temp_manager is not None,
            'storage_location': str(coco_eval_dir),
            'format_version': 'coco_val_evaluation_FIXED_v1',
            'alignment_fix_applied': True,
        }
    }
    
    # Optional raw embeddings (before projection)
    if args.save_raw_embeddings:
        embeddings_data.update({
            'clip_vision_raw': [],         # CLIP vision before projection [N, 1024]  
            'generated_clip_raw': [],      # Generated before projection [N, 1024]
        })
    
    logger.info("Starting FIXED embedding extraction...")
    print(f"\n‚ö° Extracting embeddings with ALIGNMENT FIXES...")
    
    total_processed = 0
    alignment_errors = 0
    start_time = time.time()
    
    for batch_idx, batch in enumerate(tqdm(dataloader, desc="Processing batches")):
        images = batch['images']
        captions_batch = batch['captions']
        image_ids = batch['image_ids']
        image_paths = batch['image_paths']
        
        # üîß CRITICAL FIX: Use proper indexing instead of zip
        batch_size_actual = len(images)
        
        for i in range(batch_size_actual):
            if total_processed >= args.num_samples:
                break
            
            try:
                # üîß FIXED: Use consistent indexing
                image = images[i]
                caption_list = captions_batch[i]
                image_id = image_ids[i]
                image_path = image_paths[i]
                
                # Use first caption (standard evaluation protocol)
                caption = caption_list[0]
                
                # üîß ALIGNMENT VERIFICATION: Check alignment for each sample
                if args.debug_alignment and coco_annotations:
                    is_aligned = verify_sample_alignment(
                        image_id, caption, image_path, coco_annotations, 
                        total_processed, debug=(total_processed < 10)
                    )
                    if not is_aligned:
                        alignment_errors += 1
                        if alignment_errors < 10:  # Show first 10 errors
                            print(f"‚ö†Ô∏è  Alignment error in sample {total_processed}")
                
                # Extract CLIP vision embeddings (with visual projection ‚Üí 768-dim)
                clip_vision_emb = evaluator.extract_clip_vision_embeddings([image])
                clip_vision_final = clip_vision_emb.squeeze(0).cpu()  # [768]
                
                # Extract EVA-CLIP vision embeddings  
                eva_vision_emb = evaluator.extract_eva_vision_embeddings([image])
                
                # Generate CLIP embeddings from EVA (with visual projection ‚Üí 768-dim)
                generated_clip_emb = evaluator.generate_clip_from_eva(eva_vision_emb)
                generated_clip_final = generated_clip_emb.squeeze(0).cpu()  # [768]
                
                # Extract text embeddings (already 768-dim, aligned)
                text_emb = evaluator.extract_clip_text_embeddings([caption])
                text_final = text_emb.squeeze(0).cpu()  # [768]
                
                # Optional: Extract raw embeddings before projection
                if args.save_raw_embeddings:
                    clip_vision_raw = evaluator._extract_clip_vision_raw([image])
                    clip_raw_final = clip_vision_raw.squeeze(0).cpu()  # [1024]
                    
                    generated_clip_raw = evaluator._generate_clip_from_eva_raw(eva_vision_emb)
                    generated_raw_final = generated_clip_raw.squeeze(0).cpu()  # [1024]
                    
                    embeddings_data['clip_vision_raw'].append(clip_raw_final)
                    embeddings_data['generated_clip_raw'].append(generated_raw_final)
                
                # üîß FIXED: Store with explicit alignment
                embeddings_data['clip_vision_embeddings'].append(clip_vision_final)
                embeddings_data['generated_clip_embeddings'].append(generated_clip_final)
                embeddings_data['text_embeddings'].append(text_final)
                embeddings_data['image_ids'].append(image_id)
                embeddings_data['captions'].append(caption)
                embeddings_data['image_paths'].append(image_path)
                
                total_processed += 1
                
                # Progress update
                if total_processed % 100 == 0:
                    logger.info(f"Processed {total_processed}/{args.num_samples} samples")
                    if args.debug_alignment:
                        error_rate = alignment_errors / total_processed * 100
                        print(f"   Alignment error rate: {error_rate:.1f}%")
                
            except Exception as e:
                logger.error(f"Error processing sample {total_processed}: {e}")
                continue
        
        if total_processed >= args.num_samples:
            break
    
    # Final alignment report
    if args.debug_alignment:
        final_error_rate = alignment_errors / total_processed * 100
        print(f"\nüìä Final Alignment Report:")
        print(f"   Total samples: {total_processed}")
        print(f"   Alignment errors: {alignment_errors}")
        print(f"   Error rate: {final_error_rate:.2f}%")
        print(f"   Success rate: {100 - final_error_rate:.2f}%")
        
        embeddings_data['metadata']['alignment_error_rate'] = final_error_rate
        embeddings_data['metadata']['alignment_errors'] = alignment_errors
    
    # Convert lists to tensors
    logger.info("Converting embeddings to tensors...")
    
    embeddings_data['clip_vision_embeddings'] = torch.stack(embeddings_data['clip_vision_embeddings'])
    embeddings_data['generated_clip_embeddings'] = torch.stack(embeddings_data['generated_clip_embeddings'])
    embeddings_data['text_embeddings'] = torch.stack(embeddings_data['text_embeddings'])
    
    if args.save_raw_embeddings:
        embeddings_data['clip_vision_raw'] = torch.stack(embeddings_data['clip_vision_raw'])
        embeddings_data['generated_clip_raw'] = torch.stack(embeddings_data['generated_clip_raw'])
    
    # Update metadata
    embeddings_data['metadata']['num_samples'] = total_processed
    embeddings_data['metadata']['processing_time'] = time.time() - start_time
    embeddings_data['metadata']['actual_shapes'] = {
        'clip_vision_embeddings': list(embeddings_data['clip_vision_embeddings'].shape),
        'generated_clip_embeddings': list(embeddings_data['generated_clip_embeddings'].shape),
        'text_embeddings': list(embeddings_data['text_embeddings'].shape),
    }
    
    # Validate embeddings
    logger.info("Validating extracted embeddings...")
    
    def validate_normalization(emb_tensor, name):
        norms = torch.norm(emb_tensor, p=2, dim=-1)
        mean_norm = norms.mean().item()
        std_norm = norms.std().item()
        max_dev = torch.abs(norms - 1.0).max().item()
        is_normalized = abs(mean_norm - 1.0) < 0.01 and max_dev < 0.1
        
        logger.info(f"  {name}: mean norm = {mean_norm:.6f}, std = {std_norm:.6f}, max_dev = {max_dev:.6f}")
        return is_normalized
    
    clip_vision_normalized = validate_normalization(embeddings_data['clip_vision_embeddings'], "CLIP vision")
    generated_normalized = validate_normalization(embeddings_data['generated_clip_embeddings'], "Generated")
    text_normalized = validate_normalization(embeddings_data['text_embeddings'], "Text")
    
    embeddings_data['metadata']['normalization_check'] = {
        'clip_vision_normalized': clip_vision_normalized,
        'generated_normalized': generated_normalized,
        'text_normalized': text_normalized,
    }
    
    # Save embeddings to structured location
    logger.info(f"Saving FIXED embeddings to structured location...")
    
    # Save as pickle (most flexible)
    pickle_path = coco_eval_dir / f"{output_name}.pkl"
    logger.info(f"Saving embeddings to {pickle_path}...")
    
    with open(pickle_path, 'wb') as f:
        pickle.dump(embeddings_data, f)
    
    # Save metadata as JSON (human readable)
    json_path = coco_eval_dir / f"{output_name}_metadata.json"
    with open(json_path, 'w') as f:
        # Convert tensors to lists for JSON serialization
        metadata_json = embeddings_data['metadata'].copy()
        json.dump(metadata_json, f, indent=2)
    
    # Save embeddings as NPZ (for easy loading with numpy)
    npz_path = coco_eval_dir / f"{output_name}.npz"
    logger.info(f"Saving embeddings to {npz_path}...")
    
    npz_data = {
        'clip_vision_embeddings': embeddings_data['clip_vision_embeddings'].numpy(),
        'generated_clip_embeddings': embeddings_data['generated_clip_embeddings'].numpy(),
        'text_embeddings': embeddings_data['text_embeddings'].numpy(),
        'image_ids': np.array(embeddings_data['image_ids']),
        'captions': np.array(embeddings_data['captions']),
        'image_paths': np.array(embeddings_data['image_paths']),
    }
    
    if 'clip_vision_raw' in embeddings_data:
        npz_data['clip_vision_raw'] = embeddings_data['clip_vision_raw'].numpy()
        npz_data['generated_clip_raw'] = embeddings_data['generated_clip_raw'].numpy()
    
    np.savez_compressed(npz_path, **npz_data)
    
    return {
        'embedding_file': pickle_path,
        'metadata_file': json_path,
        'npz_file': npz_path,
        'num_samples': total_processed,
        'processing_time': time.time() - start_time,
        'storage_dir': coco_eval_dir,
        'alignment_errors': alignment_errors if args.debug_alignment else 0,
        'reused_existing': False
    }

def main():
    """Main extraction function."""
    project_root = setup_paths()
    args = parse_arguments()
    logger = setup_logging(args.verbose)
    
    print("üîß FIXED COCO Embedding Extraction")
    print("=" * 50)
    print("üö® ALIGNMENT BUG FIXES APPLIED:")
    print("   ‚Ä¢ Fixed batch index alignment in extraction loop")
    print("   ‚Ä¢ Added per-sample alignment verification")
    print("   ‚Ä¢ Explicit index handling to prevent misalignment") 
    print("   ‚Ä¢ Debug output for alignment tracking")
    print("=" * 50)
    print(f"üì¶ Extracting {args.num_samples} image-caption pairs")
    print(f"üîß Device: {args.device}")
    print(f"üé≤ Random seed: {args.random_seed}")
    print(f"üîç Debug alignment: {args.debug_alignment}")
    print("=" * 50)
    
    try:
        # Setup temp manager
        temp_manager = setup_temp_manager()
        
        # Validate paths
        validate_paths(args)
        
        # Extract embeddings with FIXES
        result = extract_embeddings_FIXED(args, logger, temp_manager)
        
        if result is None:
            print("‚ùå Extraction aborted by user")
            return 1
        
        # Print summary
        print("\n" + "=" * 70)
        print("üì¶ FIXED COCO EMBEDDING EXTRACTION COMPLETED")
        print("=" * 70)
        
        print(f"‚úÖ Successfully extracted {result['num_samples']} samples")
        print(f"‚è±Ô∏è  Processing time: {result['processing_time']:.1f} seconds")
        print(f"üìÅ Storage directory: {result['storage_dir']}")
        print(f"üìÑ Files created:")
        print(f"   Primary: {result['embedding_file'].name}")
        print(f"   Metadata: {result['metadata_file'].name}")
        print(f"   NumPy: {result['npz_file'].name}")
        
        if args.debug_alignment:
            print(f"üîç Alignment verification:")
            print(f"   Alignment errors: {result['alignment_errors']}")
            error_rate = result['alignment_errors'] / result['num_samples'] * 100
            print(f"   Error rate: {error_rate:.2f}%")
            if error_rate < 5:
                print("   ‚úÖ Excellent alignment quality!")
            elif error_rate < 15:
                print("   ‚ö†Ô∏è  Some alignment issues detected")
            else:
                print("   ‚ùå Significant alignment problems remain")
        
        print("")
        print("üîß FIXES APPLIED:")
        print("   Fixed batch indexing: ‚úÖ")
        print("   Per-sample verification: ‚úÖ")
        print("   Explicit alignment tracking: ‚úÖ")
        
        print("")
        print("üöÄ Next Steps - Test the Fix:")
        print(f"   # Quick evaluation on FIXED embeddings:")
        print(f"   python fixed_quick_evaluation.py --embeddings_file {result['embedding_file']} --skip_normalization")
        print("")
        print("üéØ Expected Results with FIXES:")
        print("   ‚Ä¢ CLIP R@1: 25-35% (consistent across all sample sizes)")
        print("   ‚Ä¢ No more sample size dependency")
        print("   ‚Ä¢ Most samples should rank 1 or close to 1")
        
        print("=" * 70)
        
        return 0
        
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è  Extraction interrupted by user")
        return 1
    
    except Exception as e:
        print(f"\n‚ùå Extraction failed: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)