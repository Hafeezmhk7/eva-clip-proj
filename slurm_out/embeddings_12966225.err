============================================================================================== 
Warning! Mixing Conda and module environments may lead to corruption of the
user environment. 
We do not recommend users mixing those two environments unless absolutely
necessary. Note that 
SURF does not provide any support for Conda environment.
For more information, please refer to our software policy page:
https://servicedesk.surf.nl/wiki/display/WIKI/Software+policy+Snellius#SoftwarepolicySnellius-UseofAnacondaandMinicondaenvironmentsonSnellius 

Remember that many packages have already been installed on the system and can
be loaded using 
the 'module load <package__name>' command. If you are uncertain if a package is
already available 
on the system, please use 'module avail' or 'module spider' to search for it.
============================================================================================== 
  File "<string>", line 1
    import torch; print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")
                                                                                         ^
SyntaxError: unexpected character after line continuation character
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.26s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.33it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.74it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.89it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.13it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.91it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.47it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.68it/s]
Traceback (most recent call last):
  File "/gpfs/home1/scur2711/eva-clip-flow-matching/src/modules/extract_embeddings.py", line 54, in load_eva_clip_model
    eva_model.to(device)
    ~~~~~~~~~~~~^^^^^^^^
  File "/home/scur2711/.conda/envs/eva_clip_env/lib/python3.13/site-packages/transformers/modeling_utils.py", line 4110, in to
    return super().to(*args, **kwargs)
           ~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/home/scur2711/.conda/envs/eva_clip_env/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ~~~~~~~~~~~^^^^^^^^^
  File "/home/scur2711/.conda/envs/eva_clip_env/lib/python3.13/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
    ~~~~~~~~~~~~~^^^^
  File "/home/scur2711/.conda/envs/eva_clip_env/lib/python3.13/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
    ~~~~~~~~~~~~~^^^^
  File "/home/scur2711/.conda/envs/eva_clip_env/lib/python3.13/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
    ~~~~~~~~~~~~~^^^^
  [Previous line repeated 3 more times]
  File "/home/scur2711/.conda/envs/eva_clip_env/lib/python3.13/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
  File "/home/scur2711/.conda/envs/eva_clip_env/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ~~~~^
        device,
        ^^^^^^^
        dtype if t.is_floating_point() or t.is_complex() else None,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        non_blocking,
        ^^^^^^^^^^^^^
    )
    ^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 320.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 62.12 MiB is free. Including non-PyTorch memory, this process has 39.43 GiB memory in use. Of the allocated memory 38.85 GiB is allocated by PyTorch, and 178.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/home1/scur2711/eva-clip-flow-matching/src/modules/extract_embeddings.py", line 67, in load_eva_clip_model
    eva_model = AutoModel.from_pretrained(eva_model_name, trust_remote_code=True)
  File "/home/scur2711/.conda/envs/eva_clip_env/lib/python3.13/site-packages/transformers/models/auto/auto_factory.py", line 547, in from_pretrained
    config, kwargs = AutoConfig.from_pretrained(
                     ~~~~~~~~~~~~~~~~~~~~~~~~~~^
        pretrained_model_name_or_path,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<4 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "/home/scur2711/.conda/envs/eva_clip_env/lib/python3.13/site-packages/transformers/models/auto/configuration_auto.py", line 1238, in from_pretrained
    raise ValueError(
    ...<3 lines>...
    )
ValueError: Unrecognized model in QuanSun/EVA-CLIP. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, arcee, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, bitnet, blenderbot, blenderbot-small, blip, blip-2, blip_2_qformer, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, colqwen2, conditional_detr, convbert, convnext, convnextv2, cpmant, csm, ctrl, cvt, d_fine, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v3, deformable_detr, deit, depth_anything, depth_pro, deta, detr, dia, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, donut-swin, dots1, dpr, dpt, efficientformer, efficientnet, electra, emu3, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_h1, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, gemma3n, gemma3n_audio, gemma3n_text, gemma3n_vision, git, glm, glm4, glm4v, glm4v_text, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granite_speech, granitemoe, granitemoehybrid, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hgnet_v2, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, internvl, internvl_vision, jamba, janus, jetmoe, jukebox, kosmos-2, kyutai_speech_to_text, layoutlm, layoutlmv2, layoutlmv3, led, levit, lightglue, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, minimax, mistral, mistral3, mixtral, mlcd, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_omni, qwen2_5_vl, qwen2_5_vl_text, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen2_vl_text, qwen3, qwen3_moe, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam_hq, sam_hq_vision_model, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smollm3, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, t5gemma, table-transformer, tapas, textnet, time_series_transformer, timesfm, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, vjepa2, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zamba2, zoedepth

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/home1/scur2711/eva-clip-flow-matching/src/modules/extract_embeddings.py", line 289, in <module>
    main()
    ~~~~^^
  File "/gpfs/home1/scur2711/eva-clip-flow-matching/src/modules/extract_embeddings.py", line 189, in main
    eva_processor, eva_model = load_eva_clip_model(device)
                               ~~~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/gpfs/home1/scur2711/eva-clip-flow-matching/src/modules/extract_embeddings.py", line 83, in load_eva_clip_model
    fallback_model.to(device)
    ~~~~~~~~~~~~~~~~~^^^^^^^^
  File "/home/scur2711/.conda/envs/eva_clip_env/lib/python3.13/site-packages/transformers/modeling_utils.py", line 4110, in to
    return super().to(*args, **kwargs)
           ~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/home/scur2711/.conda/envs/eva_clip_env/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ~~~~~~~~~~~^^^^^^^^^
  File "/home/scur2711/.conda/envs/eva_clip_env/lib/python3.13/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
    ~~~~~~~~~~~~~^^^^
  File "/home/scur2711/.conda/envs/eva_clip_env/lib/python3.13/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
    ~~~~~~~~~~~~~^^^^
  File "/home/scur2711/.conda/envs/eva_clip_env/lib/python3.13/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
    ~~~~~~~~~~~~~^^^^
  File "/home/scur2711/.conda/envs/eva_clip_env/lib/python3.13/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
  File "/home/scur2711/.conda/envs/eva_clip_env/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ~~~~^
        device,
        ^^^^^^^
        dtype if t.is_floating_point() or t.is_complex() else None,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        non_blocking,
        ^^^^^^^^^^^^^
    )
    ^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 146.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 62.12 MiB is free. Including non-PyTorch memory, this process has 39.43 GiB memory in use. Of the allocated memory 38.85 GiB is allocated by PyTorch, and 178.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
