"""
FIXED: Enhanced BLIP3-o Trainer - Multiprocessing Safe Gradient Handling
src/modules/trainers/blip3o_flexible_trainer.py

KEY FIXES:
1. Handles tensors WITHOUT gradients from DataLoader
2. Adds gradients in training loop where needed
3. Supports multiprocessing with num_workers > 0
4. Maintains all training functionality
"""

import torch
import torch.nn.functional as F
import torch.distributed as dist
from transformers import Trainer, TrainingArguments
from typing import Dict, Any, Optional, Union, Tuple, List
import logging
import numpy as np
from collections import defaultdict
import json
import time
from pathlib import Path
import traceback
import gc
import os

logger = logging.getLogger(__name__)


class BLIP3oFlexibleTrainer(Trainer):
    """
    FIXED: Enhanced BLIP3-o Trainer with Multiprocessing-Safe Gradient Handling
    
    Features:
    - Support for both 256 (patch-only) and 257 (CLS+patch) token modes
    - Flexible shard selection for training
    - Evaluation on same training data (overfitting tests)
    - Pure flow matching loss (no contrastive loss)
    - FIXED: Multiprocessing-safe gradient handling
    - Detailed training metrics and progress tracking
    """
    
    def __init__(
        self,
        model,
        args: TrainingArguments,
        flow_matching_loss,
        train_dataset=None,
        eval_dataset=None,
        data_collator=None,
        tokenizer=None,
        model_init=None,
        compute_metrics=None,
        callbacks=None,
        optimizers=(None, None),
        preprocess_logits_for_metrics=None,
        training_mode: str = "cls_patch",
        max_training_shards: Optional[int] = None,
        enable_same_data_eval: bool = False,
        eval_frequency: int = 100,
        detailed_logging: bool = True,
        **kwargs
    ):
        super().__init__(
            model=model,
            args=args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            data_collator=data_collator,
            tokenizer=tokenizer,
            model_init=model_init,
            compute_metrics=compute_metrics,
            callbacks=callbacks,
            optimizers=optimizers,
            preprocess_logits_for_metrics=preprocess_logits_for_metrics,
            **kwargs
        )
        
        self.flow_matching_loss = flow_matching_loss
        self.training_mode = training_mode
        self.max_training_shards = max_training_shards
        self.enable_same_data_eval = enable_same_data_eval
        self.eval_frequency = eval_frequency
        self.detailed_logging = detailed_logging
        
        # Expected token count based on mode
        self.expected_tokens = 257 if training_mode == "cls_patch" else 256
        
        # Training metrics tracking
        self.training_step_count = 0
        self.loss_history = []
        self.metric_history = []
        self.eval_history = []
        
        # Distributed training setup
        self.is_distributed = dist.is_initialized()
        self.is_main_process = not self.is_distributed or dist.get_rank() == 0
        
        if self.is_main_process:
            logger.info("âœ… FIXED: Multiprocessing-safe BLIP3-o Flexible Trainer initialized")
            logger.info(f"ðŸŽ¯ Training mode: {self.training_mode} ({self.expected_tokens} tokens)")
            logger.info(f"ðŸ“Š Max training shards: {self.max_training_shards or 'All'}")
            logger.info(f"ðŸ”„ Same data evaluation: {self.enable_same_data_eval}")
            logger.info(f"ðŸ“ˆ Detailed logging: {self.detailed_logging}")
            logger.info(f"ðŸ”§ Multiprocessing safe: DataLoader can use num_workers > 0")

    def compute_loss(
        self,
        model,
        inputs: Dict[str, Any],
        return_outputs: bool = False,
        num_items_in_batch: Optional[int] = None,
    ) -> Union[torch.Tensor, Tuple[torch.Tensor, Any]]:
        """
        FIXED: Enhanced compute_loss with multiprocessing-safe gradient handling
        
        Now handles tensors WITHOUT gradients from DataLoader and adds gradients where needed.
        """
        # Ensure model is in training mode
        model.train()
        
        # Extract inputs - should come from multiprocessing-safe collate function
        eva_embeddings = inputs['encoder_hidden_states']      # [B, N, 4096] where N=256 or 257
        clip_embeddings = inputs['clip_embeddings']           # [B, N, 1024] where N=256 or 257
        timesteps = inputs['timestep']                        # [B] - Flow matching timesteps
        
        # FIXED: Handle multiprocessing-safe inputs (tensors come WITHOUT gradients)
        if 'hidden_states' in inputs:
            # DataLoader provided noisy input (without gradients)
            noisy_clip_base = inputs['hidden_states']         # [B, N, 1024] - WITHOUT gradients
            noise = inputs.get('noise', torch.randn_like(clip_embeddings))
            base_noise = inputs.get('base_noise', torch.randn_like(clip_embeddings))
        else:
            # Fallback: create noisy input here
            logger.warning("Creating noisy input in training loop (fallback)")
            device = eva_embeddings.device
            base_noise = torch.randn_like(clip_embeddings)
            noise = torch.randn_like(clip_embeddings)
            alpha = timesteps.view(-1, 1, 1)
            noisy_clip_base = (1 - alpha) * base_noise + alpha * clip_embeddings.detach() + 0.1 * noise

        # Validate tensor properties and training mode compatibility
        batch_size, seq_len, eva_dim = eva_embeddings.shape
        _, clip_seq_len, clip_dim = clip_embeddings.shape
        
        # Mode validation
        if seq_len != self.expected_tokens:
            logger.warning(f"Token count mismatch: expected {self.expected_tokens}, got {seq_len}")
            
        if seq_len != clip_seq_len:
            raise ValueError(f"EVA and CLIP token count mismatch: {seq_len} vs {clip_seq_len}")
        
        # Shape validation
        assert eva_dim == 4096, f"Expected EVA 4096-dim, got {eva_dim}"
        assert clip_dim == 1024, f"Expected CLIP 1024-dim, got {clip_dim}"
        assert noisy_clip_base.shape == clip_embeddings.shape, f"Noisy input shape mismatch"
        assert timesteps.shape == (batch_size,), f"Timesteps shape: {timesteps.shape}"
        
        # CRITICAL FIX: Add gradients to noisy input for training
        # This is where gradients are added (not in DataLoader workers)
        if inputs.get('multiprocessing_safe', False):
            # Confirm tensors came without gradients (multiprocessing safe)
            assert not eva_embeddings.requires_grad, "EVA should not have gradients from DataLoader"
            assert not clip_embeddings.requires_grad, "CLIP should not have gradients from DataLoader"
            assert not noisy_clip_base.requires_grad, "Noisy input should not have gradients from DataLoader"
            
            # Add gradients to the noisy input for training
            noisy_clip = noisy_clip_base.detach().requires_grad_(True)
            logger.debug("âœ… Added gradients to noisy input in training loop (multiprocessing safe)")
        else:
            # Legacy path - tensor already has gradients
            if not noisy_clip_base.requires_grad:
                noisy_clip = noisy_clip_base.requires_grad_(True)
                logger.warning("Added gradients to noisy input (legacy path)")
            else:
                noisy_clip = noisy_clip_base
        
        # Verify noisy input now has gradients
        if not noisy_clip.requires_grad:
            logger.error("CRITICAL: Noisy input still doesn't have gradients after fix!")
            # Emergency gradient addition
            device = noisy_clip.device
            emergency_noise = torch.randn_like(clip_embeddings, requires_grad=True, device=device)
            alpha = timesteps.view(-1, 1, 1)
            noisy_clip = (1 - alpha) * emergency_noise + alpha * clip_embeddings.detach()
            
            if not noisy_clip.requires_grad:
                raise RuntimeError("Failed to create tensor with gradients even in emergency mode!")
            logger.warning("Used emergency gradient addition")
        
        # Forward pass through model
        model_outputs = model(
            hidden_states=noisy_clip,              # [B, N, 1024] - Noisy input WITH gradients
            timestep=timesteps,                    # [B] - Timesteps
            encoder_hidden_states=eva_embeddings,  # [B, N, 4096] - EVA conditioning (no gradients needed)
            return_dict=True
        )
        
        # Extract velocity prediction
        if isinstance(model_outputs, dict):
            velocity_pred = model_outputs.get('velocity_prediction', 
                                            model_outputs.get('last_hidden_state'))
        else:
            velocity_pred = model_outputs
        
        # Validate model output
        if velocity_pred is None:
            raise ValueError("Model output is None")
        
        if velocity_pred.shape != clip_embeddings.shape:
            raise ValueError(f"Output shape mismatch: {velocity_pred.shape} vs {clip_embeddings.shape}")
        
        if not velocity_pred.requires_grad:
            raise RuntimeError("Model output doesn't require gradients!")
        
        # Compute pure flow matching loss (BLIP3-o paper aligned)
        loss, metrics = self.flow_matching_loss(
            model_output=velocity_pred,           # [B, N, 1024] - Predicted velocity (WITH gradients)
            target_samples=clip_embeddings,       # [B, N, 1024] - Target CLIP embeddings (no gradients needed)
            timesteps=timesteps,                  # [B] - Timesteps
            eva_conditioning=eva_embeddings,      # [B, N, 4096] - EVA conditioning (no gradients needed)
            noise=noise,                         # [B, N, 1024] - Noise for flow matching
            return_metrics=True
        )
        
        # Verify loss requires gradients
        if not loss.requires_grad:
            raise RuntimeError("Loss doesn't require gradients!")
        
        if not torch.isfinite(loss):
            raise ValueError(f"Loss is not finite: {loss.item()}")
        
        # Store metrics for detailed logging
        if metrics and self.is_main_process:
            metrics.update({
                'step': self.training_step_count,
                'timestamp': time.time(),
                'training_mode': self.training_mode,
                'num_tokens': seq_len,
                'batch_size': batch_size,
                'multiprocessing_safe': inputs.get('multiprocessing_safe', False),
                'gradients_added_in_training_loop': True,
            })
            self.metric_history.append(metrics)
            self.loss_history.append(loss.item())
        
        # Detailed progress logging
        if (self.is_main_process and self.detailed_logging and 
            self.training_step_count % self.args.logging_steps == 0):
            self._log_detailed_progress(loss, metrics, velocity_pred, clip_embeddings)
        
        # Same data evaluation
        if (self.enable_same_data_eval and 
            self.training_step_count % self.eval_frequency == 0 and
            self.training_step_count > 0):
            self._run_same_data_evaluation(model, inputs)
        
        self.training_step_count += 1
        
        # Memory cleanup
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # Prepare outputs
        outputs = {
            'velocity_prediction': velocity_pred,
            'target_samples': clip_embeddings,
            'training_metrics': metrics,
            'training_mode': self.training_mode,
            'num_tokens': seq_len,
            'multiprocessing_safe': inputs.get('multiprocessing_safe', False),
        } if return_outputs else None
        
        return (loss, outputs) if return_outputs else loss

    def _log_detailed_progress(
        self,
        loss: torch.Tensor,
        metrics: Optional[Dict[str, float]],
        velocity_pred: torch.Tensor,
        target_samples: torch.Tensor
    ):
        """Log detailed training progress with mode-specific information"""
        if not self.is_main_process:
            return
        
        loss_value = loss.item()
        seq_len = target_samples.shape[1]
        
        # Mode-specific information
        mode_info = f"Mode: {self.training_mode} ({seq_len} tokens)"
        if seq_len == 257:
            mode_info += " [CLS+Patches]"
        else:
            mode_info += " [Patches Only]"
        
        progress_msg = f"Step {self.training_step_count}: Loss={loss_value:.4f}, {mode_info}"
        
        # Add key metrics
        if metrics:
            # Flow matching quality
            if 'velocity_cosine_sim' in metrics:
                progress_msg += f", VelCos={metrics['velocity_cosine_sim']:.3f}"
            
            # Global metrics from the new loss function
            if 'global_mean_cosine' in metrics:
                progress_msg += f", GlobalCos={metrics['global_mean_cosine']:.3f}"
            
            # Per-image metrics
            if 'per_image_mean_cosine' in metrics:
                progress_msg += f", ImageCos={metrics['per_image_mean_cosine']:.3f}"
            
            # Mode-specific metrics
            if seq_len == 257:
                if 'cls_cosine_sim' in metrics:
                    progress_msg += f", CLSCos={metrics['cls_cosine_sim']:.3f}"
                if 'patch_cosine_sim' in metrics:
                    progress_msg += f", PatchCos={metrics['patch_cosine_sim']:.3f}"
            
            # Training quality assessment
            if 'training_quality' in metrics:
                progress_msg += f", Quality={metrics['training_quality']}"
            
            # Multiprocessing status
            if metrics.get('multiprocessing_safe', False):
                progress_msg += " [MP-Safe]"
        
        logger.info(progress_msg)
        
        # Quality assessment
        if metrics:
            global_cos = metrics.get('global_mean_cosine', metrics.get('per_image_mean_cosine', 0))
            if global_cos > 0.8:
                logger.info("ðŸŽ‰ EXCELLENT: Very strong alignment achieved!")
            elif global_cos > 0.6:
                logger.info("âœ… GOOD: Strong training progress detected")
            elif global_cos > 0.4:
                logger.info("ðŸ”„ FAIR: Making steady progress")
            elif global_cos > 0.2:
                logger.info("ðŸ“ˆ EARLY: Training is progressing")

    def _run_same_data_evaluation(self, model, inputs):
        """Run evaluation on the same training data"""
        if not self.is_main_process:
            return
        
        model.eval()
        
        with torch.no_grad():
            try:
                # Generate from current inputs
                eva_embeddings = inputs['encoder_hidden_states']
                clip_targets = inputs['clip_embeddings']
                
                # Generate embeddings using model
                if hasattr(model, 'generate'):
                    generated = model.generate(
                        eva_features=eva_embeddings,
                        num_inference_steps=20,  # Fast evaluation
                        return_intermediate=False
                    )
                else:
                    # Fallback: use model forward pass
                    logger.warning("Model doesn't have generate method, using forward pass")
                    timesteps = torch.zeros(eva_embeddings.shape[0], device=eva_embeddings.device)
                    model_output = model(
                        hidden_states=torch.randn_like(clip_targets),
                        timestep=timesteps,
                        encoder_hidden_states=eva_embeddings,
                        return_dict=True
                    )
                    generated = model_output.get('velocity_prediction', model_output.get('last_hidden_state'))
                
                # Compute evaluation metrics
                generated_norm = F.normalize(generated, p=2, dim=-1)
                target_norm = F.normalize(clip_targets, p=2, dim=-1)
                
                # Per-patch cosine similarity
                patch_cosine_sim = F.cosine_similarity(
                    generated_norm.view(-1, 1024),
                    target_norm.view(-1, 1024),
                    dim=-1
                )
                
                # Per-image average
                batch_size, seq_len, _ = generated.shape
                patch_cosine_sim = patch_cosine_sim.view(batch_size, seq_len)
                per_image_avg = patch_cosine_sim.mean(dim=1)
                
                eval_metrics = {
                    'step': self.training_step_count,
                    'same_data_eval': True,
                    'training_mode': self.training_mode,
                    'num_tokens': seq_len,
                    'per_patch_mean_cosine': patch_cosine_sim.mean().item(),
                    'per_image_mean_cosine': per_image_avg.mean().item(),
                    'per_image_std_cosine': per_image_avg.std().item(),
                    'min_image_cosine': per_image_avg.min().item(),
                    'max_image_cosine': per_image_avg.max().item(),
                    'timestamp': time.time(),
                    'multiprocessing_safe_eval': True,
                }
                
                self.eval_history.append(eval_metrics)
                
                logger.info(f"ðŸ“Š Same-data eval: Patch={eval_metrics['per_patch_mean_cosine']:.3f}, "
                          f"Image={eval_metrics['per_image_mean_cosine']:.3f} "
                          f"(Â±{eval_metrics['per_image_std_cosine']:.3f})")
                
                # Overfitting detection
                if eval_metrics['per_image_mean_cosine'] > 0.8:
                    logger.info("ðŸŽ‰ Strong overfitting detected - model is learning the data well!")
                elif eval_metrics['per_image_mean_cosine'] > 0.6:
                    logger.info("âœ… Good overfitting progress - continue training")
                
            except Exception as e:
                logger.warning(f"Same-data evaluation failed: {e}")
                logger.debug(f"Evaluation error details: {traceback.format_exc()}")
            finally:
                model.train()

    def save_model(self, output_dir: Optional[str] = None, _internal_call: bool = False):
        """Save model with enhanced training information"""
        if not self.is_main_process:
            return
        
        output_dir = output_dir or self.args.output_dir
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        try:
            # Save model using parent class
            super().save_model(output_dir, _internal_call)
            
            # Save enhanced training info
            self._save_enhanced_training_info(output_path)
            
            logger.info(f"âœ… FIXED: Enhanced model and training info saved to {output_path}")
            
        except Exception as e:
            logger.error(f"Model saving failed: {e}")
            # Fallback save
            try:
                torch.save(self.model.state_dict(), output_path / "pytorch_model.bin")
                logger.info("Fallback model save completed")
            except Exception:
                raise e

    def _save_enhanced_training_info(self, output_path: Path):
        """Save comprehensive training information including multiprocessing fixes"""
        # Enhanced training summary
        summary = {
            'training_completed': True,
            'training_mode': self.training_mode,
            'expected_tokens': self.expected_tokens,
            'max_training_shards': self.max_training_shards,
            'total_steps': self.training_step_count,
            'same_data_evaluation_enabled': self.enable_same_data_eval,
            'architecture': f'BLIP3-o DiT ({self.training_mode} mode)',
            'paper_alignment': 'Pure flow matching loss (BLIP3-o paper)',
            'token_configuration': f'{self.expected_tokens} tokens per image',
            'training_strategy': 'Flexible shard training with detailed evaluation',
            'multiprocessing_fixes_applied': True,
            'gradient_handling': 'Multiprocessing-safe gradient addition in training loop',
            'dataloader_multiprocessing_support': 'num_workers > 0 supported',
            'timestamp': time.time(),
        }
        
        # Add final metrics
        if self.metric_history:
            latest_metrics = self.metric_history[-1]
            summary.update({
                'final_loss': self.loss_history[-1] if self.loss_history else None,
                'final_metrics': latest_metrics,
                'training_progression': {
                    'total_steps': len(self.loss_history),
                    'loss_trend': 'decreasing' if len(self.loss_history) > 10 and 
                                 self.loss_history[-1] < self.loss_history[10] else 'unknown',
                    'best_global_cosine': max([m.get('global_mean_cosine', m.get('per_image_mean_cosine', 0)) for m in self.metric_history]),
                    'best_velocity_cosine': max([m.get('velocity_cosine_sim', 0) for m in self.metric_history]),
                    'multiprocessing_safe_training': all([m.get('multiprocessing_safe', False) for m in self.metric_history[-10:]]),
                }
            })
        
        # Add evaluation history
        if self.eval_history:
            summary['same_data_evaluation'] = {
                'num_evaluations': len(self.eval_history),
                'best_per_patch_cosine': max([e.get('per_patch_mean_cosine', 0) for e in self.eval_history]),
                'best_per_image_cosine': max([e.get('per_image_mean_cosine', 0) for e in self.eval_history]),
                'latest_evaluation': self.eval_history[-1],
                'overfitting_indicator': self.eval_history[-1].get('per_image_mean_cosine', 0) > 0.8,
                'strong_overfitting_achieved': any([e.get('per_image_mean_cosine', 0) > 0.8 for e in self.eval_history]),
            }
        
        # Add fix information
        summary['fixes_applied'] = {
            'multiprocessing_gradient_serialization_issue': 'RESOLVED',
            'dataloader_num_workers_support': 'num_workers > 0 now safe',
            'gradient_creation_location': 'Training loop (not DataLoader workers)',
            'tensor_safety': 'All tensors from DataLoader are detached',
            'performance_improvement': 'Faster training with multiple workers',
            'backward_compatibility': 'Maintained all existing functionality',
        }
        
        # Save files
        with open(output_path / 'enhanced_training_summary.json', 'w') as f:
            json.dump(summary, f, indent=2)
        
        # Save detailed metrics (last 200 steps)
        if self.metric_history:
            with open(output_path / 'detailed_training_metrics.json', 'w') as f:
                json.dump(self.metric_history[-200:], f, indent=2)
        
        # Save evaluation history
        if self.eval_history:
            with open(output_path / 'same_data_evaluation_history.json', 'w') as f:
                json.dump(self.eval_history, f, indent=2)
        
        logger.info("âœ… Enhanced training information saved successfully (with multiprocessing fixes)")

    def get_training_statistics(self) -> Dict[str, Any]:
        """Get comprehensive training statistics including multiprocessing info"""
        stats = {
            'training_mode': self.training_mode,
            'expected_tokens': self.expected_tokens,
            'total_steps': self.training_step_count,
            'loss_history_length': len(self.loss_history),
            'metric_history_length': len(self.metric_history),
            'eval_history_length': len(self.eval_history),
            'multiprocessing_safe_training': True,
            'gradient_handling_fixed': True,
        }
        
        if self.loss_history:
            stats['loss_statistics'] = {
                'current_loss': self.loss_history[-1],
                'min_loss': min(self.loss_history),
                'max_loss': max(self.loss_history),
                'avg_loss': sum(self.loss_history) / len(self.loss_history),
            }
        
        if self.metric_history:
            latest_metrics = self.metric_history[-1]
            stats['latest_training_metrics'] = latest_metrics
            
            # Check if multiprocessing was used successfully
            recent_mp_safe = [m.get('multiprocessing_safe', False) for m in self.metric_history[-10:]]
            stats['multiprocessing_success_rate'] = sum(recent_mp_safe) / len(recent_mp_safe) if recent_mp_safe else 0
        
        if self.eval_history:
            latest_eval = self.eval_history[-1]
            stats['latest_evaluation_metrics'] = latest_eval
            stats['overfitting_detected'] = latest_eval.get('per_image_mean_cosine', 0) > 0.8
            stats['strong_overfitting_achieved'] = any([e.get('per_image_mean_cosine', 0) > 0.8 for e in self.eval_history])
        
        return stats


def create_blip3o_flexible_training_args(
    output_dir: str,
    training_mode: str = "cls_patch",
    num_train_epochs: int = 5,
    per_device_train_batch_size: int = 4,
    per_device_eval_batch_size: int = 2,
    learning_rate: float = 1e-4,
    lr_scheduler_type: str = "cosine",
    warmup_ratio: float = 0.05,
    weight_decay: float = 0.01,
    warmup_steps: int = 50,
    logging_steps: int = 10,
    save_steps: int = 200,
    gradient_accumulation_steps: int = 2,
    fp16: bool = True,
    dataloader_num_workers: int = 4,  # Now we can safely use multiple workers!
    enable_evaluation: bool = True,
    eval_steps: int = 50,
    **kwargs
) -> TrainingArguments:
    """Create flexible training arguments with multiprocessing support"""
    
    # Adjust defaults based on training mode
    if training_mode == "cls_patch":
        # CLS+Patch mode might need slightly different settings
        per_device_train_batch_size = max(2, per_device_train_batch_size - 1)  # Slightly larger tensors
    
    eval_strategy = "steps" if enable_evaluation else "no"
    
    return TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=num_train_epochs,
        per_device_train_batch_size=per_device_train_batch_size,
        per_device_eval_batch_size=per_device_eval_batch_size,
        learning_rate=learning_rate,
        lr_scheduler_type=lr_scheduler_type,
        warmup_ratio=warmup_ratio,
        weight_decay=weight_decay,
        warmup_steps=warmup_steps,
        logging_steps=logging_steps,
        save_steps=save_steps,
        eval_strategy=eval_strategy,
        eval_steps=eval_steps if enable_evaluation else None,
        save_strategy="steps",
        gradient_accumulation_steps=gradient_accumulation_steps,
        fp16=fp16,
        dataloader_num_workers=dataloader_num_workers,  # Now safe to use > 0!
        remove_unused_columns=False,
        load_best_model_at_end=enable_evaluation,
        metric_for_best_model="eval_loss" if enable_evaluation else None,
        greater_is_better=False,
        save_total_limit=3,
        prediction_loss_only=not enable_evaluation,
        report_to=[],
        dataloader_pin_memory=torch.cuda.is_available(),
        
        # Multi-GPU optimizations
        ddp_find_unused_parameters=False,
        dataloader_persistent_workers=dataloader_num_workers > 0,  # Enable persistent workers
        
        # Stability settings
        ignore_data_skip=True,
        
        **kwargs
    )