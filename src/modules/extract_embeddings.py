"""
Simple Embedding Extraction Script
Place this file as: extract_embeddings.py (in project root)

This script does:
‚úÖ Load images from your dataset.py
‚úÖ Extract EVA-CLIP embeddings (REAL implementation!)
‚úÖ Extract CLIP ViT-L/14 embeddings
‚úÖ Save them to files

Ready for flow matching training!

Requirements:
pip install transformers torch torchvision tqdm pillow
"""

import sys
import os
import torch
from transformers import CLIPProcessor, CLIPModel, AutoModel, CLIPImageProcessor
import pickle
from tqdm import tqdm

# Add project root to path so we can import our dataset
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))  # Go up to project root

from src.data_hand.dataset import BLIP3oWebDataset

def load_clip_model(device):
    """Load CLIP ViT-L/14 model"""
    print("üì¶ Loading CLIP ViT-L/14...")
    
    processor = CLIPProcessor.from_pretrained("openai/clip-vit-large-patch14")
    model = CLIPModel.from_pretrained("openai/clip-vit-large-patch14")
    model.to(device)
    model.eval()
    
    print("‚úÖ CLIP ViT-L/14 loaded")
    return processor, model

def load_eva_clip_model(device):
    """Load EVA-CLIP model"""
    print("üì¶ Loading EVA-CLIP...")
    
    try:
        # Try to load BAAI/EVA-CLIP-8B first (newer, better performance)
        print("   Attempting to load BAAI/EVA-CLIP-8B...")
        eva_model_name = "BAAI/EVA-CLIP-8B"
        
        # Use AutoModel for EVA-CLIP
        eva_model = AutoModel.from_pretrained(eva_model_name, trust_remote_code=True)
        eva_processor = CLIPImageProcessor.from_pretrained("openai/clip-vit-large-patch14")
        
        eva_model.to(device)
        eva_model.eval()
        
        print("‚úÖ EVA-CLIP-8B loaded successfully")
        return eva_processor, eva_model
        
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to load BAAI/EVA-CLIP-8B: {e}")
        print("   Trying QuanSun/EVA-CLIP as fallback...")
        
        try:
            # Fallback to QuanSun model
            eva_model_name = "QuanSun/EVA-CLIP" 
            eva_model = AutoModel.from_pretrained(eva_model_name, trust_remote_code=True)
            eva_processor = CLIPImageProcessor.from_pretrained("openai/clip-vit-large-patch14")
            
            eva_model.to(device)
            eva_model.eval()
            
            print("‚úÖ QuanSun/EVA-CLIP loaded successfully")
            return eva_processor, eva_model
            
        except Exception as e2:
            print(f"‚ùå Failed to load any EVA-CLIP model: {e2}")
            print("üîß Using CLIP ViT-L/14 as fallback for EVA-CLIP")
            
            # Fallback: use regular CLIP
            fallback_processor = CLIPProcessor.from_pretrained("openai/clip-vit-large-patch14")
            fallback_model = CLIPModel.from_pretrained("openai/clip-vit-large-patch14")
            fallback_model.to(device)
            fallback_model.eval()
            
            return fallback_processor, fallback_model

def extract_clip_embeddings(images, processor, model, device):
    """Extract CLIP embeddings from PIL images"""
    embeddings = []
    
    for img in images:
        try:
            # Process image
            inputs = processor(images=img, return_tensors="pt")
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # Get embeddings
            with torch.no_grad():
                features = model.get_image_features(**inputs)
                # Normalize
                features = features / features.norm(p=2, dim=-1, keepdim=True)
                embeddings.append(features.squeeze().cpu())
        
        except Exception as e:
            print(f"‚ö†Ô∏è Error processing image: {e}")
            # Fallback: zero embedding
            embeddings.append(torch.zeros(768))
    
    return torch.stack(embeddings)

def extract_eva_embeddings(images, processor, model, device):
    """Extract EVA-CLIP embeddings from PIL images"""
    if model is None:
        print("‚ö†Ô∏è EVA-CLIP model not loaded, using random embeddings")
        batch_size = len(images)
        return torch.randn(batch_size, 768)
    
    embeddings = []
    
    for img in images:
        try:
            # Check if we're using EVA-CLIP or fallback CLIP
            if hasattr(model, 'get_image_features'):
                # Using regular CLIP as fallback
                inputs = processor(images=img, return_tensors="pt")
                inputs = {k: v.to(device) for k, v in inputs.items()}
                
                with torch.no_grad():
                    features = model.get_image_features(**inputs)
                    # Normalize
                    features = features / features.norm(p=2, dim=-1, keepdim=True)
                    embeddings.append(features.squeeze().cpu())
                    
            else:
                # Using actual EVA-CLIP model
                inputs = processor(images=img, return_tensors="pt")
                pixel_values = inputs['pixel_values'].to(device)
                
                with torch.no_grad():
                    # EVA-CLIP models typically have encode_image method
                    if hasattr(model, 'encode_image'):
                        features = model.encode_image(pixel_values)
                    elif hasattr(model, 'vision_model'):
                        # Alternative method for some EVA-CLIP variants
                        features = model.vision_model(pixel_values)[1]  # pooled output
                    else:
                        # Generic forward pass
                        outputs = model(pixel_values=pixel_values)
                        features = outputs.image_embeds if hasattr(outputs, 'image_embeds') else outputs.last_hidden_state.mean(dim=1)
                    
                    # Normalize
                    features = features / features.norm(p=2, dim=-1, keepdim=True)
                    embeddings.append(features.squeeze().cpu())
        
        except Exception as e:
            print(f"‚ö†Ô∏è Error processing image with EVA-CLIP: {e}")
            # Fallback: zero embedding
            embeddings.append(torch.zeros(768))
    
    return torch.stack(embeddings)

def main():
    """Main embedding extraction function"""
    print("üöÄ Simple Embedding Extraction")
    print("=" * 50)
    
    # Setup
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"üîß Using device: {device}")
    
    # Load models
    print("\nüì¶ Loading models...")
    clip_processor, clip_model = load_clip_model(device)
    eva_processor, eva_model = load_eva_clip_model(device)
    
    # Check which EVA model was loaded
    eva_model_info = "Unknown EVA model"
    if eva_model is not None:
        if hasattr(eva_model, 'config') and hasattr(eva_model.config, 'name_or_path'):
            eva_model_info = eva_model.config.name_or_path
        elif hasattr(eva_model, 'get_image_features'):
            eva_model_info = "CLIP ViT-L/14 (fallback)"
        else:
            eva_model_info = "EVA-CLIP (loaded successfully)"
    
    print(f"üîç Using EVA model: {eva_model_info}")
    model = load_clip_model(device)
    eva_processor, eva_model = load_eva_clip_model(device)
    
    # Load dataset
    print("\nüìÇ Loading dataset...")
    tar_paths = ["./data/00000.tar"]
    dataset = BLIP3oWebDataset(
        tar_paths=tar_paths,
        batch_size=32,  # Process 32 images at a time
        shuffle=False,  # Don't shuffle for consistent results
        num_workers=2
    )
    
    dataloader = dataset.get_dataloader()
    
    # Extract embeddings
    print("\nüß† Extracting embeddings...")
    
    all_eva_embeddings = []
    all_clip_embeddings = []
    all_captions = []
    all_keys = []
    
    total_samples = 0
    
    for batch_idx, batch in enumerate(tqdm(dataloader, desc="Processing batches")):
        images = batch['image']
        captions = batch['caption']
        keys = batch['key']
        
        # Extract CLIP embeddings
        clip_embeddings = extract_clip_embeddings(images, clip_processor, clip_model, device)
        
        # Extract EVA embeddings (placeholder)
        eva_embeddings = extract_eva_embeddings(images, eva_processor, eva_model, device)
        
        # Store results
        all_clip_embeddings.append(clip_embeddings)
        all_eva_embeddings.append(eva_embeddings)
        all_captions.extend(captions)
        all_keys.extend(keys)
        
        total_samples += len(images)
        
        # Print progress
        if batch_idx % 10 == 0:
            print(f"   Processed {total_samples} samples...")
    
    # Combine all embeddings
    print(f"\nüìä Combining {total_samples} embeddings...")
    final_eva_embeddings = torch.cat(all_eva_embeddings, dim=0)
    final_clip_embeddings = torch.cat(all_clip_embeddings, dim=0)
    
    print(f"‚úÖ Final shapes:")
    print(f"   EVA embeddings: {final_eva_embeddings.shape}")
    print(f"   CLIP embeddings: {final_clip_embeddings.shape}")
    
    # Save embeddings
    print(f"\nüíæ Saving embeddings...")
    
    embeddings_data = {
        'eva_embeddings': final_eva_embeddings,
        'clip_embeddings': final_clip_embeddings,
        'captions': all_captions,
        'keys': all_keys,
        'total_samples': total_samples
    }
    
    # Create output directory
    os.makedirs('embeddings', exist_ok=True)
    
    # Save to file
    output_file = 'embeddings/blip3o_embeddings.pkl'
    with open(output_file, 'wb') as f:
        pickle.dump(embeddings_data, f)
    
    file_size_mb = os.path.getsize(output_file) / (1024 * 1024)
    print(f"‚úÖ Embeddings saved to: {output_file}")
    print(f"üìä File size: {file_size_mb:.2f} MB")
    
    # Test loading
    print(f"\nüß™ Testing loading...")
    with open(output_file, 'rb') as f:
        loaded_data = pickle.load(f)
    
    print(f"‚úÖ Loading test successful!")
    print(f"   üìä Loaded EVA embeddings: {loaded_data['eva_embeddings'].shape}")
    print(f"   üìä Loaded CLIP embeddings: {loaded_data['clip_embeddings'].shape}")
    print(f"   üìù Sample caption: {loaded_data['captions'][0]}")
    
    print(f"\nüéâ Embedding extraction complete!")
    print(f"üìÅ Your embeddings are saved in: {output_file}")
    print(f"üî¢ Total samples processed: {total_samples}")
    
    print(f"\nüí° Next steps:")
    print(f"   1. ‚úÖ EVA-CLIP and CLIP embeddings extracted!")
    print(f"   2. Test the saved embeddings")
    print(f"   3. Build flow matching model on these embeddings")
    print(f"   4. Train EVA‚ÜíCLIP flow matching pipeline")

if __name__ == "__main__":
    main()