============================================================================================== 
Warning! Mixing Conda and module environments may lead to corruption of the
user environment. 
We do not recommend users mixing those two environments unless absolutely
necessary. Note that 
SURF does not provide any support for Conda environment.
For more information, please refer to our software policy page:
https://servicedesk.surf.nl/wiki/display/WIKI/Software+policy+Snellius#SoftwarepolicySnellius-UseofAnacondaandMinicondaenvironmentsonSnellius 

Remember that many packages have already been installed on the system and can
be loaded using 
the 'module load <package__name>' command. If you are uncertain if a package is
already available 
on the system, please use 'module avail' or 'module spider' to search for it.
============================================================================================== 
=== Testing Single GPU ===
/gpfs/home1/scur2711/eva-clip-flow-matching/eva-clip-v3/test_multi_gpu.py:95: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=True):
/gpfs/home1/scur2711/eva-clip-flow-matching/eva-clip-v3/src/modules/trainers/blip3o_trainer.py:73: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `BLIP3oTrainer.__init__`. Use `processing_class` instead.
  super().__init__(
ðŸ§ª Testing Multi-GPU BLIP3-o Setup
==================================================
Rank 0: Setting up distributed training...
  Local rank: 0
  Global rank: 0
  World size: 1
  CUDA available: True
  CUDA device count: 3
ðŸ”§ Environment Info:
  PyTorch version: 2.7.1+cu126
  CUDA version: 12.6
  Device: cuda:0
  GPU name: NVIDIA H100
  GPU memory: 100.0 GB

ðŸ§ª Test 1: Model Creation (Rank 0)
âœ… Final 3D RoPE compatible config:
   dim=256, n_heads=4, head_dim=64
   head_dim % 4 = 0 (must be 0)
âœ… Validated 256-token configuration: 16x16 = 256 tokens
âœ… Final 3D RoPE compatible config:
   dim=512, n_heads=8, head_dim=64
   head_dim % 4 = 0 (must be 0)
âœ… Validated 256-token configuration: 16x16 = 256 tokens
âœ… Final 3D RoPE compatible config:
   dim=768, n_heads=12, head_dim=64
   head_dim % 4 = 0 (must be 0)
âœ… Validated 256-token configuration: 16x16 = 256 tokens
âœ… Final 3D RoPE compatible config:
   dim=768, n_heads=12, head_dim=64
   head_dim % 4 = 0 (must be 0)
âœ… Validated 256-token configuration: 16x16 = 256 tokens
Creating tiny model: dim=256, layers=4
ðŸ”§ Model configured for:
   CLIP dimension: 1024
   EVA-CLIP dimension: 4096
   Hidden dimension: 256
   Grid: 16x16 = 256 tokens
   ðŸ“Š Using 256 tokens (16x16 grid) - updated format
ðŸ”§ Model configured for 256 tokens (16x16 grid)
âœ… Head dimension 64 is compatible with 3D RoPE
âœ… Token embedder initialized for 256 tokens
âœ… BLIP3-o DiT model with FIXED 3D RoPE initialized
   Parameters: 8,619,520
   Final dimensions: dim=256, heads=4, head_dim=64
   Tokens: 256 (16x16 grid)
   3D RoPE compatible: head_dim % 4 = 0
âœ… Model created successfully: 8,619,520 parameters
âœ… Loss function created successfully

ðŸ§ª Test 2: Forward Pass (Rank 0)
Testing forward pass with batch_size=2
  EVA shape: torch.Size([2, 256, 4096])
  CLIP shape: torch.Size([2, 256, 1024])
  Timesteps shape: torch.Size([2])
âœ… Forward pass successful
  Output shape: torch.Size([2, 256, 1024])
âœ… Loss computation successful
  Loss: 3.3885
  Cosine similarity: 0.0012

ðŸ§ª Test 3: Trainer Creation
âœ… Trainer created successfully

âœ… All tests passed! Multi-GPU setup is working correctly.
ðŸš€ Ready for full training with the fixed script.
=== Testing Multi-GPU ===
W0715 14:34:04.083000 1879578 /gpfs/home1/scur2711/.conda/envs/eva_clip_env/lib/python3.13/site-packages/torch/distributed/run.py:766] 
W0715 14:34:04.083000 1879578 /gpfs/home1/scur2711/.conda/envs/eva_clip_env/lib/python3.13/site-packages/torch/distributed/run.py:766] *****************************************
W0715 14:34:04.083000 1879578 /gpfs/home1/scur2711/.conda/envs/eva_clip_env/lib/python3.13/site-packages/torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0715 14:34:04.083000 1879578 /gpfs/home1/scur2711/.conda/envs/eva_clip_env/lib/python3.13/site-packages/torch/distributed/run.py:766] *****************************************
ðŸ§ª Testing Multi-GPU BLIP3-o Setup
==================================================
Rank 1: Setting up distributed training...
  Local rank: 1
  Global rank: 1
  World size: 3
ðŸ§ª Testing Multi-GPU BLIP3-o Setup
==================================================
Rank 2: Setting up distributed training...
  Local rank: 2
  Global rank: 2
  World size: 3
ðŸ§ª Testing Multi-GPU BLIP3-o Setup
==================================================
Rank 0: Setting up distributed training...
  Local rank: 0
  Global rank: 0
  World size: 3
  CUDA available: True
  CUDA available: True
  CUDA available: True
  CUDA device count: 3
  CUDA device count: 3
  CUDA device count: 3
ðŸ”§ Environment Info:
  PyTorch version: 2.7.1+cu126
  CUDA version: 12.6
  Device: cuda:0
  GPU name: NVIDIA H100
  GPU memory: 100.0 GB

ðŸ§ª Test 1: Model Creation (Rank 0)

ðŸ§ª Test 1: Model Creation (Rank 1)

ðŸ§ª Test 1: Model Creation (Rank 2)
âœ… Final 3D RoPE compatible config:
   dim=256, n_heads=4, head_dim=64
   head_dim % 4 = 0 (must be 0)
âœ… Validated 256-token configuration: 16x16 = 256 tokens
âœ… Final 3D RoPE compatible config:
   dim=512, n_heads=8, head_dim=64
   head_dim % 4 = 0 (must be 0)
âœ… Validated 256-token configuration: 16x16 = 256 tokens
âœ… Final 3D RoPE compatible config:
   dim=768, n_heads=12, head_dim=64
   head_dim % 4 = 0 (must be 0)
âœ… Validated 256-token configuration: 16x16 = 256 tokens
âœ… Final 3D RoPE compatible config:
   dim=768, n_heads=12, head_dim=64
   head_dim % 4 = 0 (must be 0)
âœ… Validated 256-token configuration: 16x16 = 256 tokens
Creating tiny model: dim=256, layers=4
ðŸ”§ Model configured for:
   CLIP dimension: 1024
   EVA-CLIP dimension: 4096
   Hidden dimension: 256
   Grid: 16x16 = 256 tokens
   ðŸ“Š Using 256 tokens (16x16 grid) - updated format
ðŸ”§ Model configured for 256 tokens (16x16 grid)
âœ… Head dimension 64 is compatible with 3D RoPE
âœ… Token embedder initialized for 256 tokens
âœ… BLIP3-o DiT model with FIXED 3D RoPE initialized
   Parameters: 8,619,520
   Final dimensions: dim=256, heads=4, head_dim=64
   Tokens: 256 (16x16 grid)
   3D RoPE compatible: head_dim % 4 = 0
âœ… Model created successfully: 8,619,520 parameters
âœ… Loss function created successfully

ðŸ§ª Test 2: Forward Pass (Rank 0)
Testing forward pass with batch_size=2
  EVA shape: torch.Size([2, 256, 4096])
  CLIP shape: torch.Size([2, 256, 1024])
  Timesteps shape: torch.Size([2])
/gpfs/home1/scur2711/eva-clip-flow-matching/eva-clip-v3/test_multi_gpu.py:95: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=True):
âœ… Final 3D RoPE compatible config:
   dim=256, n_heads=4, head_dim=64
   head_dim % 4 = 0 (must be 0)
âœ… Validated 256-token configuration: 16x16 = 256 tokens
âœ… Final 3D RoPE compatible config:
   dim=512, n_heads=8, head_dim=64
   head_dim % 4 = 0 (must be 0)
âœ… Validated 256-token configuration: 16x16 = 256 tokens
âœ… Final 3D RoPE compatible config:
   dim=768, n_heads=12, head_dim=64
   head_dim % 4 = 0 (must be 0)
âœ… Validated 256-token configuration: 16x16 = 256 tokens
âœ… Final 3D RoPE compatible config:
   dim=768, n_heads=12, head_dim=64
   head_dim % 4 = 0 (must be 0)
âœ… Validated 256-token configuration: 16x16 = 256 tokens
Creating tiny model: dim=256, layers=4
ðŸ”§ Model configured for:
   CLIP dimension: 1024
   EVA-CLIP dimension: 4096
   Hidden dimension: 256
   Grid: 16x16 = 256 tokens
   ðŸ“Š Using 256 tokens (16x16 grid) - updated format
ðŸ”§ Model configured for 256 tokens (16x16 grid)
âœ… Head dimension 64 is compatible with 3D RoPE
âœ… Token embedder initialized for 256 tokens
âœ… Final 3D RoPE compatible config:
   dim=256, n_heads=4, head_dim=64
   head_dim % 4 = 0 (must be 0)
âœ… Validated 256-token configuration: 16x16 = 256 tokens
âœ… Final 3D RoPE compatible config:
   dim=512, n_heads=8, head_dim=64
   head_dim % 4 = 0 (must be 0)
âœ… Validated 256-token configuration: 16x16 = 256 tokens
âœ… Final 3D RoPE compatible config:
   dim=768, n_heads=12, head_dim=64
   head_dim % 4 = 0 (must be 0)
âœ… Validated 256-token configuration: 16x16 = 256 tokens
âœ… Final 3D RoPE compatible config:
   dim=768, n_heads=12, head_dim=64
   head_dim % 4 = 0 (must be 0)
âœ… Validated 256-token configuration: 16x16 = 256 tokens
Creating tiny model: dim=256, layers=4
ðŸ”§ Model configured for:
   CLIP dimension: 1024
   EVA-CLIP dimension: 4096
   Hidden dimension: 256
   Grid: 16x16 = 256 tokens
   ðŸ“Š Using 256 tokens (16x16 grid) - updated format
ðŸ”§ Model configured for 256 tokens (16x16 grid)
âœ… Head dimension 64 is compatible with 3D RoPE
âœ… Token embedder initialized for 256 tokensâœ… BLIP3-o DiT model with FIXED 3D RoPE initialized

   Parameters: 8,619,520
   Final dimensions: dim=256, heads=4, head_dim=64
   Tokens: 256 (16x16 grid)
   3D RoPE compatible: head_dim % 4 = 0
âœ… Model created successfully: 8,619,520 parameters
âœ… Loss function created successfully

ðŸ§ª Test 2: Forward Pass (Rank 2)
Testing forward pass with batch_size=2
  EVA shape: torch.Size([2, 256, 4096])
  CLIP shape: torch.Size([2, 256, 1024])
  Timesteps shape: torch.Size([2])
/gpfs/home1/scur2711/eva-clip-flow-matching/eva-clip-v3/test_multi_gpu.py:95: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=True):
âœ… BLIP3-o DiT model with FIXED 3D RoPE initialized
   Parameters: 8,619,520
   Final dimensions: dim=256, heads=4, head_dim=64
   Tokens: 256 (16x16 grid)
   3D RoPE compatible: head_dim % 4 = 0
âœ… Model created successfully: 8,619,520 parameters
âœ… Loss function created successfully

ðŸ§ª Test 2: Forward Pass (Rank 1)
Testing forward pass with batch_size=2
  EVA shape: torch.Size([2, 256, 4096])
  CLIP shape: torch.Size([2, 256, 1024])
  Timesteps shape: torch.Size([2])
/gpfs/home1/scur2711/eva-clip-flow-matching/eva-clip-v3/test_multi_gpu.py:95: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=True):
âœ… Forward pass successful
  Output shape: torch.Size([2, 256, 1024])
âœ… Loss computation successful
  Loss: 3.3946
  Cosine similarity: 0.0000

ðŸ§ª Test 3: Trainer Creation
/gpfs/home1/scur2711/eva-clip-flow-matching/eva-clip-v3/src/modules/trainers/blip3o_trainer.py:73: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `BLIP3oTrainer.__init__`. Use `processing_class` instead.
  super().__init__(
âœ… Trainer created successfully
/home/scur2711/.conda/envs/eva_clip_env/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank0]:[W715 14:34:13.081724759 ProcessGroupNCCL.cpp:4718] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
âœ… Forward pass successful
  Output shape: torch.Size([2, 256, 1024])
âœ… Loss computation successful
  Loss: 3.4017
  Cosine similarity: 0.0005
/home/scur2711/.conda/envs/eva_clip_env/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank2]:[W715 14:34:13.161924676 ProcessGroupNCCL.cpp:4718] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
âœ… Forward pass successful
  Output shape: torch.Size([2, 256, 1024])
âœ… Loss computation successful
  Loss: 3.4023
  Cosine similarity: -0.0007
/home/scur2711/.conda/envs/eva_clip_env/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank1]:[W715 14:34:13.244151107 ProcessGroupNCCL.cpp:4718] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.

âœ… All tests passed! Multi-GPU setup is working correctly.
ðŸš€ Ready for full training with the fixed script.
[rank0]:[W715 14:34:14.035071378 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

JOB STATISTICS
==============
Job ID: 13162544
Cluster: snellius
User/Group: scur2711/scur2711
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 48
CPU Utilized: 00:00:54
CPU Efficiency: 2.34% of 00:38:24 core-walltime
Job Wall-clock time: 00:00:48
Memory Utilized: 4.54 GB
Memory Efficiency: 0.84% of 540.00 GB (540.00 GB/node)
