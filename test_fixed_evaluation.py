#!/usr/bin/env python3
"""
FIXED Test script to verify the dual supervision model loading works correctly
"""

import sys
import torch
from pathlib import Path

# Add project root to path
script_dir = Path(__file__).parent
project_root = script_dir
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "src"))

def test_fixed_model_loading():
    """Test the fixed model loading with correct imports"""
    
    model_path = "/scratch-shared/scur2711/blip3o_workspace/checkpoints/blip3o_multi_gpu_fixed_cosine_13219643_20250719_081230"
    
    print("üß™ Testing FIXED model loading...")
    print("=" * 50)
    
    try:
        # Test basic imports first
        print("üì¶ Testing imports...")
        try:
            # Test config import
            from src.modules.config.blip3o_config import BLIP3oDiTConfig
            print("‚úÖ Config import successful")
            
            # Test standard loss import first
            from src.modules.losses.flow_matching_loss import (
                BLIP3oFlowMatchingLoss,
                create_blip3o_flow_matching_loss
            )
            print("‚úÖ Standard loss import successful")
            
            # Test dual supervision loss import
            from src.modules.losses.dual_supervision_flow_matching_loss import (
                DualSupervisionFlowMatchingLoss,
                create_dual_supervision_loss
            )
            print("‚úÖ Dual supervision loss import successful")
            
            # Test model import - this is the critical one
            from src.modules.models.dual_supervision_blip3o_dit import (
                DualSupervisionBLIP3oDiTModel,
                create_blip3o_dit_model
            )
            print("‚úÖ Dual supervision model import successful")
            
        except ImportError as e:
            print(f"‚ùå Import failed: {e}")
            print("\nüîß Debugging import issue...")
            
            # Check if files exist
            model_file = Path("src/modules/models/dual_supervision_blip3o_dit.py")
            loss_file = Path("src/modules/losses/dual_supervision_flow_matching_loss.py")
            
            print(f"Model file exists: {model_file.exists()}")
            print(f"Loss file exists: {loss_file.exists()}")
            
            if not model_file.exists():
                print("‚ùå Missing dual supervision model file!")
                return False
                
            if not loss_file.exists():
                print("‚ùå Missing dual supervision loss file!")
                return False
            
            # Try to import with different approach
            try:
                import importlib.util
                
                # Load model module
                spec = importlib.util.spec_from_file_location("dual_supervision_model", model_file)
                dual_model_module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(dual_model_module)
                
                DualSupervisionBLIP3oDiTModel = dual_model_module.DualSupervisionBLIP3oDiTModel
                create_blip3o_dit_model = dual_model_module.create_blip3o_dit_model
                
                print("‚úÖ Direct import successful")
                
            except Exception as e2:
                print(f"‚ùå Direct import also failed: {e2}")
                return False
        
        # Load config
        import json
        model_path_obj = Path(model_path)
        
        config_file = model_path_obj / "config.json"
        if not config_file.exists():
            config_file = model_path_obj / "blip3o_model_config.json"
        
        if not config_file.exists():
            print(f"‚ùå Config file not found in {model_path}")
            return False
        
        with open(config_file, 'r') as f:
            config_dict = json.load(f)
        
        config = BLIP3oDiTConfig(**config_dict)
        print(f"‚úÖ Loaded config with {len(config_dict)} parameters")
        
        # Create model
        print("üèóÔ∏è  Creating dual supervision model...")
        model = create_blip3o_dit_model(
            config=config,
            load_clip_projection=True,
            enable_dual_supervision=True,
        )
        print("‚úÖ Created dual supervision model")
        
        # Check for key components
        has_global_velocity_proj = hasattr(model, 'global_velocity_proj')
        has_frozen_clip_proj = hasattr(model, 'frozen_clip_visual_proj') and model.frozen_clip_visual_proj is not None
        has_global_adaptation_mlp = hasattr(model, 'global_adaptation_mlp')
        
        print("üîç Model capabilities:")
        print(f"   Has global velocity projection: {'‚úÖ' if has_global_velocity_proj else '‚ùå'}")
        print(f"   Has frozen CLIP projection: {'‚úÖ' if has_frozen_clip_proj else '‚ùå'}")
        print(f"   Has global adaptation MLP: {'‚úÖ' if has_global_adaptation_mlp else '‚ùå'}")
        
        if not has_global_velocity_proj:
            print("‚ùå CRITICAL: Missing global_velocity_proj - this is required for dual supervision!")
            return False
        
        # Load weights
        model_file = model_path_obj / "model.safetensors"
        if not model_file.exists():
            print(f"‚ùå Model file not found: {model_file}")
            return False
        
        from safetensors.torch import load_file
        state_dict = load_file(str(model_file))
        print(f"‚úÖ Loaded state dict with {len(state_dict)} keys")
        
        # Check for dual supervision keys
        dual_keys = [k for k in state_dict.keys() if 'global_velocity_proj' in k]
        if dual_keys:
            print(f"‚úÖ Found dual supervision keys: {dual_keys}")
        else:
            print("‚ùå No dual supervision keys found in checkpoint")
            print("‚ö†Ô∏è  This model was not trained with dual supervision")
            return False
        
        # Load weights into model
        missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
        print("‚úÖ Loaded weights into model")
        
        if missing_keys:
            print(f"‚ö†Ô∏è  Missing keys: {len(missing_keys)} (first 5: {missing_keys[:5]})")
        if unexpected_keys:
            print(f"‚ö†Ô∏è  Unexpected keys: {len(unexpected_keys)} (first 5: {unexpected_keys[:5]})")
        
        # Test generation
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = model.to(device)
        model.eval()
        
        # Create dummy EVA embeddings
        dummy_eva = torch.randn(2, 256, 4096, device=device)  # [batch, tokens, eva_dim]
        
        print("üß™ Testing generation...")
        
        try:
            with torch.no_grad():
                # Test different generation modes
                if hasattr(model, 'generate') and 'generation_mode' in str(model.generate.__code__.co_varnames):
                    print("   Testing global generation mode...")
                    generated = model.generate(
                        encoder_hidden_states=dummy_eva,
                        num_inference_steps=5,  # Quick test
                        generation_mode="global",
                        return_global_only=True,
                    )
                    print(f"   ‚úÖ Global generation: {generated.shape}")
                    
                else:
                    print("   Testing standard generation...")
                    generated = model.generate(
                        encoder_hidden_states=dummy_eva,
                        num_inference_steps=5,  # Quick test
                    )
                    print(f"   ‚úÖ Standard generation: {generated.shape}")
                    
                    # Convert to global if needed
                    if generated.dim() == 3 and generated.shape[1] == 256:
                        generated = generated.mean(dim=1)  # Average pool
                        if hasattr(model, 'frozen_clip_visual_proj') and model.frozen_clip_visual_proj is not None:
                            generated = model.frozen_clip_visual_proj(generated)
                        print(f"   ‚úÖ Converted to global: {generated.shape}")
            
            print("‚úÖ Generation test successful!")
            
        except Exception as e:
            print(f"‚ùå Generation test failed: {e}")
            import traceback
            traceback.print_exc()
            return False
        
        print("\nüéâ SUCCESS: Fixed model loading works correctly!")
        print("üéØ Your model has dual supervision and should achieve 50-70% recall!")
        print("\nüí° Next steps:")
        print("   1. Your evaluation script should now work")
        print("   2. Re-run comp_eval.py with your model")
        print("   3. You should see much better recall performance")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_evaluation_script():
    """Test that the evaluation script can import everything correctly"""
    print("\nüß™ Testing evaluation script imports...")
    
    try:
        # Test the main evaluation class import
        from comp_eval import FixedBLIP3oRecallEvaluator
        print("‚úÖ Evaluation script imports successful")
        
        # Test creating evaluator
        evaluator = FixedBLIP3oRecallEvaluator(device="cpu")
        print("‚úÖ Evaluator creation successful")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Evaluation script test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("üöÄ BLIP3-o Fixed Model and Evaluation Testing")
    print("=" * 60)
    
    # Test model loading
    model_success = test_fixed_model_loading()
    
    # Test evaluation script
    eval_success = test_evaluation_script()
    
    print("\n" + "=" * 60)
    print("üìä TEST RESULTS:")
    print(f"   Model loading: {'‚úÖ PASS' if model_success else '‚ùå FAIL'}")
    print(f"   Evaluation script: {'‚úÖ PASS' if eval_success else '‚ùå FAIL'}")
    
    if model_success and eval_success:
        print("\nüéâ ALL TESTS PASSED!")
        print("‚úÖ Ready to run evaluation with dual supervision model")
        print("\nüìã To run evaluation:")
        print("python comp_eval.py \\")
        print("  --coco_root ./data/coco \\")
        print("  --blip3o_model_path /scratch-shared/scur2711/blip3o_workspace/checkpoints/blip3o_multi_gpu_fixed_cosine_13219643_20250719_081230 \\")
        print("  --num_samples 1000 \\")
        print("  --generation_mode global \\")
        print("  --save_results results/recall_evaluation.json")
    else:
        print("\n‚ùå TESTS FAILED!")
        print("üí° Check the error messages above and fix the import issues")
    
    print("=" * 60)