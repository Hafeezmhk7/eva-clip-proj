#!/bin/bash
#SBATCH --job-name=blip3o_enhanced_training
#SBATCH --partition=gpu_h100
#SBATCH --nodes=1
#SBATCH --gpus=3
#SBATCH --cpus-per-gpu=18
#SBATCH --time=12:00:00
#SBATCH --mem=0
#SBATCH --output=./slurm_out/blip3o_enhanced_%j.out
#SBATCH --error=./slurm_out/blip3o_enhanced_%j.err


# =============================================================================
# ENVIRONMENT SETUP
# =============================================================================


cd $SLURM_SUBMIT_DIR


module purge
module load 2024
module load Miniconda3/24.7.1-0
module load CUDA/12.6.0
source activate eva_clip_env


# =============================================================================
# TEMP AND WORKSPACE SETUP
# =============================================================================


export BLIP3O_USER=$(whoami)
export BLIP3O_JOB_ID=${SLURM_JOB_ID}
export BLIP3O_WORKSPACE="/scratch-shared/${BLIP3O_USER}/blip3o_workspace"
export BLIP3O_EMBEDDINGS="${BLIP3O_WORKSPACE}/embeddings"
export BLIP3O_CHECKPOINTS="${BLIP3O_WORKSPACE}/checkpoints"


if [ ! -z "$TMPDIR" ]; then
    export BLIP3O_JOB_TEMP="$TMPDIR/blip3o_job_${BLIP3O_JOB_ID}"
else
    export BLIP3O_JOB_TEMP="/tmp/blip3o_job_${BLIP3O_USER}_${BLIP3O_JOB_ID}"
fi


mkdir -p "${BLIP3O_WORKSPACE}"/{embeddings,checkpoints,logs}
mkdir -p "${BLIP3O_JOB_TEMP}"/cache


export TORCH_HOME="${BLIP3O_JOB_TEMP}/cache/torch"
export HF_HOME="${BLIP3O_JOB_TEMP}/cache/huggingface"
export TRANSFORMERS_CACHE="${BLIP3O_JOB_TEMP}/cache/transformers"


# =============================================================================
# NCCL & CUDA ENV SETTINGS
# =============================================================================


export NCCL_IB_DISABLE=0
export NCCL_NET_GDR_LEVEL=3
export NCCL_P2P_LEVEL=NVL
export NCCL_ASYNC_ERROR_HANDLING=1
export NCCL_DEBUG=WARN
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512,expandable_segments:True
export OMP_NUM_THREADS=1
export TOKENIZERS_PARALLELISM=false
export CUDA_LAUNCH_BLOCKING=0


# =============================================================================
# LOCATE EMBEDDINGS
# =============================================================================

# Set the embedding directory path as requested
EMBEDDINGS_DIR="/scratch-shared/scur2711/blip3o_workspace/embeddings/chunked_256_tokens"


# =============================================================================
# OUTPUT DIRECTORY
# =============================================================================


TIMESTAMP=$(date +%Y%m%d_%H%M%S)
OUTPUT_DIR="./checkpoints/blip3o_enhanced_${SLURM_JOB_ID}_${TIMESTAMP}"
mkdir -p "$OUTPUT_DIR"
mkdir -p "./slurm_out"


# =============================================================================
# TRAINING CONFIG
# =============================================================================


TRAINING_MODE="patch_only"
MAX_TRAINING_SHARDS=1
MODEL_SIZE="base"
HIDDEN_SIZE=768
NUM_LAYERS=12
NUM_HEADS=12
NUM_EPOCHS=10
BATCH_SIZE=4
EVAL_BATCH_SIZE=2
LEARNING_RATE=1e-4
WEIGHT_DECAY=0.01
WARMUP_STEPS=50
GRADIENT_ACCUMULATION_STEPS=2
LR_SCHEDULER="cosine"
DATALOADER_NUM_WORKERS=2
LOGGING_STEPS=5
SAVE_STEPS=100


# =============================================================================
# LAUNCH TRAINING
# =============================================================================


python train_blip3o_enhanced.py \
    --chunked_embeddings_dir "$EMBEDDINGS_DIR" \
    --output_dir "$OUTPUT_DIR" \
    --training_mode "$TRAINING_MODE" \
    --max_training_shards $MAX_TRAINING_SHARDS \
    --overfitting_test \
    --enable_same_data_eval \
    --enable_detailed_eval \
    --same_data_eval_frequency 50 \
    --detailed_eval_frequency 200 \
    --max_eval_batches 20 \
    --model_size "$MODEL_SIZE" \
    --hidden_size $HIDDEN_SIZE \
    --num_layers $NUM_LAYERS \
    --num_heads $NUM_HEADS \
    --num_epochs $NUM_EPOCHS \
    --batch_size $BATCH_SIZE \
    --eval_batch_size $EVAL_BATCH_SIZE \
    --learning_rate $LEARNING_RATE \
    --weight_decay $WEIGHT_DECAY \
    --warmup_steps $WARMUP_STEPS \
    --gradient_accumulation_steps $GRADIENT_ACCUMULATION_STEPS \
    --lr_scheduler "$LR_SCHEDULER" \
    --normalize_targets \
    --prediction_type "velocity" \
    --fp16 \
    --dataloader_num_workers $DATALOADER_NUM_WORKERS \
    --logging_steps $LOGGING_STEPS \
    --save_steps $SAVE_STEPS \
    --detailed_logging \
    --test_gradient_flow


EXIT_CODE=$?


# =============================================================================
# CLEANUP
# =============================================================================


if [ -d "${BLIP3O_JOB_TEMP}/cache" ]; then
    rm -rf "${BLIP3O_JOB_TEMP}/cache"
fi


exit $EXIT_CODE
