#!/bin/bash
#SBATCH --job-name=blip3o_enhanced_training
#SBATCH --partition=gpu_h100
#SBATCH --nodes=1
#SBATCH --gpus=3
#SBATCH --cpus-per-gpu=18
#SBATCH --time=12:00:00
#SBATCH --mem=0
#SBATCH --output=./slurm_out/blip3o_enhanced_%j.out
#SBATCH --error=./slurm_out/blip3o_enhanced_%j.err

echo "üöÄ Enhanced BLIP3-o Training with CLS+Patch Support"
echo "================================================================"
echo "üéØ ENHANCED TRAINING FEATURES:"
echo "  ‚úÖ Support for both patch-only (256) and CLS+patch (257) modes"
echo "  ‚úÖ Flexible shard selection for training"
echo "  ‚úÖ Same-data evaluation (overfitting verification)"
echo "  ‚úÖ Pure flow matching loss (BLIP3-o paper aligned)"
echo "  ‚úÖ Detailed cosine similarity evaluation with plots"
echo "  ‚úÖ Comprehensive logging and progress tracking"
echo "================================================================"
echo "Job ID: $SLURM_JOB_ID"
echo "Node: $SLURMD_NODENAME"
echo "Date: $(date)"
echo "Working directory: $(pwd)"
echo "User: $(whoami)"
echo "GPUs requested: $SLURM_GPUS"
echo "CPUs per GPU: $SLURM_CPUS_PER_GPU"
echo "================================================================"

# =============================================================================
# ENVIRONMENT SETUP
# =============================================================================

echo ""
echo "üîß Enhanced Environment Setup..."
echo "==============================="

# Change to submit directory
cd $SLURM_SUBMIT_DIR
echo "‚úÖ Working directory: $(pwd)"

# Purge modules and load required ones
module purge
echo "üì¶ Loading modules..."

# Load modules
module load 2024
module load Miniconda3/24.7.1-0
module load CUDA/12.6.0

echo "‚úÖ Modules loaded successfully"

# Activate conda environment
source activate eva_clip_env
echo "‚úÖ Conda environment activated: $CONDA_DEFAULT_ENV"

# =============================================================================
# ENHANCED GPU ENVIRONMENT SETUP
# =============================================================================

echo ""
echo "üéÆ Enhanced GPU Environment Setup..."
echo "==================================="

# Print initial SLURM GPU allocation
echo "üìä SLURM GPU Variables:"
echo "  SLURM_GPUS: $SLURM_GPUS"
echo "  SLURM_GPUS_ON_NODE: $SLURM_GPUS_ON_NODE"
echo "  SLURM_LOCALID: $SLURM_LOCALID"
echo "  SLURM_PROCID: $SLURM_PROCID"
echo "  CUDA_VISIBLE_DEVICES: $CUDA_VISIBLE_DEVICES"

# Apply enhanced GPU fixes
echo ""
echo "üîß Applying enhanced GPU environment fixes..."

# Fix CUDA_VISIBLE_DEVICES if empty
if [ -z "$CUDA_VISIBLE_DEVICES" ] || [ "$CUDA_VISIBLE_DEVICES" = "" ]; then
    echo "‚ö†Ô∏è  CUDA_VISIBLE_DEVICES is empty - applying fix..."
    
    if [ ! -z "$SLURM_LOCALID" ]; then
        export CUDA_VISIBLE_DEVICES=$SLURM_LOCALID
        echo "‚úÖ Set CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES (from SLURM_LOCALID)"
    elif [ ! -z "$SLURM_GPUS" ]; then
        export CUDA_VISIBLE_DEVICES=$(seq -s, 0 $((SLURM_GPUS-1)))
        echo "‚úÖ Set CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES (from SLURM_GPUS)"
    fi
fi

# Enhanced NCCL settings for convergence optimization
echo ""
echo "üîß Optimizing enhanced NCCL settings..."

export NCCL_IB_DISABLE=0
export NCCL_NET_GDR_LEVEL=3
export NCCL_P2P_LEVEL=NVL
export NCCL_ASYNC_ERROR_HANDLING=1
export NCCL_DEBUG=WARN
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512,expandable_segments:True
export OMP_NUM_THREADS=1
export TOKENIZERS_PARALLELISM=false
export CUDA_LAUNCH_BLOCKING=0

echo "‚úÖ Enhanced GPU environment optimized"

# =============================================================================
# ENHANCED WORKSPACE SETUP
# =============================================================================

echo ""
echo "üìÅ Enhanced Workspace Setup..."
echo "=============================="

# Setup directories
export BLIP3O_USER=$(whoami)
export BLIP3O_JOB_ID=${SLURM_JOB_ID}

# Use scratch-shared for persistent storage
export BLIP3O_WORKSPACE="/scratch-shared/${BLIP3O_USER}/blip3o_workspace"
export BLIP3O_EMBEDDINGS="${BLIP3O_WORKSPACE}/embeddings"
export BLIP3O_CHECKPOINTS="${BLIP3O_WORKSPACE}/checkpoints"

# Job-specific temp directory
if [ ! -z "$TMPDIR" ]; then
    export BLIP3O_JOB_TEMP="$TMPDIR/blip3o_job_${BLIP3O_JOB_ID}"
else
    export BLIP3O_JOB_TEMP="/tmp/blip3o_job_${BLIP3O_USER}_${BLIP3O_JOB_ID}"
fi

# Create directories
echo "üìÅ Creating enhanced workspace directories..."
mkdir -p "${BLIP3O_WORKSPACE}"/{embeddings,checkpoints,logs,enhanced}
mkdir -p "${BLIP3O_JOB_TEMP}"/{cache,working,enhanced}

# Redirect model caches to job temp
export TORCH_HOME="${BLIP3O_JOB_TEMP}/cache/torch"
export HF_HOME="${BLIP3O_JOB_TEMP}/cache/huggingface"
export TRANSFORMERS_CACHE="${BLIP3O_JOB_TEMP}/cache/transformers"

mkdir -p "${TORCH_HOME}" "${HF_HOME}" "${TRANSFORMERS_CACHE}"

echo "‚úÖ Enhanced workspace configured:"
echo "  Workspace: ${BLIP3O_WORKSPACE}"
echo "  Job temp: ${BLIP3O_JOB_TEMP}"
echo "  Model cache: ${TORCH_HOME}"

# =============================================================================
# ENHANCED GPU DIAGNOSTICS
# =============================================================================

echo ""
echo "üß™ Enhanced GPU Diagnostics..."
echo "============================="

echo "üîç Python CUDA check:"
python -c "
import torch
print(f'PyTorch version: {torch.__version__}')
print(f'CUDA available: {torch.cuda.is_available()}')
if torch.cuda.is_available():
    print(f'CUDA version: {torch.version.cuda}')
    print(f'GPU count: {torch.cuda.device_count()}')
    for i in range(torch.cuda.device_count()):
        props = torch.cuda.get_device_properties(i)
        memory_gb = props.total_memory / (1024**3)
        print(f'  GPU {i}: {props.name} ({memory_gb:.1f} GB)')
        
        try:
            test_tensor = torch.randn(100, 100, device=f'cuda:{i}')
            print(f'    ‚úÖ GPU {i} accessible and working')
        except Exception as e:
            print(f'    ‚ùå GPU {i} error: {e}')
else:
    print('‚ùå CUDA not available!')
"

echo ""
echo "üîç nvidia-smi check:"
if command -v nvidia-smi &> /dev/null; then
    nvidia-smi --query-gpu=index,name,memory.total,memory.used --format=csv,noheader,nounits
else
    echo "‚ùå nvidia-smi not available"
fi

# =============================================================================
# FIND EMBEDDINGS
# =============================================================================

echo ""
echo "üîç Finding embeddings for enhanced training..."
echo "=============================================="

EMBEDDINGS_DIR=""

SEARCH_LOCATIONS=(
    "${BLIP3O_EMBEDDINGS}"
    "./embeddings"
    "./embeddings/chunked_256_tokens"
    "./embeddings/chunked_257_tokens"
)

for location in "${SEARCH_LOCATIONS[@]}"; do
    if [ -d "$location" ] && [ -f "$location/embeddings_manifest.json" ]; then
        EMBEDDINGS_DIR="$location"
        echo "‚úÖ Found embeddings: $EMBEDDINGS_DIR"
        break
    fi
done

if [ -z "$EMBEDDINGS_DIR" ]; then
    echo "‚ùå No embeddings found!"
    echo "üí° Searched locations:"
    for location in "${SEARCH_LOCATIONS[@]}"; do
        echo "    $location"
    done
    echo ""
    echo "üí° Please run embedding extraction first:"
    echo "    python src/modules/extract_embeddings_g.py --include_cls"
    exit 1
fi

# Verify embeddings
MANIFEST_FILE="${EMBEDDINGS_DIR}/embeddings_manifest.json"
if [ -f "$MANIFEST_FILE" ]; then
    echo "üìä Dataset info:"
    python -c "
import json
try:
    with open('$MANIFEST_FILE') as f:
        manifest = json.load(f)
    print(f'  Total shards: {manifest.get(\"total_shards\", \"unknown\")}')
    print(f'  Total samples: {manifest.get(\"total_samples\", \"unknown\"):,}')
    print(f'  Extraction mode: {manifest.get(\"extraction_mode\", \"unknown\")}')
    print(f'  Tokens per sample: {manifest.get(\"tokens_per_sample\", \"unknown\")}')
    print(f'  Format: {manifest.get(\"format_version\", \"unknown\")}')
except Exception as e:
    print(f'  Error reading manifest: {e}')
"
else
    echo "‚ùå Manifest file missing: $MANIFEST_FILE"
    exit 1
fi

# =============================================================================
# CREATE ENHANCED OUTPUT DIRECTORY
# =============================================================================

echo ""
echo "üìÇ Setting up enhanced output directory..."
echo "========================================="

TIMESTAMP=$(date +%Y%m%d_%H%M%S)
OUTPUT_DIR="./checkpoints/blip3o_enhanced_${SLURM_JOB_ID}_${TIMESTAMP}"

mkdir -p "$OUTPUT_DIR"
mkdir -p "./slurm_out"

echo "‚úÖ Enhanced output directory: $OUTPUT_DIR"

# =============================================================================
# ENHANCED TRAINING CONFIGURATION
# =============================================================================

echo ""
echo "‚öôÔ∏è Enhanced Training Configuration..."
echo "===================================="

# Configuration options
TRAINING_MODE="cls_patch"           # "cls_patch" (257 tokens) or "patch_only" (256 tokens)
MAX_TRAINING_SHARDS=1               # Use single shard for overfitting test
OVERFITTING_TEST="--overfitting_test"  # Enable overfitting test
ENABLE_SAME_DATA_EVAL="--enable_same_data_eval"
ENABLE_DETAILED_EVAL="--enable_detailed_eval"

# Model configuration
MODEL_SIZE="base"
HIDDEN_SIZE=768
NUM_LAYERS=12
NUM_HEADS=12

# Training hyperparameters
NUM_EPOCHS=10
BATCH_SIZE=4
EVAL_BATCH_SIZE=2
LEARNING_RATE=1e-4
WEIGHT_DECAY=0.01
WARMUP_STEPS=50
GRADIENT_ACCUMULATION_STEPS=2
LR_SCHEDULER="cosine"

# Hardware settings
FP16="--fp16"
DATALOADER_NUM_WORKERS=2

# Evaluation settings
SAME_DATA_EVAL_FREQUENCY=50
DETAILED_EVAL_FREQUENCY=200
MAX_EVAL_BATCHES=20

# Logging
LOGGING_STEPS=5
SAVE_STEPS=100
DETAILED_LOGGING="--detailed_logging"

echo "Enhanced Training Configuration:"
echo "  üéØ Training mode: ${TRAINING_MODE}"
echo "  üì¶ Max training shards: ${MAX_TRAINING_SHARDS} (overfitting test)"
echo "  üèóÔ∏è  Model: ${MODEL_SIZE} (${HIDDEN_SIZE}D, ${NUM_LAYERS}L, ${NUM_HEADS}H)"
echo "  üìö Epochs: $NUM_EPOCHS"
echo "  üì¶ Batch size per GPU: $BATCH_SIZE"
echo "  üìà Learning rate: $LEARNING_RATE"
echo "  üîÑ Same-data evaluation: Yes (every ${SAME_DATA_EVAL_FREQUENCY} steps)"
echo "  üìä Detailed evaluation: Yes (every ${DETAILED_EVAL_FREQUENCY} steps)"
echo "  üîß Features: Overfitting test, pure flow matching, detailed analysis"

# =============================================================================
# ENHANCED FINAL VERIFICATION
# =============================================================================

echo ""
echo "üîç Enhanced final verification..."
echo "==============================="

# Verify enhanced Python imports
echo "üêç Verifying enhanced BLIP3-o modules..."
python -c "
import sys
from pathlib import Path
sys.path.insert(0, str(Path.cwd() / 'src'))

try:
    from src.modules.models.blip3o_patch_dit import create_blip3o_patch_dit_model, BLIP3oDiTConfig
    print('‚úÖ Enhanced patch DiT model')
    
    from src.modules.losses.blip3o_flow_matching_loss import create_blip3o_flow_matching_loss
    print('‚úÖ Pure flow matching loss')
    
    from src.modules.trainers.blip3o_flexible_trainer import BLIP3oFlexibleTrainer, create_blip3o_flexible_training_args
    print('‚úÖ Flexible trainer')
    
    from src.modules.datasets.blip3o_dataset import create_flexible_dataloaders
    print('‚úÖ Flexible dataset')
    
    from src.modules.evaluation.blip3o_detailed_evaluator import create_detailed_evaluator
    print('‚úÖ Detailed evaluator')
    
    print('üéâ All enhanced modules ready!')
    
except ImportError as e:
    print(f'‚ùå Import error: {e}')
    sys.exit(1)
"

if [ $? -ne 0 ]; then
    echo "‚ùå Enhanced module verification failed!"
    exit 1
fi

# =============================================================================
# START ENHANCED TRAINING
# =============================================================================

echo ""
echo "üöÄ Starting Enhanced BLIP3-o Training..."
echo "========================================"
echo "üéØ ENHANCED TRAINING FEATURES:"
echo "  ‚Ä¢ Support for CLS+patch (257 tokens) and patch-only (256 tokens)"
echo "  ‚Ä¢ Flexible shard selection for controlled training"
echo "  ‚Ä¢ Same-data evaluation for overfitting verification"
echo "  ‚Ä¢ Pure flow matching loss (BLIP3-o paper aligned)"
echo "  ‚Ä¢ Detailed cosine similarity evaluation with plots"
echo "  ‚Ä¢ Comprehensive logging and progress tracking"
echo ""
echo "üìà ENHANCED EXPECTED RESULTS:"
echo "  ‚Ä¢ Overfitting test with single shard"
echo "  ‚Ä¢ High cosine similarity on training data"
echo "  ‚Ä¢ Detailed per-patch and per-image analysis"
echo "  ‚Ä¢ Comprehensive evaluation plots and JSON reports"
echo "  ‚Ä¢ Verification of training pipeline effectiveness"
echo ""

TRAIN_START_TIME=$(date +%s)

echo "üî• Launching enhanced training mode..."

# ENHANCED: Use enhanced training script with all features
python train_blip3o_enhanced.py \
    --chunked_embeddings_dir "$EMBEDDINGS_DIR" \
    --output_dir "$OUTPUT_DIR" \
    --training_mode "$TRAINING_MODE" \
    --max_training_shards $MAX_TRAINING_SHARDS \
    $OVERFITTING_TEST \
    $ENABLE_SAME_DATA_EVAL \
    $ENABLE_DETAILED_EVAL \
    --same_data_eval_frequency $SAME_DATA_EVAL_FREQUENCY \
    --detailed_eval_frequency $DETAILED_EVAL_FREQUENCY \
    --max_eval_batches $MAX_EVAL_BATCHES \
    --model_size "$MODEL_SIZE" \
    --hidden_size $HIDDEN_SIZE \
    --num_layers $NUM_LAYERS \
    --num_heads $NUM_HEADS \
    --num_epochs $NUM_EPOCHS \
    --batch_size $BATCH_SIZE \
    --eval_batch_size $EVAL_BATCH_SIZE \
    --learning_rate $LEARNING_RATE \
    --weight_decay $WEIGHT_DECAY \
    --warmup_steps $WARMUP_STEPS \
    --gradient_accumulation_steps $GRADIENT_ACCUMULATION_STEPS \
    --lr_scheduler "$LR_SCHEDULER" \
    --normalize_targets \
    --prediction_type "velocity" \
    $FP16 \
    --dataloader_num_workers $DATALOADER_NUM_WORKERS \
    --logging_steps $LOGGING_STEPS \
    --save_steps $SAVE_STEPS \
    $DETAILED_LOGGING \
    --test_gradient_flow

TRAINING_EXIT_CODE=$?
TRAIN_END_TIME=$(date +%s)
TRAIN_DURATION=$((TRAIN_END_TIME - TRAIN_START_TIME))

echo ""
echo "üèÅ ENHANCED TRAINING COMPLETED"
echo "============================="
echo "Exit code: $TRAINING_EXIT_CODE"
echo "Duration: $TRAIN_DURATION seconds ($(($TRAIN_DURATION / 60)) minutes)"

# =============================================================================
# ENHANCED RESULTS AND CLEANUP
# =============================================================================

if [ $TRAINING_EXIT_CODE -eq 0 ]; then
    echo ""
    echo "‚úÖ SUCCESS! Enhanced BLIP3-o training completed!"
    echo "=============================================="
    
    # Copy to persistent storage
    if [ -d "$OUTPUT_DIR" ]; then
        PERSISTENT_DIR="${BLIP3O_CHECKPOINTS}/$(basename $OUTPUT_DIR)"
        mkdir -p "$PERSISTENT_DIR"
        
        echo "üìÅ Copying enhanced model to persistent storage..."
        if cp -r "$OUTPUT_DIR"/* "$PERSISTENT_DIR/" 2>/dev/null; then
            echo "‚úÖ Enhanced model copied to: $PERSISTENT_DIR"
        else
            echo "‚ö†Ô∏è  Copy failed, model remains at: $OUTPUT_DIR"
        fi
    fi
    
    echo ""
    echo "üéâ ENHANCED TRAINING SUCCESS!"
    echo "============================"
    echo "‚úÖ Training mode: $TRAINING_MODE"
    echo "‚úÖ Overfitting test completed with $MAX_TRAINING_SHARDS shard(s)"
    echo "‚úÖ Same-data evaluation performed"
    echo "‚úÖ Detailed cosine similarity analysis completed"
    echo "‚úÖ Pure flow matching loss (BLIP3-o paper aligned)"
    echo "‚úÖ Comprehensive evaluation plots and JSON reports generated"
    echo "‚úÖ Enhanced model ready for analysis"
    echo ""
    echo "üìÅ Enhanced model locations:"
    echo "   Primary: $OUTPUT_DIR"
    if [ -d "$PERSISTENT_DIR" ]; then
        echo "   Persistent: $PERSISTENT_DIR"
    fi
    echo ""
    echo "üß™ Next steps for enhanced analysis:"
    echo "   1. Check evaluation results in output directory"
    echo "   2. Analyze cosine similarity plots"
    echo "   3. Review JSON evaluation reports"
    echo "   4. Run additional evaluation: python eval_blip3o_patch_similarity.py"
    echo "   5. Compare different training modes (cls_patch vs patch_only)"
    
else
    echo ""
    echo "‚ùå ENHANCED TRAINING FAILED"
    echo "=========================="
    echo "Exit code: $TRAINING_EXIT_CODE"
    echo ""
    echo "üîç Enhanced Debugging Information:"
    echo "================================"
    echo "1. Check error log: ${SLURM_JOB_ID}.err"
    echo "2. Check training logs in: $OUTPUT_DIR"
    echo "3. Check GPU diagnostics output above"
    echo "4. Check enhanced training error file: blip3o_enhanced_training_error.json"
    echo ""
    echo "üí° Enhanced solutions:"
    echo "  ‚Ä¢ GPU allocation: Check SLURM GPU request"
    echo "  ‚Ä¢ Memory issues: Reduce batch size or use fewer shards"
    echo "  ‚Ä¢ Module loading: Check conda environment"
    echo "  ‚Ä¢ Embeddings: Verify embeddings exist and are accessible"
    echo "  ‚Ä¢ Enhanced code: Ensure all enhanced modules are working"
    echo "  ‚Ä¢ Dependencies: Check enhanced module imports"
fi

# Enhanced cleanup
echo ""
echo "üßπ Enhanced Cleanup..."
echo "===================="

if [ -d "${BLIP3O_JOB_TEMP}/cache" ]; then
    CACHE_SIZE=$(du -sh "${BLIP3O_JOB_TEMP}/cache" 2>/dev/null | cut -f1)
    rm -rf "${BLIP3O_JOB_TEMP}/cache" 2>/dev/null
    echo "‚úÖ Cleaned up enhanced model cache ($CACHE_SIZE)"
fi

echo ""
echo "üèÅ Enhanced job completed at $(date)"
echo "Total runtime: $SECONDS seconds"
echo "================================================================"

exit $TRAINING_EXIT_CODE