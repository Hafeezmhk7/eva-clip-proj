#!/bin/bash
#SBATCH --job-name=blip3o_patch_training
#SBATCH --partition=gpu_h100
#SBATCH --nodes=1
#SBATCH --gpus=3
#SBATCH --cpus-per-gpu=18
#SBATCH --time=12:00:00
#SBATCH --mem=0
#SBATCH --output=./slurm_out/blip3o_patch_%j.out
#SBATCH --error=./slurm_out/blip3o_patch_%j.err

echo "üöÄ BLIP3-o Patch-Level DiT Training - Aligned with Paper"
echo "================================================================"
echo "üéØ TRAINING FEATURES:"
echo "  ‚úÖ 256-token patch-level flow matching"
echo "  ‚úÖ EVA-CLIP conditioning (4096-dim)"
echo "  ‚úÖ CLIP output supervision (1024-dim)"
echo "  ‚úÖ Image-to-text recall evaluation"
echo "  ‚úÖ 3D Rotary Position Embedding"
echo "  ‚úÖ Enhanced multi-GPU support"
echo "================================================================"
echo "Job ID: $SLURM_JOB_ID"
echo "Node: $SLURMD_NODENAME"
echo "Date: $(date)"
echo "Working directory: $(pwd)"
echo "User: $(whoami)"
echo "GPUs requested: $SLURM_GPUS"
echo "CPUs per GPU: $SLURM_CPUS_PER_GPU"
echo "================================================================"

# =============================================================================
# ENVIRONMENT SETUP
# =============================================================================

echo ""
echo "üîß Environment Setup..."
echo "======================="

# Change to submit directory
cd $SLURM_SUBMIT_DIR
echo "‚úÖ Working directory: $(pwd)"

# Purge modules and load required ones
module purge
echo "üì¶ Loading modules..."

# Load modules
module load 2024
module load Miniconda3/24.7.1-0
module load CUDA/12.6.0

echo "‚úÖ Modules loaded successfully"

# Activate conda environment
source activate eva_clip_env
echo "‚úÖ Conda environment activated: $CONDA_DEFAULT_ENV"

# =============================================================================
# GPU ENVIRONMENT SETUP
# =============================================================================

echo ""
echo "üéÆ GPU Environment Setup..."
echo "=========================="

# Print initial SLURM GPU allocation
echo "üìä SLURM GPU Variables:"
echo "  SLURM_GPUS: $SLURM_GPUS"
echo "  SLURM_GPUS_ON_NODE: $SLURM_GPUS_ON_NODE"
echo "  SLURM_LOCALID: $SLURM_LOCALID"
echo "  SLURM_PROCID: $SLURM_PROCID"
echo "  CUDA_VISIBLE_DEVICES: $CUDA_VISIBLE_DEVICES"

# Apply GPU fixes
echo ""
echo "üîß Applying GPU environment fixes..."

# Fix CUDA_VISIBLE_DEVICES if empty
if [ -z "$CUDA_VISIBLE_DEVICES" ] || [ "$CUDA_VISIBLE_DEVICES" = "" ]; then
    echo "‚ö†Ô∏è  CUDA_VISIBLE_DEVICES is empty - applying fix..."
    
    if [ ! -z "$SLURM_LOCALID" ]; then
        export CUDA_VISIBLE_DEVICES=$SLURM_LOCALID
        echo "‚úÖ Set CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES (from SLURM_LOCALID)"
    elif [ ! -z "$SLURM_GPUS" ]; then
        export CUDA_VISIBLE_DEVICES=$(seq -s, 0 $((SLURM_GPUS-1)))
        echo "‚úÖ Set CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES (from SLURM_GPUS)"
    fi
fi

# Optimize NCCL settings for Snellius
echo ""
echo "üîß Optimizing NCCL settings for multi-GPU training..."

export NCCL_IB_DISABLE=0
export NCCL_NET_GDR_LEVEL=3
export NCCL_P2P_LEVEL=NVL
export NCCL_ASYNC_ERROR_HANDLING=1
export NCCL_DEBUG=WARN
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
export OMP_NUM_THREADS=1
export TOKENIZERS_PARALLELISM=false

echo "‚úÖ GPU environment optimized"

# =============================================================================
# WORKSPACE SETUP
# =============================================================================

echo ""
echo "üìÅ Workspace Setup..."
echo "===================="

# Setup directories
export BLIP3O_USER=$(whoami)
export BLIP3O_JOB_ID=${SLURM_JOB_ID}

# Use scratch-shared for persistent storage
export BLIP3O_WORKSPACE="/scratch-shared/${BLIP3O_USER}/blip3o_workspace"
export BLIP3O_EMBEDDINGS="${BLIP3O_WORKSPACE}/embeddings"
export BLIP3O_CHECKPOINTS="${BLIP3O_WORKSPACE}/checkpoints"

# Job-specific temp directory
if [ ! -z "$TMPDIR" ]; then
    export BLIP3O_JOB_TEMP="$TMPDIR/blip3o_job_${BLIP3O_JOB_ID}"
else
    export BLIP3O_JOB_TEMP="/tmp/blip3o_job_${BLIP3O_USER}_${BLIP3O_JOB_ID}"
fi

# Create directories
echo "üìÅ Creating workspace directories..."
mkdir -p "${BLIP3O_WORKSPACE}"/{embeddings,checkpoints,logs}
mkdir -p "${BLIP3O_JOB_TEMP}"/{cache,working}

# Redirect model caches to job temp
export TORCH_HOME="${BLIP3O_JOB_TEMP}/cache/torch"
export HF_HOME="${BLIP3O_JOB_TEMP}/cache/huggingface"
export TRANSFORMERS_CACHE="${BLIP3O_JOB_TEMP}/cache/transformers"

mkdir -p "${TORCH_HOME}" "${HF_HOME}" "${TRANSFORMERS_CACHE}"

echo "‚úÖ Workspace configured:"
echo "  Workspace: ${BLIP3O_WORKSPACE}"
echo "  Job temp: ${BLIP3O_JOB_TEMP}"
echo "  Model cache: ${TORCH_HOME}"

# =============================================================================
# GPU DIAGNOSTICS
# =============================================================================

echo ""
echo "üß™ GPU Diagnostics..."
echo "===================="

echo "üîç Python CUDA check:"
python -c "
import torch
print(f'PyTorch version: {torch.__version__}')
print(f'CUDA available: {torch.cuda.is_available()}')
if torch.cuda.is_available():
    print(f'CUDA version: {torch.version.cuda}')
    print(f'GPU count: {torch.cuda.device_count()}')
    for i in range(torch.cuda.device_count()):
        props = torch.cuda.get_device_properties(i)
        memory_gb = props.total_memory / (1024**3)
        print(f'  GPU {i}: {props.name} ({memory_gb:.1f} GB)')
        
        try:
            test_tensor = torch.randn(100, 100, device=f'cuda:{i}')
            print(f'    ‚úÖ GPU {i} accessible and working')
        except Exception as e:
            print(f'    ‚ùå GPU {i} error: {e}')
else:
    print('‚ùå CUDA not available!')
"

echo ""
echo "üîç nvidia-smi check:"
if command -v nvidia-smi &> /dev/null; then
    nvidia-smi --query-gpu=index,name,memory.total,memory.used --format=csv,noheader,nounits
else
    echo "‚ùå nvidia-smi not available"
fi

# =============================================================================
# FIND EMBEDDINGS
# =============================================================================

echo ""
echo "üîç Finding patch-level embeddings..."
echo "==================================="

EMBEDDINGS_DIR=""

SEARCH_LOCATIONS=(
    "${BLIP3O_EMBEDDINGS}/chunked_256_tokens"
    "${BLIP3O_EMBEDDINGS}"
    "./embeddings/chunked_256_tokens"
    "./embeddings"
)

for location in "${SEARCH_LOCATIONS[@]}"; do
    if [ -d "$location" ] && [ -f "$location/embeddings_manifest.json" ]; then
        EMBEDDINGS_DIR="$location"
        echo "‚úÖ Found embeddings: $EMBEDDINGS_DIR"
        break
    fi
done

if [ -z "$EMBEDDINGS_DIR" ]; then
    echo "‚ùå No patch-level embeddings found!"
    echo "üí° Searched locations:"
    for location in "${SEARCH_LOCATIONS[@]}"; do
        echo "    $location"
    done
    echo ""
    echo "üí° Please run embedding extraction first:"
    echo "    python src/modules/extract_embeddings_g.py"
    exit 1
fi

# Verify embeddings
MANIFEST_FILE="${EMBEDDINGS_DIR}/embeddings_manifest.json"
if [ -f "$MANIFEST_FILE" ]; then
    echo "üìä Dataset info:"
    python -c "
import json
try:
    with open('$MANIFEST_FILE') as f:
        manifest = json.load(f)
    print(f'  Total shards: {manifest.get(\"total_shards\", \"unknown\")}')
    print(f'  Total samples: {manifest.get(\"total_samples\", \"unknown\"):,}')
    print(f'  Format: {manifest.get(\"format_version\", \"unknown\")}')
    print(f'  Tokens per image: 256 (patch-level)')
    print(f'  CLIP dimension: 1024')
    print(f'  EVA dimension: 4096')
except Exception as e:
    print(f'  Error reading manifest: {e}')
"
else
    echo "‚ùå Manifest file missing: $MANIFEST_FILE"
    exit 1
fi

# =============================================================================
# CREATE OUTPUT DIRECTORY
# =============================================================================

echo ""
echo "üìÇ Setting up output directory..."
echo "================================"

TIMESTAMP=$(date +%Y%m%d_%H%M%S)
OUTPUT_DIR="./checkpoints/blip3o_patch_${SLURM_JOB_ID}_${TIMESTAMP}"

mkdir -p "$OUTPUT_DIR"
mkdir -p "./slurm_out"

echo "‚úÖ Output directory: $OUTPUT_DIR"

# =============================================================================
# TRAINING CONFIGURATION
# =============================================================================

echo ""
echo "‚öôÔ∏è Training Configuration..."
echo "============================"

# Patch-level training configuration
NUM_EPOCHS=6
BATCH_SIZE=8
EVAL_BATCH_SIZE=4
LEARNING_RATE=1e-4
MODEL_SIZE="base"
HIDDEN_SIZE=768
NUM_LAYERS=12
NUM_HEADS=12
GRADIENT_ACCUMULATION=4
WARMUP_STEPS=100

# Calculate effective batch size
ACTUAL_GPU_COUNT=$(python -c "import torch; print(torch.cuda.device_count() if torch.cuda.is_available() else 1)")
EFFECTIVE_BATCH_SIZE=$((BATCH_SIZE * ACTUAL_GPU_COUNT * GRADIENT_ACCUMULATION))

echo "Patch-Level Training Configuration:"
echo "  üéØ Target GPUs: ${SLURM_GPUS:-1}"
echo "  üîç Detected GPUs: $ACTUAL_GPU_COUNT"
echo "  üèóÔ∏è  Model: ${MODEL_SIZE} (${HIDDEN_SIZE}D, ${NUM_LAYERS}L, ${NUM_HEADS}H)"
echo "  üìö Epochs: $NUM_EPOCHS"
echo "  üì¶ Batch size per GPU: $BATCH_SIZE"
echo "  üîÑ Gradient accumulation: $GRADIENT_ACCUMULATION"
echo "  üìä Effective batch size: $EFFECTIVE_BATCH_SIZE"
echo "  üìà Learning rate: $LEARNING_RATE"
echo "  üî• Warmup steps: $WARMUP_STEPS"
echo "  üéØ Architecture: 256-token patch-level DiT"
echo "  üìê Input: EVA-CLIP (256√ó4096) ‚Üí CLIP (256√ó1024)"
echo "  üìä Evaluation: Image-to-text recall (R@1, R@5, R@10)"

# =============================================================================
# FINAL VERIFICATION
# =============================================================================

echo ""
echo "üîç Final verification..."
echo "======================="

# Verify Python imports
echo "üêç Verifying BLIP3-o patch modules..."
python -c "
import sys
from pathlib import Path
sys.path.insert(0, str(Path.cwd() / 'src'))

try:
    from src.modules.models.blip3o_patch_dit import BLIP3oPatchDiTModel
    print('‚úÖ Patch DiT model')
    
    from src.modules.losses.blip3o_flow_matching_loss import BLIP3oFlowMatchingLoss
    print('‚úÖ Flow matching loss')
    
    from src.modules.trainers.blip3o_patch_trainer import BLIP3oPatchTrainer
    print('‚úÖ Patch trainer')
    
    from src.modules.datasets.blip3o_dataset import BLIP3oEmbeddingDataset
    print('‚úÖ Patch dataset')
    
    from src.modules.evaluation.blip3o_recall_evaluator import BLIP3oRecallEvaluator
    print('‚úÖ Recall evaluator')
    
    print('üéâ All patch-level modules ready!')
    
except ImportError as e:
    print(f'‚ùå Import error: {e}')
    sys.exit(1)
"

if [ $? -ne 0 ]; then
    echo "‚ùå Module verification failed!"
    exit 1
fi

# =============================================================================
# START TRAINING
# =============================================================================

echo ""
echo "üöÄ Starting BLIP3-o Patch-Level Training..."
echo "==========================================="
echo "üéØ PATCH-LEVEL FEATURES:"
echo "  ‚Ä¢ 256-token patch embeddings (16√ó16 grid)"
echo "  ‚Ä¢ EVA-CLIP conditioning (4096-dim)"
echo "  ‚Ä¢ CLIP patch supervision (1024-dim)"
echo "  ‚Ä¢ Flow matching training objective"
echo "  ‚Ä¢ Image-to-text recall evaluation"
echo "  ‚Ä¢ 3D Rotary Position Embedding"
echo "  ‚Ä¢ Multi-GPU distributed training"
echo ""
echo "üìà EXPECTED PERFORMANCE:"
echo "  ‚Ä¢ Target: >25% Recall@1 (good performance)"
echo "  ‚Ä¢ Target: >40% Recall@5 (strong performance)"
echo "  ‚Ä¢ Baseline: CLIP ViT-L/14 comparison"
echo ""

TRAIN_START_TIME=$(date +%s)

echo "üî• Launching patch-level DiT training..."

# Use the patch-level training script
python train_blip3o_patch_dit.py \
    --chunked_embeddings_dir "$EMBEDDINGS_DIR" \
    --output_dir "$OUTPUT_DIR" \
    --model_size "$MODEL_SIZE" \
    --hidden_size $HIDDEN_SIZE \
    --num_layers $NUM_LAYERS \
    --num_heads $NUM_HEADS \
    --num_epochs $NUM_EPOCHS \
    --batch_size $BATCH_SIZE \
    --eval_batch_size $EVAL_BATCH_SIZE \
    --learning_rate $LEARNING_RATE \
    --weight_decay 0.01 \
    --warmup_steps $WARMUP_STEPS \
    --gradient_accumulation_steps $GRADIENT_ACCUMULATION \
    --fp16 \
    --dataloader_num_workers 4 \
    --enable_recall_eval \
    --recall_eval_steps 250 \
    --recall_eval_samples 50 \
    --use_contrastive_loss \
    --contrastive_weight 0.1 \
    --enhanced_loss

TRAINING_EXIT_CODE=$?
TRAIN_END_TIME=$(date +%s)
TRAIN_DURATION=$((TRAIN_END_TIME - TRAIN_START_TIME))

echo ""
echo "üèÅ PATCH-LEVEL TRAINING COMPLETED"
echo "================================="
echo "Exit code: $TRAINING_EXIT_CODE"
echo "Duration: $TRAIN_DURATION seconds ($(($TRAIN_DURATION / 60)) minutes)"

# =============================================================================
# RESULTS AND CLEANUP
# =============================================================================

if [ $TRAINING_EXIT_CODE -eq 0 ]; then
    echo ""
    echo "‚úÖ SUCCESS! Patch-level training completed!"
    echo "=========================================="
    
    # Copy to persistent storage
    if [ -d "$OUTPUT_DIR" ]; then
        PERSISTENT_DIR="${BLIP3O_CHECKPOINTS}/$(basename $OUTPUT_DIR)"
        mkdir -p "$PERSISTENT_DIR"
        
        echo "üìÅ Copying model to persistent storage..."
        if cp -r "$OUTPUT_DIR"/* "$PERSISTENT_DIR/" 2>/dev/null; then
            echo "‚úÖ Model copied to: $PERSISTENT_DIR"
        else
            echo "‚ö†Ô∏è  Copy failed, model remains at: $OUTPUT_DIR"
        fi
    fi
    
    echo ""
    echo "üéâ PATCH-LEVEL TRAINING SUCCESS!"
    echo "================================"
    echo "‚úÖ 256-token patch-level flow matching completed"
    echo "‚úÖ EVA-CLIP ‚Üí CLIP patch translation trained"
    echo "‚úÖ Image-to-text recall optimization successful"
    echo "‚úÖ BLIP3-o paper alignment achieved"
    echo ""
    echo "üìÅ Model locations:"
    echo "   Primary: $OUTPUT_DIR"
    if [ -d "$PERSISTENT_DIR" ]; then
        echo "   Persistent: $PERSISTENT_DIR"
    fi
    echo ""
    echo "üß™ Next steps:"
    echo "   1. Run recall evaluation: python eval_blip3o_patch_recall.py"
    echo "   2. Check training logs for performance metrics"
    echo "   3. Compare with CLIP baseline performance"
    echo "   4. Fine-tune if recall performance needs improvement"
    
else
    echo ""
    echo "‚ùå PATCH-LEVEL TRAINING FAILED"
    echo "=============================="
    echo "Exit code: $TRAINING_EXIT_CODE"
    echo ""
    echo "üîç Debugging Information:"
    echo "========================"
    echo "1. Check error log: ${SLURM_JOB_ID}.err"
    echo "2. Check training logs in: $OUTPUT_DIR"
    echo "3. Check GPU diagnostics output above"
    echo ""
    echo "üí° Common solutions:"
    echo "  ‚Ä¢ GPU allocation: Check SLURM GPU request"
    echo "  ‚Ä¢ Memory issues: Reduce batch size"
    echo "  ‚Ä¢ Module loading: Check conda environment"
    echo "  ‚Ä¢ Embeddings: Verify patch-level embeddings exist"
fi

# Cleanup
echo ""
echo "üßπ Cleanup..."
echo "============"

if [ -d "${BLIP3O_JOB_TEMP}/cache" ]; then
    CACHE_SIZE=$(du -sh "${BLIP3O_JOB_TEMP}/cache" 2>/dev/null | cut -f1)
    rm -rf "${BLIP3O_JOB_TEMP}/cache" 2>/dev/null
    echo "‚úÖ Cleaned up model cache ($CACHE_SIZE)"
fi

echo ""
echo "üèÅ Job completed at $(date)"
echo "Total runtime: $SECONDS seconds"
echo "================================================================"

exit $TRAINING_EXIT_CODE