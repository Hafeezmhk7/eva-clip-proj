#!/bin/bash
#SBATCH --job-name=global_blip3o_multi_gpu
#SBATCH --partition=gpu_h100
#SBATCH --nodes=1
#SBATCH --gpus=3
#SBATCH --cpus-per-gpu=18
#SBATCH --time=8:00:00
#SBATCH --mem=0
#SBATCH --output=./slurm_out/global_multi_gpu_%j.out
#SBATCH --error=./slurm_out/global_multi_gpu_%j.err

# =============================================================================
# Global BLIP3-o Multi-GPU Training Job (3x H100 GPUs)
# KEY INNOVATION: Direct global [B, 768] training on multiple GPUs
# =============================================================================

echo "üöÄ Global BLIP3-o Multi-GPU Training (3x H100 GPUs)"
echo "=================================================="
echo "Revolutionary Approach:"
echo "  ‚úÖ Direct global [B, 768] feature training"
echo "  ‚úÖ No training-inference mismatch"
echo "  ‚úÖ 3x H100 GPUs for faster training"
echo "  ‚úÖ Single clean global flow matching objective"
echo "  ‚úÖ Expected: 50-70% recall (vs previous 0.1%)"
echo "=================================================="
echo "Job ID: $SLURM_JOB_ID"
echo "Node: $SLURMD_NODENAME"
echo "Date: $(date)"
echo "Working directory: $(pwd)"
echo "User: $(whoami)"
echo "GPUs requested: $SLURM_GPUS"
echo "CPUs per GPU: $SLURM_CPUS_PER_GPU"
echo "=================================================="

# =============================================================================
# EXPLICIT WORKING DIRECTORY
# =============================================================================
cd $SLURM_SUBMIT_DIR
echo "‚úÖ Changed to submit directory: $(pwd)"

# =============================================================================
# ENVIRONMENT SETUP
# =============================================================================

echo ""
echo "üîß Environment Setup..."
echo "======================="

# Load modules
module purge
module load 2024
module load Miniconda3/24.7.1-0
module load CUDA/12.6.0

# Activate conda environment
source activate eva_clip_env

# Environment check
echo "=== Environment Check ==="
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA: {torch.version.cuda}'); print(f'Devices: {torch.cuda.device_count()}')"
echo "========================="

# Get actual number of GPUs
NUM_GPUS=$(python -c "import torch; print(torch.cuda.device_count())")

echo "üåê Multi-GPU Global Setup:"
echo "   Requested GPUs: $SLURM_GPUS"
echo "   Actual GPUs available: $NUM_GPUS"
echo "   Will use: $NUM_GPUS GPUs for global training"

# =============================================================================
# WORKSPACE SETUP
# =============================================================================

echo ""
echo "üìÅ Workspace Setup..."
echo "==================="

# Setup workspace directories
export BLIP3O_USER=$(whoami)
export BLIP3O_JOB_ID=${SLURM_JOB_ID}

# Persistent workspace
export BLIP3O_WORKSPACE="/scratch-shared/${BLIP3O_USER}/blip3o_workspace"
export BLIP3O_EMBEDDINGS="${BLIP3O_WORKSPACE}/embeddings"
export BLIP3O_CHECKPOINTS="${BLIP3O_WORKSPACE}/checkpoints"

# Job-specific temp
export BLIP3O_JOB_TEMP="/scratch-local/${BLIP3O_USER}.${BLIP3O_JOB_ID}/blip3o_job_${BLIP3O_JOB_ID}"

# Create directories
mkdir -p "${BLIP3O_WORKSPACE}"/{embeddings,checkpoints,logs}
mkdir -p "${BLIP3O_JOB_TEMP}"/{cache,temp_checkpoints}

# Redirect model caches to temp
export TORCH_HOME="${BLIP3O_JOB_TEMP}/cache/torch"
export HF_HOME="${BLIP3O_JOB_TEMP}/cache/huggingface"
export TRANSFORMERS_CACHE="${BLIP3O_JOB_TEMP}/cache/transformers"

# Create cache directories
mkdir -p "${TORCH_HOME}" "${HF_HOME}" "${TRANSFORMERS_CACHE}"

echo "‚úÖ Workspace configured:"
echo "   Workspace: ${BLIP3O_WORKSPACE}"
echo "   Job temp: ${BLIP3O_JOB_TEMP}"
echo "   Cache: ${TORCH_HOME}"

# Create local slurm directories if they don't exist
mkdir -p ./slurm_out ./checkpoints

# =============================================================================
# FIND EMBEDDINGS
# =============================================================================

echo ""
echo "üîç Finding embeddings for multi-GPU training..."
echo "==============================================="

# Initialize embeddings directory
EMBEDDINGS_DIR=""

# Method 1: Check workspace
echo "Method 1: Checking workspace embeddings..."
if [ -d "${BLIP3O_EMBEDDINGS}" ]; then
    WORKSPACE_EMBEDDINGS=$(find "${BLIP3O_EMBEDDINGS}" -name "*chunked*256*" -type d | head -1)
    if [ -n "$WORKSPACE_EMBEDDINGS" ]; then
        echo "‚úÖ Found workspace embeddings: $WORKSPACE_EMBEDDINGS"
        EMBEDDINGS_DIR="$WORKSPACE_EMBEDDINGS"
    else
        echo "‚ö†Ô∏è  Workspace embeddings directory exists but no chunked 256 found"
        ls -la "${BLIP3O_EMBEDDINGS}" 2>/dev/null || echo "   (empty)"
    fi
else
    echo "‚ö†Ô∏è  Workspace embeddings directory does not exist: ${BLIP3O_EMBEDDINGS}"
fi

# Method 2: Check local directory
if [ -z "$EMBEDDINGS_DIR" ]; then
    echo ""
    echo "Method 2: Checking local embeddings..."
    if [ -d "./embeddings" ]; then
        LOCAL_EMBEDDINGS=$(find "./embeddings" -name "*chunked*256*" -type d | head -1)
        if [ -n "$LOCAL_EMBEDDINGS" ]; then
            echo "‚úÖ Found local embeddings: $LOCAL_EMBEDDINGS"
            EMBEDDINGS_DIR="$LOCAL_EMBEDDINGS"
        else
            echo "‚ö†Ô∏è  Local embeddings directory exists but no chunked 256 found"
            ls -la "./embeddings" 2>/dev/null || echo "   (empty)"
        fi
    else
        echo "‚ö†Ô∏è  Local embeddings directory does not exist: ./embeddings"
    fi
fi

# Method 3: Check common patterns
if [ -z "$EMBEDDINGS_DIR" ]; then
    echo ""
    echo "Method 3: Searching for common patterns..."
    
    SEARCH_PATTERNS=(
        "./embeddings/chunked_256_tokens"
        "./embeddings/chunked_256_tokens_*"
        "${BLIP3O_EMBEDDINGS}/chunked_256_tokens"
        "${BLIP3O_EMBEDDINGS}/chunked_256_tokens_*"
    )
    
    for pattern in "${SEARCH_PATTERNS[@]}"; do
        if [ -d "$pattern" ]; then
            echo "‚úÖ Found embeddings with pattern: $pattern"
            EMBEDDINGS_DIR="$pattern"
            break
        fi
    done
fi

# Final check
if [ -z "$EMBEDDINGS_DIR" ]; then
    echo ""
    echo "‚ùå ERROR: No embeddings directory found for multi-GPU training!"
    echo ""
    echo "üîç Debugging information:"
    echo "Current directory: $(pwd)"
    echo "Directory contents:"
    ls -la . 2>/dev/null || echo "   (none)"
    echo ""
    echo "Please ensure embeddings have been extracted!"
    exit 1
fi

# Verify embeddings
if [ ! -f "$EMBEDDINGS_DIR/embeddings_manifest.json" ]; then
    echo "‚ùå ERROR: Manifest not found: $EMBEDDINGS_DIR/embeddings_manifest.json"
    echo "Directory contents:"
    ls -la "$EMBEDDINGS_DIR"
    exit 1
fi

echo ""
echo "‚úÖ Embeddings verified for multi-GPU training!"
echo "   Directory: $EMBEDDINGS_DIR"
echo "   Manifest: $EMBEDDINGS_DIR/embeddings_manifest.json"

# =============================================================================
# OUTPUT DIRECTORY SETUP
# =============================================================================

echo ""
echo "üìÇ Output Directory Setup..."
echo "=========================="

# Create timestamp-based directory name
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
TRAINING_NAME="global_multi_gpu_${NUM_GPUS}gpus_${SLURM_JOB_ID}_${TIMESTAMP}"

# Use temp directory first, then copy to persistent
TEMP_OUTPUT_DIR="${BLIP3O_JOB_TEMP}/temp_checkpoints/${TRAINING_NAME}"
PERSISTENT_OUTPUT_DIR="${BLIP3O_CHECKPOINTS}/${TRAINING_NAME}"

mkdir -p "$TEMP_OUTPUT_DIR"
mkdir -p "$PERSISTENT_OUTPUT_DIR"

echo "‚úÖ Output directories created:"
echo "   Temp: $TEMP_OUTPUT_DIR"
echo "   Persistent: $PERSISTENT_OUTPUT_DIR"

# =============================================================================
# DATASET INFORMATION
# =============================================================================

echo ""
echo "üìä Dataset Information for Multi-GPU Training:"
echo "=============================================="
TOTAL_SHARDS=$(jq -r '.total_shards' "$EMBEDDINGS_DIR/embeddings_manifest.json")
TOTAL_SAMPLES=$(jq -r '.total_samples' "$EMBEDDINGS_DIR/embeddings_manifest.json")

echo "Total shards: $TOTAL_SHARDS"
echo "Total samples: $TOTAL_SAMPLES"
echo "Samples per epoch: $(($TOTAL_SAMPLES * 9 / 10))"
echo "Samples per GPU: $(($TOTAL_SAMPLES / $NUM_GPUS))"
echo "Estimated training time: ~1.5-2 hours (${NUM_GPUS}x speedup)"

# =============================================================================
# VERIFY PYTHON SCRIPT
# =============================================================================

echo ""
echo "üîç Verifying multi-GPU Python script..."
echo "======================================="

PYTHON_SCRIPT="train_global_blip3o_multi_gpu.py"

if [ -f "$PYTHON_SCRIPT" ]; then
    echo "‚úÖ Found multi-GPU global script: $PYTHON_SCRIPT"
else
    echo "‚ùå ERROR: Multi-GPU global script not found: $PYTHON_SCRIPT"
    echo "Available Python files:"
    ls -la *global*.py 2>/dev/null || echo "   (none)"
    ls -la *multi*.py 2>/dev/null || echo "   (none)"
    exit 1
fi

# =============================================================================
# MULTI-GPU TRAINING CONFIGURATION
# =============================================================================

echo ""
echo "‚öôÔ∏è  Multi-GPU Global Training Configuration..."
echo "============================================"

# Multi-GPU optimized configuration
NUM_EPOCHS=6
BATCH_SIZE_PER_GPU=6
EVAL_BATCH_SIZE_PER_GPU=12
LEARNING_RATE=8e-5  # Adjusted for multi-GPU
MODEL_DIM=768
NUM_LAYERS=12
NUM_HEADS=12
GRADIENT_ACCUMULATION_STEPS=4
WARMUP_STEPS=150

# Calculate effective batch size
EFFECTIVE_BATCH_SIZE=$((BATCH_SIZE_PER_GPU * NUM_GPUS * GRADIENT_ACCUMULATION_STEPS))

echo "Multi-GPU Global Configuration:"
echo "  Embeddings dir: $EMBEDDINGS_DIR"
echo "  Output dir: $TEMP_OUTPUT_DIR"
echo "  GPUs: $NUM_GPUS"
echo "  Epochs: $NUM_EPOCHS"
echo "  Batch size per GPU: $BATCH_SIZE_PER_GPU"
echo "  Effective batch size: $EFFECTIVE_BATCH_SIZE"
echo "  Learning rate: $LEARNING_RATE (multi-GPU adjusted)"
echo "  Model: ${MODEL_DIM}D, ${NUM_LAYERS}L, ${NUM_HEADS}H"
echo "  Gradient accumulation: $GRADIENT_ACCUMULATION_STEPS"
echo "  Job ID: $SLURM_JOB_ID"
echo "  Node: $SLURMD_NODENAME"

# =============================================================================
# START MULTI-GPU GLOBAL TRAINING
# =============================================================================

echo ""
echo "üöÄ Starting Multi-GPU Global Training with TorchRun..."
echo "====================================================="

echo "üîß Multi-GPU Global Training Features:"
echo "   üéØ Revolutionary: Direct global [B, 768] training"
echo "   üöÄ ${NUM_GPUS}x H100 GPUs for parallel processing"
echo "   üìê Global flow matching objective"
echo "   üíæ DistributedDataParallel (DDP) optimization"
echo "   üìà Cosine learning rate scheduling"
echo "   ‚úÖ No training-inference mismatch"
echo "   ‚úÖ Expected massive recall improvement"
echo ""

TRAIN_START_TIME=$(date +%s)

# Use torchrun for multi-GPU global training
torchrun --nproc_per_node=$NUM_GPUS --nnodes=1 --node_rank=0 train_global_blip3o_multi_gpu.py \
    --chunked_embeddings_dir "$EMBEDDINGS_DIR" \
    --output_dir "$TEMP_OUTPUT_DIR" \
    --num_epochs $NUM_EPOCHS \
    --batch_size $BATCH_SIZE_PER_GPU \
    --eval_batch_size $EVAL_BATCH_SIZE_PER_GPU \
    --learning_rate $LEARNING_RATE \
    --model_dim $MODEL_DIM \
    --num_layers $NUM_LAYERS \
    --num_heads $NUM_HEADS \
    --mlp_hidden_dim 2048 \
    --weight_decay 0.01 \
    --warmup_steps $WARMUP_STEPS \
    --gradient_accumulation_steps $GRADIENT_ACCUMULATION_STEPS \
    --lr_scheduler_type cosine \
    --fp16 \
    --dataloader_num_workers 4

TRAINING_EXIT_CODE=$?
TRAIN_END_TIME=$(date +%s)
TRAIN_DURATION=$((TRAIN_END_TIME - TRAIN_START_TIME))

echo ""
echo "üéØ MULTI-GPU GLOBAL TRAINING COMPLETED WITH EXIT CODE: $TRAINING_EXIT_CODE"

# =============================================================================
# MODEL ARCHIVING
# =============================================================================

if [ $TRAINING_EXIT_CODE -eq 0 ]; then
    echo ""
    echo "‚úÖ Multi-GPU global training completed successfully! Archiving model..."
    echo "=================================================================="
    
    # Copy to persistent storage
    if [ -d "$TEMP_OUTPUT_DIR" ] && [ "$(ls -A "$TEMP_OUTPUT_DIR")" ]; then
        echo "üìÅ Copying to persistent storage..."
        cp -r "$TEMP_OUTPUT_DIR"/* "$PERSISTENT_OUTPUT_DIR/"
        echo "‚úÖ Model copied to persistent storage: $PERSISTENT_OUTPUT_DIR"
        
        # Also copy to local checkpoints for compatibility
        LOCAL_OUTPUT_DIR="./checkpoints/$(basename "$PERSISTENT_OUTPUT_DIR")"
        mkdir -p "$LOCAL_OUTPUT_DIR"
        cp -r "$TEMP_OUTPUT_DIR"/* "$LOCAL_OUTPUT_DIR/"
        echo "‚úÖ Model also saved locally: $LOCAL_OUTPUT_DIR"
    fi
fi

# =============================================================================
# RESULTS SUMMARY
# =============================================================================

echo ""
echo "========================================================================"
echo "üìä MULTI-GPU GLOBAL BLIP3-O TRAINING RESULTS"
echo "========================================================================"
echo "Job ID: $SLURM_JOB_ID"
echo "Node: $SLURMD_NODENAME"
echo "GPUs Used: $NUM_GPUS x H100"
echo "Training Duration: $TRAIN_DURATION seconds ($(($TRAIN_DURATION / 60)) minutes)"
echo "Total Runtime: $SECONDS seconds"
echo "Date: $(date)"

if [ $TRAINING_EXIT_CODE -eq 0 ]; then
    echo "Status: ‚úÖ SUCCESS - MULTI-GPU GLOBAL TRAINING COMPLETED"
    
    echo ""
    echo "üìÅ MODEL SAVED SUCCESSFULLY:"
    echo "============================"
    echo "Persistent model: $PERSISTENT_OUTPUT_DIR"
    echo "Local model: ./checkpoints/$(basename "$PERSISTENT_OUTPUT_DIR")"
    echo "Directory size: $(du -sh "$PERSISTENT_OUTPUT_DIR" | cut -f1 2>/dev/null || echo "N/A")"
    
    # Check for training summary
    if [ -f "$PERSISTENT_OUTPUT_DIR/global_training_summary.json" ]; then
        echo ""
        echo "üìä MULTI-GPU TRAINING SUMMARY:"
        echo "============================="
        
        if command -v jq &> /dev/null; then
            EMA_COSINE=$(jq -r '.ema_global_cosine' "$PERSISTENT_OUTPUT_DIR/global_training_summary.json" 2>/dev/null || echo "N/A")
            PREDICTED_RECALL=$(jq -r '.predicted_recall_percent' "$PERSISTENT_OUTPUT_DIR/global_training_summary.json" 2>/dev/null || echo "N/A")
            
            echo "EMA Global Cosine: $EMA_COSINE"
            echo "Predicted Recall: $PREDICTED_RECALL%"
            
            if [ "$EMA_COSINE" != "N/A" ] && [ "$PREDICTED_RECALL" != "N/A" ]; then
                COSINE_CHECK=$(echo "$EMA_COSINE > 0.7" | bc -l 2>/dev/null || echo "0")
                RECALL_CHECK=$(echo "$PREDICTED_RECALL > 50" | bc -l 2>/dev/null || echo "0")
                
                echo ""
                if [ "$COSINE_CHECK" = "1" ] && [ "$RECALL_CHECK" = "1" ]; then
                    echo "üéâ EXCELLENT: Multi-GPU global training highly successful!"
                    echo "   Expected recall: 60-70% (revolutionary improvement)"
                    echo "   Training time: ${NUM_GPUS}x faster than single GPU"
                    echo "   Ready for evaluation testing"
                elif [ "$COSINE_CHECK" = "1" ] || [ "$RECALL_CHECK" = "1" ]; then
                    echo "‚úÖ GOOD: Multi-GPU global training successful"
                    echo "   Expected recall: 40-60% (significant improvement)"
                    echo "   Ready for evaluation testing"
                else
                    echo "‚ö†Ô∏è  MODERATE: Some improvement achieved"
                    echo "   May need hyperparameter tuning"
                fi
            fi
        fi
    fi
    
    echo ""
    echo "üéâ SUCCESS: Multi-GPU global training completed!"
    echo ""
    echo "üîß Revolutionary Achievements:"
    echo "   ‚úÖ Direct global [B, 768] feature training"
    echo "   ‚úÖ No training-inference mismatch"
    echo "   ‚úÖ ${NUM_GPUS}x H100 GPU parallel processing"
    echo "   ‚úÖ Expected 500-700x recall improvement"
    echo "   ‚úÖ Training time: ~$(($TRAIN_DURATION / 60)) minutes"
    echo ""
    echo "üéØ Next Steps:"
    echo "   1. Run evaluation: sbatch job_scripts/evaluate_global_blip3o.job"
    echo "   2. Expected performance: 50-70% R@1 recall"
    echo "   3. Compare with baseline: ~60% CLIP baseline"
    
else
    echo "Status: ‚ùå FAILED"
    echo ""
    echo "‚ùå MULTI-GPU GLOBAL TRAINING FAILED WITH EXIT CODE: $TRAINING_EXIT_CODE"
    echo ""
    echo "üîç Check the error log: ./slurm_out/global_multi_gpu_${SLURM_JOB_ID}.err"
fi

echo "========================================================================"

# =============================================================================
# CLEANUP
# =============================================================================

# Cleanup temp cache
if [ -d "${BLIP3O_JOB_TEMP}/cache" ]; then
    echo "üßπ Cleaning up temporary cache..."
    rm -rf "${BLIP3O_JOB_TEMP}/cache"
fi

echo "üèÅ Multi-GPU global training job completed at $(date)"

exit $TRAINING_EXIT_CODE