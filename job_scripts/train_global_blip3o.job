#!/bin/bash
#SBATCH --job-name=blip3o_fixed_multi_gpu
#SBATCH --partition=gpu_h100
#SBATCH --nodes=1
#SBATCH --gpus=3
#SBATCH --cpus-per-gpu=18
#SBATCH --time=12:00:00
#SBATCH --mem=0
#SBATCH --output=./slurm_out/fixed_multi_gpu_%j.out
#SBATCH --error=./slurm_out/fixed_multi_gpu_%j.err

echo "üöÄ FIXED BLIP3-o Multi-GPU Training with Enhanced GPU Detection"
echo "================================================================"
echo "üîß ENHANCED FEATURES:"
echo "  ‚úÖ Robust GPU detection and error handling"
echo "  ‚úÖ Automatic fixes for common GPU issues"
echo "  ‚úÖ Better SLURM GPU allocation handling"
echo "  ‚úÖ CPU fallback if GPU issues persist"
echo "  ‚úÖ Detailed logging and debugging"
echo "================================================================"
echo "Job ID: $SLURM_JOB_ID"
echo "Node: $SLURMD_NODENAME"
echo "Date: $(date)"
echo "Working directory: $(pwd)"
echo "User: $(whoami)"
echo "GPUs requested: $SLURM_GPUS"
echo "CPUs per GPU: $SLURM_CPUS_PER_GPU"
echo "================================================================"

# =============================================================================
# ENHANCED ENVIRONMENT SETUP
# =============================================================================

echo ""
echo "üîß Enhanced Environment Setup..."
echo "==============================="

# Change to submit directory
cd $SLURM_SUBMIT_DIR
echo "‚úÖ Working directory: $(pwd)"

# Purge modules and load required ones
module purge
echo "üì¶ Loading modules..."

# Load modules
module load 2024
module load Miniconda3/24.7.1-0
module load CUDA/12.6.0

echo "‚úÖ Modules loaded successfully"

# Activate your conda environment
source activate eva_clip_env
echo "‚úÖ Conda environment activated: $CONDA_DEFAULT_ENV"

# =============================================================================
# ENHANCED GPU ENVIRONMENT SETUP
# =============================================================================

echo ""
echo "üéÆ Enhanced GPU Environment Setup..."
echo "=================================="

# Print initial SLURM GPU allocation
echo "üìä SLURM GPU Variables (before fixes):"
echo "  SLURM_GPUS: $SLURM_GPUS"
echo "  SLURM_GPUS_ON_NODE: $SLURM_GPUS_ON_NODE"
echo "  SLURM_LOCALID: $SLURM_LOCALID"
echo "  SLURM_PROCID: $SLURM_PROCID"
echo "  CUDA_VISIBLE_DEVICES: $CUDA_VISIBLE_DEVICES"

# ENHANCED: Fix common GPU allocation issues
echo ""
echo "üîß Applying GPU environment fixes..."

# Fix 1: Handle empty CUDA_VISIBLE_DEVICES
if [ -z "$CUDA_VISIBLE_DEVICES" ] || [ "$CUDA_VISIBLE_DEVICES" = "" ]; then
    echo "‚ö†Ô∏è  CUDA_VISIBLE_DEVICES is empty - this blocks GPU access!"
    
    # Try to fix using SLURM variables
    if [ ! -z "$SLURM_LOCALID" ]; then
        export CUDA_VISIBLE_DEVICES=$SLURM_LOCALID
        echo "‚úÖ Fixed: Set CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES (from SLURM_LOCALID)"
    elif [ ! -z "$SLURM_GPUS" ]; then
        # Create comma-separated list of GPU IDs
        export CUDA_VISIBLE_DEVICES=$(seq -s, 0 $((SLURM_GPUS-1)))
        echo "‚úÖ Fixed: Set CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES (from SLURM_GPUS)"
    else
        echo "‚ö†Ô∏è  Cannot auto-fix CUDA_VISIBLE_DEVICES - will try detection in Python"
    fi
fi

# Fix 2: Optimize NCCL for Snellius InfiniBand
echo ""
echo "üîß Optimizing NCCL settings for Snellius..."

# InfiniBand settings for Snellius
export NCCL_IB_DISABLE=0           # Enable InfiniBand
export NCCL_NET_GDR_LEVEL=3        # GPU Direct RDMA
export NCCL_P2P_LEVEL=NVL          # NVLink for P2P

# Alternative if InfiniBand causes issues (uncomment if needed):
# export NCCL_IB_DISABLE=1
# export NCCL_P2P_DISABLE=1
# export NCCL_NET_GDR_LEVEL=0

# Enhanced error handling and debugging
export NCCL_ASYNC_ERROR_HANDLING=1
export NCCL_DEBUG=WARN              # Change to INFO for detailed debugging
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512

# Threading optimization
export OMP_NUM_THREADS=1

echo "‚úÖ NCCL environment optimized for Snellius"

# Fix 3: Print final GPU environment
echo ""
echo "üìä GPU Environment (after fixes):"
echo "  CUDA_VISIBLE_DEVICES: $CUDA_VISIBLE_DEVICES"
echo "  NCCL_IB_DISABLE: $NCCL_IB_DISABLE"
echo "  NCCL_DEBUG: $NCCL_DEBUG"

# =============================================================================
# WORKSPACE SETUP
# =============================================================================

echo ""
echo "üìÅ Enhanced Workspace Setup..."
echo "============================="

# Setup directories with better structure
export BLIP3O_USER=$(whoami)
export BLIP3O_JOB_ID=${SLURM_JOB_ID}

# Use scratch-shared for persistent storage
export BLIP3O_WORKSPACE="/scratch-shared/${BLIP3O_USER}/blip3o_workspace"
export BLIP3O_EMBEDDINGS="${BLIP3O_WORKSPACE}/embeddings"
export BLIP3O_CHECKPOINTS="${BLIP3O_WORKSPACE}/checkpoints"

# Job-specific temp directory with better error handling
if [ ! -z "$TMPDIR" ]; then
    export BLIP3O_JOB_TEMP="$TMPDIR/blip3o_job_${BLIP3O_JOB_ID}"
else
    export BLIP3O_JOB_TEMP="/tmp/blip3o_job_${BLIP3O_USER}_${BLIP3O_JOB_ID}"
fi

# Create directories with error checking
echo "üìÅ Creating workspace directories..."
mkdir -p "${BLIP3O_WORKSPACE}"/{embeddings,checkpoints,logs} || {
    echo "‚ùå Failed to create persistent workspace directories"
    echo "üí° Check permissions for: ${BLIP3O_WORKSPACE}"
    exit 1
}

mkdir -p "${BLIP3O_JOB_TEMP}"/{cache,working} || {
    echo "‚ùå Failed to create job temp directories" 
    echo "üí° Check permissions for: ${BLIP3O_JOB_TEMP}"
    exit 1
}

# Redirect model caches to avoid home directory quota
export TORCH_HOME="${BLIP3O_JOB_TEMP}/cache/torch"
export HF_HOME="${BLIP3O_JOB_TEMP}/cache/huggingface"
export TRANSFORMERS_CACHE="${BLIP3O_JOB_TEMP}/cache/transformers"

mkdir -p "${TORCH_HOME}" "${HF_HOME}" "${TRANSFORMERS_CACHE}"

echo "‚úÖ Workspace configured:"
echo "  Workspace: ${BLIP3O_WORKSPACE}"
echo "  Job temp: ${BLIP3O_JOB_TEMP}"
echo "  Model cache: ${TORCH_HOME}"

# =============================================================================
# ENHANCED GPU DIAGNOSTICS
# =============================================================================

echo ""
echo "üß™ Enhanced GPU Diagnostics..."
echo "============================"

# Run comprehensive GPU diagnostics
echo "üîç Python CUDA check:"
python -c "
import torch
print(f'PyTorch version: {torch.__version__}')
print(f'CUDA available: {torch.cuda.is_available()}')
if torch.cuda.is_available():
    print(f'CUDA version: {torch.version.cuda}')
    print(f'GPU count: {torch.cuda.device_count()}')
    for i in range(torch.cuda.device_count()):
        props = torch.cuda.get_device_properties(i)
        memory_gb = props.total_memory / (1024**3)
        print(f'  GPU {i}: {props.name} ({memory_gb:.1f} GB)')
        
        # Test GPU access
        try:
            test_tensor = torch.randn(100, 100, device=f'cuda:{i}')
            print(f'    ‚úÖ GPU {i} accessible and working')
        except Exception as e:
            print(f'    ‚ùå GPU {i} error: {e}')
else:
    print('‚ùå CUDA not available!')
    print('üîç Checking common issues...')
    import os
    print(f'CUDA_VISIBLE_DEVICES: {os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"Not set\")}')
"

# Check nvidia-smi if available
echo ""
echo "üîç nvidia-smi check:"
if command -v nvidia-smi &> /dev/null; then
    nvidia-smi --query-gpu=index,name,memory.total,memory.used --format=csv,noheader,nounits || {
        echo "‚ùå nvidia-smi failed"
        echo "üí° GPU drivers might not be properly loaded"
    }
else
    echo "‚ùå nvidia-smi not available"
fi

# =============================================================================
# FIND EMBEDDINGS
# =============================================================================

echo ""
echo "üîç Finding embeddings..."
echo "======================="

# Look for chunked embeddings with better error handling
EMBEDDINGS_DIR=""

SEARCH_LOCATIONS=(
    "${BLIP3O_EMBEDDINGS}/chunked_256_tokens"
    "${BLIP3O_EMBEDDINGS}"
    "./embeddings/chunked_256_tokens"
    "./embeddings"
)

for location in "${SEARCH_LOCATIONS[@]}"; do
    if [ -d "$location" ] && [ -f "$location/embeddings_manifest.json" ]; then
        EMBEDDINGS_DIR="$location"
        echo "‚úÖ Found embeddings: $EMBEDDINGS_DIR"
        break
    fi
done

if [ -z "$EMBEDDINGS_DIR" ]; then
    echo "‚ùå No embeddings found!"
    echo "üí° Searched locations:"
    for location in "${SEARCH_LOCATIONS[@]}"; do
        echo "    $location"
    done
    echo ""
    echo "üí° Please run embedding extraction first:"
    echo "    python src/modules/extract_embeddings_g.py"
    exit 1
fi

# Verify embeddings with detailed info
MANIFEST_FILE="${EMBEDDINGS_DIR}/embeddings_manifest.json"
if [ -f "$MANIFEST_FILE" ]; then
    echo "üìä Dataset info:"
    python -c "
import json
try:
    with open('$MANIFEST_FILE') as f:
        manifest = json.load(f)
    print(f'  Total shards: {manifest.get(\"total_shards\", \"unknown\")}')
    print(f'  Total samples: {manifest.get(\"total_samples\", \"unknown\"):,}')
    print(f'  Format version: {manifest.get(\"format_version\", \"unknown\")}')
except Exception as e:
    print(f'  Error reading manifest: {e}')
"
else
    echo "‚ùå Manifest file missing: $MANIFEST_FILE"
    exit 1
fi

# =============================================================================
# CREATE OUTPUT DIRECTORY
# =============================================================================

echo ""
echo "üìÇ Setting up output directory..."
echo "==============================="

TIMESTAMP=$(date +%Y%m%d_%H%M%S)
OUTPUT_DIR="./checkpoints/fixed_multi_gpu_${SLURM_JOB_ID}_${TIMESTAMP}"

mkdir -p "$OUTPUT_DIR" || {
    echo "‚ùå Failed to create output directory: $OUTPUT_DIR"
    exit 1
}

mkdir -p "./slurm_out" || {
    echo "‚ö†Ô∏è  Failed to create slurm_out directory"
}

echo "‚úÖ Output directory: $OUTPUT_DIR"

# =============================================================================
# ENHANCED TRAINING CONFIGURATION
# =============================================================================

echo ""
echo "‚öôÔ∏è Enhanced Training Configuration..."
echo "==================================="

# Enhanced configuration with validation
NUM_EPOCHS=5
BATCH_SIZE=8        # Conservative for stability
EVAL_BATCH_SIZE=4   # Smaller for memory
LEARNING_RATE=1e-4  # Standard rate
MODEL_DIM=768       # Compatible with 12 heads
NUM_LAYERS=16       # Good balance
NUM_HEADS=12        # 768/12 = 64 (divisible by 4 for RoPE)
GRADIENT_ACCUMULATION=4
WARMUP_STEPS=100

# Auto-detect GPU count for effective batch size calculation
ACTUAL_GPU_COUNT=$(python -c "import torch; print(torch.cuda.device_count() if torch.cuda.is_available() else 1)")
EFFECTIVE_BATCH_SIZE=$((BATCH_SIZE * ACTUAL_GPU_COUNT * GRADIENT_ACCUMULATION))

echo "Enhanced Training Configuration:"
echo "  üéØ Target GPUs: ${SLURM_GPUS:-1}"
echo "  üîç Detected GPUs: $ACTUAL_GPU_COUNT"
echo "  üèóÔ∏è  Model: ${MODEL_DIM}D, ${NUM_LAYERS}L, ${NUM_HEADS}H"
echo "  üìö Epochs: $NUM_EPOCHS"
echo "  üì¶ Batch size per GPU: $BATCH_SIZE"
echo "  üîÑ Gradient accumulation: $GRADIENT_ACCUMULATION"
echo "  üìä Effective batch size: $EFFECTIVE_BATCH_SIZE"
echo "  üìà Learning rate: $LEARNING_RATE"
echo "  üî• Warmup steps: $WARMUP_STEPS"

# =============================================================================
# FINAL VERIFICATION BEFORE TRAINING
# =============================================================================

echo ""
echo "üîç Final verification before training..."
echo "======================================"

# Run the GPU diagnostics script if available
if [ -f "gpu_diagnostics.py" ]; then
    echo "üß™ Running comprehensive GPU diagnostics..."
    python gpu_diagnostics.py || {
        echo "‚ö†Ô∏è  GPU diagnostics completed with warnings - check output above"
    }
else
    echo "‚ö†Ô∏è  gpu_diagnostics.py not found - running basic checks"
    
    # Basic Python GPU test
    python -c "
import torch
import os

print('üß™ Basic GPU Verification:')
print(f'CUDA available: {torch.cuda.is_available()}')

if torch.cuda.is_available():
    gpu_count = torch.cuda.device_count()
    print(f'GPU count: {gpu_count}')
    
    # Test each GPU
    working_gpus = 0
    for i in range(gpu_count):
        try:
            device = torch.device(f'cuda:{i}')
            test_tensor = torch.randn(1000, 1000, device=device)
            result = torch.mm(test_tensor, test_tensor.t())
            working_gpus += 1
            print(f'‚úÖ GPU {i}: Working')
        except Exception as e:
            print(f'‚ùå GPU {i}: Error - {e}')
    
    if working_gpus == 0:
        print('‚ùå No working GPUs found!')
        exit(1)
    elif working_gpus < gpu_count:
        print(f'‚ö†Ô∏è  Only {working_gpus}/{gpu_count} GPUs working')
    else:
        print(f'‚úÖ All {working_gpus} GPUs working perfectly!')
else:
    print('‚ùå CUDA not available - will use CPU fallback')
" || {
        echo "‚ùå GPU verification failed!"
        echo "üí° Will attempt training with CPU fallback"
    }
fi

# =============================================================================
# START ENHANCED TRAINING
# =============================================================================

echo ""
echo "üöÄ Starting Enhanced Multi-GPU Training..."
echo "=========================================="
echo "üéØ ENHANCED FEATURES:"
echo "  ‚Ä¢ Robust GPU detection and auto-fixes"
echo "  ‚Ä¢ Automatic fallback to CPU if GPU fails"
echo "  ‚Ä¢ Better error handling and debugging"
echo "  ‚Ä¢ Enhanced memory management"
echo "  ‚Ä¢ Improved DDP compatibility"
echo ""
echo "üìà EXPECTED IMPROVEMENTS:"
echo "  ‚Ä¢ Stable training even with GPU allocation issues"
echo "  ‚Ä¢ Better error messages for debugging"
echo "  ‚Ä¢ Automatic recovery from common problems"
echo "  ‚Ä¢ Detailed logging for troubleshooting"
echo ""

TRAIN_START_TIME=$(date +%s)

# Enhanced training with better error handling
echo "üî• Launching enhanced training..."

# Use the fixed training script
python train_blip3o_multi_gpu.py \
    --chunked_embeddings_dir "$EMBEDDINGS_DIR" \
    --output_dir "$OUTPUT_DIR" \
    --num_epochs $NUM_EPOCHS \
    --batch_size $BATCH_SIZE \
    --eval_batch_size $EVAL_BATCH_SIZE \
    --learning_rate $LEARNING_RATE \
    --model_dim $MODEL_DIM \
    --num_layers $NUM_LAYERS \
    --num_heads $NUM_HEADS \
    --weight_decay 0.01 \
    --warmup_steps $WARMUP_STEPS \
    --gradient_accumulation_steps $GRADIENT_ACCUMULATION \
    --lr_scheduler_type cosine \
    --fp16 \
    --dataloader_num_workers 4 \
    --cpu_fallback

TRAINING_EXIT_CODE=$?
TRAIN_END_TIME=$(date +%s)
TRAIN_DURATION=$((TRAIN_END_TIME - TRAIN_START_TIME))

echo ""
echo "üèÅ ENHANCED TRAINING COMPLETED"
echo "============================="
echo "Exit code: $TRAINING_EXIT_CODE"
echo "Duration: $TRAIN_DURATION seconds ($(($TRAIN_DURATION / 60)) minutes)"

# =============================================================================
# ENHANCED RESULTS AND CLEANUP
# =============================================================================

if [ $TRAINING_EXIT_CODE -eq 0 ]; then
    echo ""
    echo "‚úÖ SUCCESS! Enhanced training completed!"
    echo "====================================="
    
    # Copy to persistent storage with verification
    if [ -d "$OUTPUT_DIR" ]; then
        PERSISTENT_DIR="${BLIP3O_CHECKPOINTS}/$(basename $OUTPUT_DIR)"
        mkdir -p "$PERSISTENT_DIR"
        
        echo "üìÅ Copying model to persistent storage..."
        if cp -r "$OUTPUT_DIR"/* "$PERSISTENT_DIR/" 2>/dev/null; then
            echo "‚úÖ Model copied to persistent storage: $PERSISTENT_DIR"
            
            # Verify the copy
            if [ -f "$PERSISTENT_DIR/config.json" ]; then
                echo "‚úÖ Model files verified in persistent storage"
            else
                echo "‚ö†Ô∏è  Model copy may be incomplete"
            fi
        else
            echo "‚ö†Ô∏è  Failed to copy to persistent storage (files still in: $OUTPUT_DIR)"
        fi
    fi
    
    # Enhanced success reporting
    echo ""
    echo "üéâ ENHANCED TRAINING SUCCESS!"
    echo "============================"
    echo "‚úÖ All GPU detection and fixes worked"
    echo "‚úÖ Multi-GPU training completed successfully"
    echo "‚úÖ Model saved and verified"
    echo ""
    echo "üìÅ Model locations:"
    echo "   Primary: $OUTPUT_DIR"
    if [ -d "$PERSISTENT_DIR" ]; then
        echo "   Persistent: $PERSISTENT_DIR"
    fi
    echo ""
    echo "üß™ Next steps:"
    echo "   1. Run model evaluation"
    echo "   2. Check training logs for performance metrics"
    echo "   3. Consider fine-tuning if needed"
    
else
    echo ""
    echo "‚ùå ENHANCED TRAINING FAILED"
    echo "========================="
    echo "Exit code: $TRAINING_EXIT_CODE"
    echo ""
    echo "üîç Enhanced Debugging Information:"
    echo "================================"
    echo "1. Check error log: ${SLURM_JOB_ID}.err"
    echo "2. Check training debug info: training_error_debug.json"
    echo "3. Check GPU diagnostics output above"
    echo ""
    echo "üí° Common solutions:"
    echo "  ‚Ä¢ GPU allocation: Check SLURM GPU request vs availability"
    echo "  ‚Ä¢ Memory issues: Reduce batch size or model size"
    echo "  ‚Ä¢ Module loading: Verify CUDA module is loaded"
    echo "  ‚Ä¢ Environment: Check conda environment activation"
    echo ""
    echo "üîß Quick fixes to try:"
    echo "  ‚Ä¢ Rerun with --cpu_fallback flag"
    echo "  ‚Ä¢ Check 'sbatch debug_gpu.job' for detailed diagnostics"
    echo "  ‚Ä¢ Verify embeddings exist: ls -la $EMBEDDINGS_DIR"
fi

# Enhanced cleanup
echo ""
echo "üßπ Enhanced cleanup..."
echo "===================="

# Clean up job temp cache but preserve working files
if [ -d "${BLIP3O_JOB_TEMP}/cache" ]; then
    CACHE_SIZE=$(du -sh "${BLIP3O_JOB_TEMP}/cache" 2>/dev/null | cut -f1)
    rm -rf "${BLIP3O_JOB_TEMP}/cache" 2>/dev/null
    echo "‚úÖ Cleaned up model cache ($CACHE_SIZE)"
fi

# Preserve debug files
echo "üíæ Debug files preserved:"
if [ -f "training_error_debug.json" ]; then
    echo "   training_error_debug.json"
fi
if [ -f "gpu_diagnostics_results.json" ]; then
    echo "   gpu_diagnostics_results.json"
fi

echo ""
echo "üèÅ Enhanced job completed at $(date)"
echo "Total runtime: $SECONDS seconds"
echo "================================================================"

exit $TRAINING_EXIT_CODE