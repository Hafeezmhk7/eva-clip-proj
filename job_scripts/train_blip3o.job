#!/bin/bash
#SBATCH --job-name=blip3o_50shards
#SBATCH --partition=gpu_h100
#SBATCH --nodes=1
#SBATCH --gpus=3
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=18
#SBATCH --time=15:00:00
#SBATCH --mem=64G
#SBATCH --output=./slurm_out/blip3o_50shards_%j.out
#SBATCH --error=./slurm_out/blip3o_50shards_%j.err

# =============================================================================
# BLIP3-o Large-Scale Training - 50 Shards with WandB Integration
# Optimized hyperparameters for production-scale training
# =============================================================================

echo "üöÄ BLIP3-o Large-Scale Training - 50 Shards with WandB"
echo "======================================================="
echo "Job ID: ${SLURM_JOB_ID}"
echo "Node: $(hostname)"
echo "Time: $(date)"
echo "GPUs: $(nvidia-smi --query-gpu=name --format=csv,noheader,nounits | tr '\n' ', ')"
echo "======================================================="

cd $SLURM_SUBMIT_DIR

# Setup environment
module purge
module load 2024
module load Miniconda3/24.7.1-0
module load CUDA/12.6.0
source activate eva_clip_env
wandb login 0d9895af249ee18e4fa141e8a2350e0f4adb920f --relogin


# Configuration
EMBEDDINGS_DIR="/scratch-shared/azadaianchuk1/blip3o_workspace/embeddings/patch_only_256_tokens"
OUTPUT_DIR="./checkpoints/blip3o_50shards_$(date +%Y%m%d_%H%M%S)"
TRAINING_MODE="patch_only"
MODEL_SIZE="base"

# Large-scale training hyperparameters (optimized for 50 shards)
NUM_EPOCHS=8                     # Fewer epochs with more data
BATCH_SIZE=128                    # Large batch for stability
GRADIENT_ACCUMULATION_STEPS=4    # Effective batch size: 256
LEARNING_RATE=2e-4              # Higher LR for large batches
WEIGHT_DECAY=0.01               # Regularization
WARMUP_STEPS=500                # Longer warmup for stability
LR_SCHEDULER="cosine"           # Better convergence

# Evaluation parameters (less frequent due to cost)
EVAL_EVERY_N_STEPS=200
EVAL_NUM_SAMPLES=100
EVAL_INFERENCE_STEPS=50

# WandB configuration
WANDB_PROJECT="eva-clip"
WANDB_RUN_NAME="50shards_base_$(date +%Y%m%d_%H%M%S)"
WANDB_TAGS="50shards,large_scale,base_model,production"
WANDB_NOTES="Production training on 50 shards with optimized hyperparameters. Expected ~125M samples, 2-4 days training time."

# Create output directory
mkdir -p "${OUTPUT_DIR}"
mkdir -p ./slurm_out

echo ""
echo "‚öôÔ∏è Large-Scale Training Configuration:"
echo "======================================"
echo "Embeddings: $EMBEDDINGS_DIR"
echo "Output: $OUTPUT_DIR"
echo "Training mode: $TRAINING_MODE"
echo "Model size: $MODEL_SIZE"
echo "Max shards: 50"
echo ""
echo "üìä Hyperparameters (Optimized for 50 Shards):"
echo "  Epochs: $NUM_EPOCHS"
echo "  Batch size: $BATCH_SIZE (effective: $((BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS)))"
echo "  Learning rate: $LEARNING_RATE"
echo "  Weight decay: $WEIGHT_DECAY"
echo "  Warmup steps: $WARMUP_STEPS"
echo "  LR scheduler: $LR_SCHEDULER"
echo ""
echo "üîç Evaluation Configuration:"
echo "  Eval every: $EVAL_EVERY_N_STEPS steps"
echo "  Eval samples: $EVAL_NUM_SAMPLES"
echo "  Inference steps: $EVAL_INFERENCE_STEPS"
echo ""
echo "üìä WandB Configuration:"
echo "  Project: $WANDB_PROJECT"
echo "  Run name: $WANDB_RUN_NAME"
echo "  Tags: $WANDB_TAGS"
echo ""

# Verify embeddings exist
if [ ! -d "$EMBEDDINGS_DIR" ]; then
    echo "‚ùå Embeddings directory not found: $EMBEDDINGS_DIR"
    echo "Available embeddings:"
    ls -la "/scratch-shared/azadaianchuk1/blip3o_workspace/embeddings/" 2>/dev/null || echo "No embeddings found"
    exit 1
fi

echo "‚úÖ Embeddings verified: $EMBEDDINGS_DIR"

# Check available shards
SHARD_COUNT=$(find "$EMBEDDINGS_DIR" -name "*.pkl" | wc -l)
echo "‚úÖ Found $SHARD_COUNT embedding shards"

if [ $SHARD_COUNT -lt 50 ]; then
    echo "‚ö†Ô∏è Warning: Only $SHARD_COUNT shards available (requested 50)"
    echo "   Training will use available shards"
fi



echo "‚úÖ Training script found"

echo ""
echo "üöÄ Starting Large-Scale BLIP3-o Training with WandB..."
echo "====================================================="
echo "Expected behavior:"
echo "  üìä All metrics tracked in WandB dashboard"
echo "  üéØ Target embedding similarity: >0.1 (excellent: >0.3)"
echo "  ‚è±Ô∏è Training time: 2-4 days on 3√óA100 GPUs"
echo "  üìà ~15,000-20,000 training steps total"
echo "  üîç Evaluation every 200 steps (~30 minutes)"
echo "  üíæ Checkpoints saved every 1000 steps"
echo ""
echo "üåê WandB Dashboard:"
echo "  URL will be displayed once training starts"
echo "  Monitor: loss, velocity_sim, embedding_sim, norms"
echo ""

# Launch large-scale training with WandB
python train_eva_repro.py \
    --chunked_embeddings_dir "$EMBEDDINGS_DIR" \
    --output_dir "$OUTPUT_DIR" \
    --training_mode "$TRAINING_MODE" \
    --model_size "$MODEL_SIZE" \
    --max_training_shards 1 \
    --num_epochs $NUM_EPOCHS \
    --batch_size $BATCH_SIZE \
    --gradient_accumulation_steps $GRADIENT_ACCUMULATION_STEPS \
    --learning_rate $LEARNING_RATE \
    --weight_decay $WEIGHT_DECAY \
    --warmup_steps $WARMUP_STEPS \
    --lr_scheduler_type "$LR_SCHEDULER" \
    --eval_every_n_steps $EVAL_EVERY_N_STEPS \
    --eval_num_samples $EVAL_NUM_SAMPLES \
    --eval_inference_steps $EVAL_INFERENCE_STEPS \
    --fp16 \
    --gradient_checkpointing \
    --logging_steps 20 \
    --save_steps 1000 \
    --wandb_project "$WANDB_PROJECT" \
    --wandb_run_name "$WANDB_RUN_NAME" \
    --wandb_tags $WANDB_TAGS \
    --wandb_notes "$WANDB_NOTES"


# --use_wandb \

TRAINING_EXIT_CODE=$?

echo ""
echo "========================================"
echo "üìä Large-Scale Training Results"
echo "========================================"

if [ $TRAINING_EXIT_CODE -eq 0 ]; then
    echo "‚úÖ Large-scale training completed successfully!"
    
    echo ""
    echo "üìã Training Summary:"
    echo "==================="
    
    # Find and display training info
    TRAINING_INFO="$OUTPUT_DIR/training_info.json"
    
    if [ -f "$TRAINING_INFO" ]; then
        echo ""
        echo "üìä Final Metrics:"
        echo "=================="
        
        # Extract key metrics using Python
        python -c "
import json
import sys
try:
    with open('$TRAINING_INFO', 'r') as f:
        data = json.load(f)
    
    if 'final_results' in data and 'training_summary' in data['final_results']:
        summary = data['final_results']['training_summary']
        print(f'üéØ Final Embedding Similarity: {summary.get(\"final_embedding_sim\", 0):.4f}')
        print(f'üéØ Best Embedding Similarity: {summary.get(\"best_embedding_sim\", 0):.4f}')
        print(f'üìà Final Velocity Similarity: {summary.get(\"final_velocity_sim\", 0):.4f}')
        print(f'üìà Best Velocity Similarity: {summary.get(\"best_velocity_sim\", 0):.4f}')
        print(f'üìä Total Steps: {summary.get(\"total_steps\", 0):,}')
        print(f'üè• Training Health: {summary.get(\"training_health\", \"Unknown\")}')
        print(f'üîç Evaluations Performed: {summary.get(\"evaluations_performed\", 0)}')
        
        # WandB info
        wandb_url = summary.get('wandb_url', 'Not available')
        print(f'üìä WandB Run: {wandb_url}')
        
        # Assessment
        best_emb = summary.get('best_embedding_sim', 0)
        if best_emb > 0.3:
            print('üéâ OUTSTANDING: Excellent embedding generation!')
        elif best_emb > 0.1:
            print('üéâ SUCCESS: Good embedding generation!')
        elif best_emb > 0.05:
            print('üìà PROGRESS: Shows learning, may benefit from more training')
        else:
            print('‚ö†Ô∏è NEEDS WORK: Low similarity, check hyperparameters')
    
    if 'final_results' in data and 'final_evaluation' in data['final_results']:
        eval_results = data['final_results']['final_evaluation']
        if eval_results:
            print(f'')
            print(f'üîç Final Evaluation Results:')
            print(f'   Overall Similarity: {eval_results.get(\"overall_embedding_similarity\", 0):.4f}')
            print(f'   High Quality Images: {eval_results.get(\"high_quality_images\", 0)*100:.1f}%')
            print(f'   Very High Quality Images: {eval_results.get(\"very_high_quality_images\", 0)*100:.1f}%')
            print(f'   Excellent Quality Images: {eval_results.get(\"excellent_quality_images\", 0)*100:.1f}%')
            print(f'   Samples Evaluated: {eval_results.get(\"samples_evaluated\", 0):,}')
    
    # Check WandB integration
    if 'wandb_config' in data and data['wandb_config'].get('use_wandb'):
        wandb_url = data['wandb_config'].get('wandb_url', 'URL not found')
        print(f'')
        print(f'üìä WandB Integration: SUCCESS')
        print(f'   Project: {data[\"wandb_config\"].get(\"wandb_project\", \"Unknown\")}')
        print(f'   Run: {data[\"wandb_config\"].get(\"wandb_run_name\", \"Unknown\")}')
        print(f'   URL: {wandb_url}')
    
except Exception as e:
    print(f'Could not parse training info: {e}')
    sys.exit(1)
"
        
        echo ""
        echo "üìÅ Training artifacts saved to: $OUTPUT_DIR"
        echo "üìÑ Training info: $TRAINING_INFO"
    else
        echo "‚ö†Ô∏è No training info file found"
    fi
    
    echo ""
    echo "üéØ Next Steps:"
    echo "=============="
    echo "1. Check WandB dashboard for detailed training curves"
    echo "2. Run comprehensive evaluation:"
    echo "   sbatch job_scripts/evaluate_blip3o.job $OUTPUT_DIR"
    echo "3. Compare results with smaller scale runs"
    echo "4. Consider hyperparameter refinement for future runs"
    
    echo ""
    echo "üìä WandB Analysis Recommendations:"
    echo "  ‚Ä¢ Monitor loss convergence and learning rate schedule"
    echo "  ‚Ä¢ Check velocity similarity progression"
    echo "  ‚Ä¢ Analyze evaluation curves for overfitting"
    echo "  ‚Ä¢ Compare norm stability throughout training"
    echo "  ‚Ä¢ Review high-quality image percentages trends"
    
    echo ""
    echo "‚úÖ SUCCESS: Large-scale 50-shard training completed!"
    
else
    echo "‚ùå FAILED: Training exit code $TRAINING_EXIT_CODE"
    echo ""
    echo "üí° Troubleshooting:"
    echo "  ‚Ä¢ Check log files in ./slurm_out/"
    echo "  ‚Ä¢ Verify embeddings and model paths"
    echo "  ‚Ä¢ Check CUDA memory usage"
    echo "  ‚Ä¢ Monitor WandB for error logs"
    echo "  ‚Ä¢ Try reducing batch_size if OOM"
    echo "  ‚Ä¢ Check disk space for checkpoints"
    
    echo ""
    echo "üîß Recovery Options:"
    echo "  ‚Ä¢ Resume from last checkpoint if available"
    echo "  ‚Ä¢ Reduce hyperparameters and retry"
    echo "  ‚Ä¢ Check WandB run for partial results"
fi

echo ""
echo "üìä Resource Usage Summary:"
nvidia-smi --query-gpu=name,memory.total,memory.used,utilization.gpu --format=csv,noheader,nounits | \
    awk 'BEGIN{print "GPU | Total Memory | Used Memory | Utilization"} {printf "%s | %s MB | %s MB | %s%%\n", $1, $2, $3, $4}'

echo ""
echo "üèÅ Job completed at $(date)"
echo "Total job time: $(echo "scale=2; ($(date +%s) - $SECONDS) / 3600" | bc -l) hours"
echo "========================================"

exit $TRAINING_EXIT_CODE