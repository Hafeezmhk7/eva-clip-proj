#!/bin/bash
#SBATCH --job-name=blip3o_ddp
#SBATCH --partition=gpu_h100
#SBATCH --nodes=1
#SBATCH --gpus=3
#SBATCH --ntasks=3
#SBATCH --ntasks-per-node=3
#SBATCH --cpus-per-task=12
#SBATCH --time=6:00:00
#SBATCH --mem=160G
#SBATCH --output=./slurm_out/blip3o_ddp_%j.out
#SBATCH --error=./slurm_out/blip3o_ddp_%j.err

# =============================================================================
# BLIP3-o Multi-GPU DDP Training - Fixed for Snellius
# =============================================================================

echo "üöÄ BLIP3-o Multi-GPU DDP Training"
echo "==================================="
echo "Job ID: ${SLURM_JOB_ID}"
echo "Node: $(hostname)"
echo "Time: $(date)"
echo "GPUs: ${SLURM_GPUS_ON_NODE}"
echo "Tasks: ${SLURM_NTASKS}"
echo "CPUs per task: ${SLURM_CPUS_PER_TASK}"
echo "Memory: ${SLURM_MEM_PER_NODE}MB"
echo "==================================="

cd $SLURM_SUBMIT_DIR

# Setup environment
module purge
module load 2024
module load Miniconda3/24.7.1-0
module load CUDA/12.6.0
source activate eva_clip_env

# =============================================================================
# CRITICAL DDP/NCCL CONFIGURATION FOR SNELLIUS
# =============================================================================

# Get master node information
export MASTER_ADDR=$(scontrol show hostname ${SLURM_NODELIST} | head -n 1)
export MASTER_PORT=${MASTER_PORT:-29500}
export WORLD_SIZE=${SLURM_NTASKS}

echo "DDP Configuration:"
echo "  Master address: $MASTER_ADDR"
echo "  Master port: $MASTER_PORT"
echo "  World size: $WORLD_SIZE"
echo "  Local GPUs: ${SLURM_GPUS_ON_NODE}"

# NCCL Configuration for Snellius (CRITICAL!)
export NCCL_DEBUG=INFO
export NCCL_DEBUG_SUBSYS=INIT,ENV
export NCCL_IB_DISABLE=0  # Enable InfiniBand if available
export NCCL_SOCKET_IFNAME=^lo,docker0  # Exclude loopback and docker
export NCCL_P2P_DISABLE=0  # Enable P2P for GPUs on same node

# Try to detect the correct network interface
if ip a | grep -q ib0; then
    export NCCL_SOCKET_IFNAME=ib0
    echo "Using InfiniBand interface: ib0"
elif ip a | grep -q eth0; then
    export NCCL_SOCKET_IFNAME=eth0
    echo "Using Ethernet interface: eth0"
else
    # Let NCCL auto-detect
    unset NCCL_SOCKET_IFNAME
    echo "Letting NCCL auto-detect network interface"
fi

# Additional NCCL optimizations
export NCCL_TREE_THRESHOLD=0
export NCCL_ASYNC_ERROR_HANDLING=1
export OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK}

# =============================================================================
# MEMORY OPTIMIZATION CONFIGURATION
# =============================================================================

# PyTorch memory settings
export PYTORCH_CUDA_ALLOC_CONF="max_split_size_mb:512,garbage_collection_threshold:0.6"
export CUDA_LAUNCH_BLOCKING=0
export TORCH_SHOW_CPP_STACKTRACES=1

# Python settings
export PYTHONHASHSEED=42
export MKL_NUM_THREADS=${SLURM_CPUS_PER_TASK}

# =============================================================================
# TRAINING CONFIGURATION
# =============================================================================

# Paths
EMBEDDINGS_DIR="/scratch-shared/azadaianchuk1/blip3o_workspace/embeddings/patch_only_256_tokens"
OUTPUT_DIR="./checkpoints_ddp_$(date +%Y%m%d_%H%M%S)"
TASK_MODE="clip_denoising"

# Model configuration
MODEL_SIZE="base"
TRAINING_MODE="patch_only"
PREDICTION_TYPE="velocity"

# Training hyperparameters
LEARNING_RATE=1e-4
BATCH_SIZE=2
GRADIENT_ACCUMULATION_STEPS=8
NUM_EPOCHS=5
WARMUP_STEPS=100
WEIGHT_DECAY=0.01
MAX_GRAD_NORM=1.0

# Memory optimization
MAX_SHARD_CACHE=2
SAMPLES_PER_SHARD_LOAD=500
MAX_SHARDS=35

# Spherical flow matching
SPHERE_CONSTRAINT_WEIGHT=0.1
NOISE_SCHEDULE="uniform"
MAX_NOISE_LEVEL=0.9
MIN_NOISE_LEVEL=0.1

# Evaluation
EVAL_EVERY_N_STEPS=250
EVAL_NUM_SAMPLES=300
EVAL_INFERENCE_STEPS=25

# WandB
USE_WANDB=false
WANDB_PROJECT="blip3o-ddp-35shards"
WANDB_RUN_NAME="clip_denoising_ddp_$(date +%Y%m%d_%H%M%S)"

# =============================================================================
# DIRECTORY SETUP
# =============================================================================

mkdir -p "${OUTPUT_DIR}"
mkdir -p ./slurm_out

echo ""
echo "üìÅ Directory Configuration:"
echo "  Embeddings: $EMBEDDINGS_DIR"
echo "  Output: $OUTPUT_DIR"

# =============================================================================
# TRAINING LAUNCH WITH SRUN
# =============================================================================

echo ""
echo "üöÄ Launching Multi-GPU DDP Training..."
echo "======================================="

# Launch with srun (SLURM-aware MPI launch)
srun --mpi=pmi2 python train_eva_repro_ddp.py \
    --task_mode "$TASK_MODE" \
    --chunked_embeddings_dir "$EMBEDDINGS_DIR" \
    --output_dir "$OUTPUT_DIR" \
    --model_size "$MODEL_SIZE" \
    --training_mode "$TRAINING_MODE" \
    --prediction_type "$PREDICTION_TYPE" \
    --learning_rate $LEARNING_RATE \
    --batch_size $BATCH_SIZE \
    --gradient_accumulation_steps $GRADIENT_ACCUMULATION_STEPS \
    --num_epochs $NUM_EPOCHS \
    --warmup_steps $WARMUP_STEPS \
    --weight_decay $WEIGHT_DECAY \
    --max_grad_norm $MAX_GRAD_NORM \
    --max_shard_cache $MAX_SHARD_CACHE \
    --samples_per_shard_load $SAMPLES_PER_SHARD_LOAD \
    --max_shards $MAX_SHARDS \
    --sphere_constraint_weight $SPHERE_CONSTRAINT_WEIGHT \
    --noise_schedule $NOISE_SCHEDULE \
    --max_noise_level $MAX_NOISE_LEVEL \
    --min_noise_level $MIN_NOISE_LEVEL \
    --eval_every_n_steps $EVAL_EVERY_N_STEPS \
    --eval_num_samples $EVAL_NUM_SAMPLES \
    --eval_inference_steps $EVAL_INFERENCE_STEPS \
    --num_workers 2 \
    --fp16

TRAINING_EXIT_CODE=$?

echo ""
echo "üèÅ Job completed at $(date) with exit code $TRAINING_EXIT_CODE"
echo "========================================"

exit $TRAINING_EXIT_CODE