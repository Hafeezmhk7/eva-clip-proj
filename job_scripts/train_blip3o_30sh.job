#!/bin/bash
#SBATCH --job-name=blip3o_ddp_fixed
#SBATCH --partition=gpu_h100
#SBATCH --nodes=1
#SBATCH --ntasks=3
#SBATCH --ntasks-per-node=3
#SBATCH --cpus-per-task=12
#SBATCH --gpus-per-node=3
#SBATCH --time=6:00:00
#SBATCH --mem=160G
#SBATCH --output=./slurm_out/blip3o_ddp_%j.out
#SBATCH --error=./slurm_out/blip3o_ddp_%j.err

# =============================================================================
# FIXED: BLIP3-o Multi-GPU DDP Training with Proper Network Configuration
# =============================================================================

echo "üöÄ FIXED Multi-GPU BLIP3-o Training with Proper DDP Setup"
echo "=========================================================="
echo "Job ID: ${SLURM_JOB_ID}"
echo "Node: $(hostname)"
echo "Time: $(date)"
echo "GPUs: ${SLURM_GPUS_ON_NODE}"
echo "Tasks: ${SLURM_NTASKS}"
echo "=========================================================="

cd $SLURM_SUBMIT_DIR

# Load modules
module purge
module load 2024
module load Miniconda3/24.7.1-0
module load CUDA/12.6.0
source activate eva_clip_env

# =============================================================================
# FIXED: Proper Network and DDP Configuration
# =============================================================================

# Get node information
export MASTER_ADDR=$(hostname)
export MASTER_PORT=$(python -c "import socket; s=socket.socket(); s.bind(('', 0)); print(s.getsockname()[1]); s.close()")
export WORLD_SIZE=${SLURM_NTASKS}
export NNODES=${SLURM_NNODES}

echo "üåê Network Configuration:"
echo "  Master node: $MASTER_ADDR"
echo "  Master port: $MASTER_PORT" 
echo "  World size: $WORLD_SIZE"
echo "  Nodes: $NNODES"

# FIXED: NCCL Configuration for Snellius
export NCCL_DEBUG=INFO
export NCCL_DEBUG_SUBSYS=ALL

# Network interface detection and configuration
if ip addr show ib0 &>/dev/null; then
    echo "  Using InfiniBand interface: ib0"
    export NCCL_SOCKET_IFNAME=ib0
    export NCCL_IB_DISABLE=0
    export NCCL_IB_HCA=mlx5_4
    export NCCL_NET_GDR_LEVEL=2
    export NCCL_P2P_LEVEL=NVL
    ip addr show ib0 | grep inet
elif ip addr show eth0 &>/dev/null; then
    echo "  Using Ethernet interface: eth0"
    export NCCL_SOCKET_IFNAME=eth0
    export NCCL_IB_DISABLE=1
    ip addr show eth0 | grep inet
else
    echo "  Auto-detecting network interface"
    # Find the first available network interface
    AVAILABLE_IF=$(ip route | grep default | awk '{print $5}' | head -1)
    if [ ! -z "$AVAILABLE_IF" ]; then
        export NCCL_SOCKET_IFNAME=$AVAILABLE_IF
        export NCCL_IB_DISABLE=1
        echo "  Using interface: $AVAILABLE_IF"
        ip addr show $AVAILABLE_IF | grep inet
    else
        echo "  Warning: No suitable network interface found, using loopback"
        export NCCL_SOCKET_IFNAME=lo
        export NCCL_IB_DISABLE=1
    fi
fi

# Additional NCCL optimizations
export NCCL_ASYNC_ERROR_HANDLING=1
export NCCL_TIMEOUT=600
export NCCL_BUFFSIZE=8388608

# PyTorch distributed settings
export TORCH_DISTRIBUTED_DEBUG=INFO
export TORCH_SHOW_CPP_STACKTRACES=1

# Memory optimization
export PYTORCH_CUDA_ALLOC_CONF="max_split_size_mb:512,garbage_collection_threshold:0.6,expandable_segments:True"
export CUDA_LAUNCH_BLOCKING=0

# Threading optimization
export OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK}
export MKL_NUM_THREADS=${SLURM_CPUS_PER_TASK}

# =============================================================================
# Training Configuration (FIXED)
# =============================================================================

EMBEDDINGS_DIR="/scratch-shared/azadaianchuk1/blip3o_workspace/embeddings/patch_only_256_tokens"
OUTPUT_DIR="./checkpoints_ddp_fixed_$(date +%Y%m%d_%H%M%S)"
TASK_MODE="clip_denoising"

# Model configuration
MODEL_SIZE="base"
TRAINING_MODE="patch_only"
PREDICTION_TYPE="velocity"

# FIXED: Optimized training hyperparameters for DDP
LEARNING_RATE=1e-4
BATCH_SIZE=2          # Per GPU - small to avoid OOM
GRADIENT_ACCUMULATION_STEPS=8  # Higher to maintain effective batch size
NUM_EPOCHS=5
WARMUP_STEPS=100
WEIGHT_DECAY=0.01
MAX_GRAD_NORM=1.0

# FIXED: Memory optimization for DDP
MAX_SHARD_CACHE=2
SAMPLES_PER_SHARD_LOAD=500
MAX_SHARDS=10          # Reduced for initial testing

# Create directories
mkdir -p "${OUTPUT_DIR}"
mkdir -p ./slurm_out

# =============================================================================
# FIXED: Network Verification
# =============================================================================

echo ""
echo "üîç Verifying network configuration..."
echo "Available network interfaces:"
ip addr show | grep -E "^[0-9]+:" | awk '{print $2}' | sed 's/://'

echo ""
echo "Testing connectivity between ranks..."
if [ "$SLURM_NTASKS" -gt 1 ]; then
    echo "Master address reachable: $(ping -c 1 $MASTER_ADDR > /dev/null && echo 'YES' || echo 'NO')"
    echo "Port $MASTER_PORT status: $(netstat -an | grep $MASTER_PORT | wc -l) connections"
fi

# =============================================================================
# FIXED: Launch Training with srun
# =============================================================================

echo ""
echo "üöÄ Launching FIXED Multi-GPU DDP Training..."
echo "============================================="
echo "Configuration Summary:"
echo "  Task: $TASK_MODE"
echo "  Model: $MODEL_SIZE"
echo "  Batch size per GPU: $BATCH_SIZE"
echo "  Gradient accumulation: $GRADIENT_ACCUMULATION_STEPS"
echo "  Effective batch size: $((BATCH_SIZE * SLURM_NTASKS * GRADIENT_ACCUMULATION_STEPS))"
echo "  Learning rate: $LEARNING_RATE"
echo "  Max shards: $MAX_SHARDS"
echo "  Network interface: $NCCL_SOCKET_IFNAME"
echo "============================================="

# FIXED: Use srun with proper task distribution
srun --ntasks=$SLURM_NTASKS \
     --ntasks-per-node=$SLURM_NTASKS_PER_NODE \
     --cpus-per-task=$SLURM_CPUS_PER_TASK \
     --gpus-per-task=1 \
     python train_eva_repro_ddp.py \
    --task_mode "$TASK_MODE" \
    --chunked_embeddings_dir "$EMBEDDINGS_DIR" \
    --output_dir "$OUTPUT_DIR" \
    --model_size "$MODEL_SIZE" \
    --training_mode "$TRAINING_MODE" \
    --prediction_type "$PREDICTION_TYPE" \
    --learning_rate $LEARNING_RATE \
    --batch_size $BATCH_SIZE \
    --gradient_accumulation_steps $GRADIENT_ACCUMULATION_STEPS \
    --num_epochs $NUM_EPOCHS \
    --warmup_steps $WARMUP_STEPS \
    --weight_decay $WEIGHT_DECAY \
    --max_grad_norm $MAX_GRAD_NORM \
    --max_shard_cache $MAX_SHARD_CACHE \
    --samples_per_shard_load $SAMPLES_PER_SHARD_LOAD \
    --max_shards $MAX_SHARDS \
    --sphere_constraint_weight 0.1 \
    --noise_schedule uniform \
    --max_noise_level 0.9 \
    --min_noise_level 0.1 \
    --eval_every_n_steps 250 \
    --eval_num_samples 300 \
    --eval_inference_steps 25 \
    --num_workers 2 \
    --fp16 \
    --debug_mode

TRAINING_EXIT_CODE=$?

echo ""
echo "Training completed with exit code: $TRAINING_EXIT_CODE"
echo "======================================"

# Cleanup
if [ $TRAINING_EXIT_CODE -eq 0 ]; then
    echo "‚úÖ Training completed successfully!"
    echo "üìÅ Output directory: $OUTPUT_DIR"
    echo "üìä Check logs in: ./slurm_out/blip3o_ddp_${SLURM_JOB_ID}.{out,err}"
else
    echo "‚ùå Training failed with exit code: $TRAINING_EXIT_CODE"
    echo "üîç Check error logs for debugging information"
fi

exit $TRAINING_EXIT_CODE