#!/bin/bash
#SBATCH --job-name=blip3o_ddp
#SBATCH --partition=gpu_h100
#SBATCH --nodes=1
#SBATCH --gpus=3
#SBATCH --ntasks=3
#SBATCH --ntasks-per-node=4
#SBATCH --cpus-per-task=12
#SBATCH --time=6:00:00
#SBATCH --mem=160G
#SBATCH --output=./slurm_out/blip3o_ddp_%j.out
#SBATCH --error=./slurm_out/blip3o_ddp_%j.err

# =============================================================================
# BLIP3-o Multi-GPU DDP Training - Memory Optimized for Large Scale
# Fixes OOM issues and enables training on 35+ shards with multiple GPUs
# =============================================================================

echo "🚀 BLIP3-o Multi-GPU DDP Training"
echo "==================================="
echo "Job ID: ${SLURM_JOB_ID}"
echo "Node: $(hostname)"
echo "Time: $(date)"
echo "GPUs: ${SLURM_GPUS_ON_NODE}"
echo "Tasks: ${SLURM_NTASKS}"
echo "CPUs per task: ${SLURM_CPUS_PER_TASK}"
echo "Memory: ${SLURM_MEM_PER_NODE}MB"
echo "==================================="

cd $SLURM_SUBMIT_DIR

# Setup environment
module purge
module load 2024
module load Miniconda3/24.7.1-0
module load CUDA/12.6.0
source activate eva_clip_env

# =============================================================================
# MEMORY OPTIMIZATION CONFIGURATION
# =============================================================================

# Set memory limits to prevent OOM
export PYTORCH_CUDA_ALLOC_CONF="max_split_size_mb:512,garbage_collection_threshold:0.6,max_split_size_mb:32"
export CUDA_LAUNCH_BLOCKING=0
export TORCH_SHOW_CPP_STACKTRACES=1

# Optimize Python memory usage
export PYTHONHASHSEED=42
export OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK}
export MKL_NUM_THREADS=${SLURM_CPUS_PER_TASK}

# =============================================================================
# DDP CONFIGURATION
# =============================================================================

# DDP environment variables
export MASTER_ADDR=$(scontrol show hostname ${SLURM_NODELIST} | head -n 1)
export MASTER_PORT=12355
export WORLD_SIZE=${SLURM_NTASKS}
export NCCL_DEBUG=INFO
export NCCL_TREE_THRESHOLD=0

# Optimize NCCL for H100
export NCCL_IB_DISABLE=1
export NCCL_SOCKET_IFNAME=ib0
export NCCL_NET_GDR_LEVEL=2

echo "DDP Configuration:"
echo "  Master address: $MASTER_ADDR"
echo "  Master port: $MASTER_PORT"
echo "  World size: $WORLD_SIZE"
echo "  GPUs per node: ${SLURM_GPUS_ON_NODE}"

# =============================================================================
# TRAINING CONFIGURATION - MEMORY OPTIMIZED
# =============================================================================

# Paths
EMBEDDINGS_DIR="/scratch-shared/azadaianchuk1/blip3o_workspace/embeddings/patch_only_256_tokens"
OUTPUT_DIR="./checkpoints_ddp_$(date +%Y%m%d_%H%M%S)"
TASK_MODE="clip_denoising"  # or "eva_denoising"

# Model configuration - optimized for memory
MODEL_SIZE="base"  # Reduced from large to base for memory
TRAINING_MODE="patch_only"
PREDICTION_TYPE="velocity"

# Training hyperparameters - MEMORY OPTIMIZED
LEARNING_RATE=1e-4
BATCH_SIZE=2  # Small per-GPU batch size
GRADIENT_ACCUMULATION_STEPS=8  # Large accumulation for effective batch size
NUM_EPOCHS=5
WARMUP_STEPS=100
WEIGHT_DECAY=0.01
MAX_GRAD_NORM=1.0

# Memory optimization parameters - NEW!
MAX_SHARD_CACHE=2  # Reduced cache size
SAMPLES_PER_SHARD_LOAD=500  # Smaller chunks
MAX_SHARDS=35  # Full dataset

# Spherical flow matching
SPHERE_CONSTRAINT_WEIGHT=0.1
NOISE_SCHEDULE="uniform"
MAX_NOISE_LEVEL=0.9
MIN_NOISE_LEVEL=0.1

# Evaluation - optimized for memory
EVAL_EVERY_N_STEPS=250
EVAL_NUM_SAMPLES=300
EVAL_INFERENCE_STEPS=25

# Debugging
DEBUG_MODE=""
OVERFIT_TEST_SIZE=""  # Disabled for full training

# WandB configuration
USE_WANDB=false
WANDB_PROJECT="blip3o-ddp-35shards"
WANDB_RUN_NAME="clip_denoising_ddp_35shards_$(date +%Y%m%d_%H%M%S)"

# Build WandB args
WANDB_ARGS=""
if [ "$USE_WANDB" = true ]; then
    WANDB_ARGS="--use_wandb --wandb_project $WANDB_PROJECT --wandb_run_name $WANDB_RUN_NAME --wandb_tags ddp multi_gpu memory_optimized 35_shards $TASK_MODE"
fi

# =============================================================================
# MEMORY AND SYSTEM MONITORING
# =============================================================================

echo "📊 Pre-training System Status:"
echo "CPU Usage: $(top -bn1 | grep "Cpu(s)" | awk '{print $2}' | awk -F'%' '{print $1}')"
echo "Memory Usage: $(free -h | awk '/^Mem:/ {print $3 "/" $2}')"
echo "GPU Status:"
nvidia-smi --query-gpu=index,name,memory.used,memory.total,utilization.gpu --format=csv,noheader,nounits

# Function to monitor memory during training
monitor_memory() {
    while true; do
        sleep 300  # Check every 5 minutes
        echo "$(date): Memory check - $(free -h | awk '/^Mem:/ {print $3 "/" $2}')"
        nvidia-smi --query-gpu=memory.used,memory.total --format=csv,noheader,nounits | awk '{print "GPU Memory: " $1 "/" $2 " MB"}'
    done
}

# Start background memory monitoring
monitor_memory &
MONITOR_PID=$!

# =============================================================================
# DIRECTORY SETUP
# =============================================================================

# Create output directory
mkdir -p "${OUTPUT_DIR}"
mkdir -p ./slurm_out

echo ""
echo "📁 Directory Configuration:"
echo "  Embeddings: $EMBEDDINGS_DIR"
echo "  Output: $OUTPUT_DIR"
echo "  Logs: ./slurm_out/"

# Verify embeddings directory
if [ ! -d "$EMBEDDINGS_DIR" ]; then
    echo "❌ Embeddings directory not found: $EMBEDDINGS_DIR"
    exit 1
fi

# Count available shards
SHARD_COUNT=$(find "$EMBEDDINGS_DIR" -name "*.pkl" | wc -l)
echo "  Available shards: $SHARD_COUNT"

if [ $SHARD_COUNT -lt $MAX_SHARDS ]; then
    echo "⚠️  Warning: Requested $MAX_SHARDS shards but only $SHARD_COUNT available"
    MAX_SHARDS=$SHARD_COUNT
fi

# =============================================================================
# EFFECTIVE BATCH SIZE CALCULATION
# =============================================================================

EFFECTIVE_BATCH_SIZE=$((BATCH_SIZE * WORLD_SIZE * GRADIENT_ACCUMULATION_STEPS))

echo ""
echo "🔢 Batch Size Configuration:"
echo "  Batch size per GPU: $BATCH_SIZE"
echo "  Number of GPUs: $WORLD_SIZE"
echo "  Gradient accumulation: $GRADIENT_ACCUMULATION_STEPS"
echo "  Effective batch size: $EFFECTIVE_BATCH_SIZE"
echo "  Memory per batch (est.): $((BATCH_SIZE * 256 * 1024 * 4 / 1024 / 1024))MB per GPU (CLIP)"

# =============================================================================
# TRAINING LAUNCH
# =============================================================================

echo ""
echo "🚀 Launching Multi-GPU DDP Training..."
echo "======================================="
echo "Task: $TASK_MODE"
echo "Model: $MODEL_SIZE"
echo "Shards: $MAX_SHARDS"
echo "Memory optimization: Enabled"
echo "Cache size: $MAX_SHARD_CACHE shards"
echo "Samples per load: $SAMPLES_PER_SHARD_LOAD"

if [ "$USE_WANDB" = true ]; then
    echo "WandB: $WANDB_PROJECT/$WANDB_RUN_NAME"
fi
echo ""

# Export all configuration for the training script
export TASK_MODE EMBEDDINGS_DIR OUTPUT_DIR MODEL_SIZE TRAINING_MODE PREDICTION_TYPE
export LEARNING_RATE BATCH_SIZE GRADIENT_ACCUMULATION_STEPS NUM_EPOCHS WARMUP_STEPS
export WEIGHT_DECAY MAX_GRAD_NORM MAX_SHARD_CACHE SAMPLES_PER_SHARD_LOAD MAX_SHARDS
export SPHERE_CONSTRAINT_WEIGHT NOISE_SCHEDULE MAX_NOISE_LEVEL MIN_NOISE_LEVEL
export EVAL_EVERY_N_STEPS EVAL_NUM_SAMPLES EVAL_INFERENCE_STEPS

# Launch training with srun for proper DDP setup
srun python train_eva_repro_ddp.py \
    --task_mode "$TASK_MODE" \
    --chunked_embeddings_dir "$EMBEDDINGS_DIR" \
    --output_dir "$OUTPUT_DIR" \
    --model_size "$MODEL_SIZE" \
    --training_mode "$TRAINING_MODE" \
    --prediction_type "$PREDICTION_TYPE" \
    --learning_rate $LEARNING_RATE \
    --batch_size $BATCH_SIZE \
    --gradient_accumulation_steps $GRADIENT_ACCUMULATION_STEPS \
    --num_epochs $NUM_EPOCHS \
    --warmup_steps $WARMUP_STEPS \
    --weight_decay $WEIGHT_DECAY \
    --max_grad_norm $MAX_GRAD_NORM \
    --max_shard_cache $MAX_SHARD_CACHE \
    --samples_per_shard_load $SAMPLES_PER_SHARD_LOAD \
    --max_shards $MAX_SHARDS \
    --sphere_constraint_weight $SPHERE_CONSTRAINT_WEIGHT \
    --noise_schedule $NOISE_SCHEDULE \
    --max_noise_level $MAX_NOISE_LEVEL \
    --min_noise_level $MIN_NOISE_LEVEL \
    --eval_every_n_steps $EVAL_EVERY_N_STEPS \
    --eval_num_samples $EVAL_NUM_SAMPLES \
    --eval_inference_steps $EVAL_INFERENCE_STEPS \
    --num_workers 2 \
    --fp16 \
    $DEBUG_MODE \
    $WANDB_ARGS

TRAINING_EXIT_CODE=$?

# Stop memory monitoring
kill $MONITOR_PID 2>/dev/null

# =============================================================================
# POST-TRAINING ANALYSIS
# =============================================================================

echo ""
echo "========================================"
echo "📊 Training Results Analysis"
echo "========================================"

if [ $TRAINING_EXIT_CODE -eq 0 ]; then
    echo "✅ Multi-GPU DDP training completed successfully!"
    
    # Analyze results
    if [ -f "${OUTPUT_DIR}/ddp_training_summary.json" ]; then
        echo ""
        echo "📋 Training Summary:"
        echo "=================="
        
        python -c "
import json
import sys
try:
    with open('${OUTPUT_DIR}/ddp_training_summary.json', 'r') as f:
        summary = json.load(f)
    
    print(f'🎯 Task: {summary.get(\"task_mode\", \"unknown\")}')
    print(f'⏱️  Duration: {summary.get(\"total_time_seconds\", 0):.1f} seconds')
    print(f'📊 Steps: {summary.get(\"total_steps\", 0):,}')
    print(f'📈 Best Loss: {summary.get(\"best_loss\", 0):.6f}')
    print(f'🎯 Best Similarity: {summary.get(\"best_eval_similarity\", 0):.4f}')
    
    # DDP info
    ddp_info = summary.get('ddp_info', {})
    print(f'🔢 World Size: {ddp_info.get(\"world_size\", 1)}')
    print(f'📦 Effective Batch Size: {ddp_info.get(\"effective_batch_size\", \"unknown\")}')
    
    # Memory stats
    memory_stats = summary.get('memory_stats', {})
    print(f'💾 Peak Memory: {memory_stats.get(\"peak_memory_gb\", 0):.1f} GB')
    print(f'💾 Final Memory: {memory_stats.get(\"final_memory_gb\", 0):.1f} GB')
    
    # Final evaluation
    final_eval = summary.get('final_eval', {})
    if final_eval:
        task_mode = final_eval.get('eval_task_mode', summary.get('task_mode', 'unknown'))
        if task_mode == 'eva_denoising':
            sim_key = 'eval_eva_similarity'
        elif task_mode == 'clip_denoising':
            sim_key = 'eval_clip_similarity'
        else:
            sim_key = 'eval_generic_similarity'
        
        if sim_key in final_eval:
            final_sim = final_eval[sim_key]
            print(f'🏆 Final Evaluation Similarity: {final_sim:.4f}')
            
            # Success assessment
            if task_mode == 'eva_denoising':
                if final_sim > 0.8:
                    print('🎉 OUTSTANDING: EVA similarity > 0.8!')
                elif final_sim > 0.7:
                    print('🎊 EXCELLENT: EVA similarity > 0.7!')
                elif final_sim > 0.5:
                    print('✅ GOOD: EVA similarity > 0.5!')
                else:
                    print(f'📈 Progress: EVA similarity = {final_sim:.4f}')
            elif task_mode == 'clip_denoising':
                if final_sim > 0.7:
                    print('🎉 OUTSTANDING: CLIP similarity > 0.7!')
                elif final_sim > 0.6:
                    print('🎊 EXCELLENT: CLIP similarity > 0.6!')
                elif final_sim > 0.4:
                    print('✅ GOOD: CLIP similarity > 0.4!')
                else:
                    print(f'📈 Progress: CLIP similarity = {final_sim:.4f}')
                    
            # Quality breakdown
            high_quality = final_eval.get(f'{sim_key.replace(\"similarity\", \"high_quality\")}', 0)
            very_high_quality = final_eval.get(f'{sim_key.replace(\"similarity\", \"very_high_quality\")}', 0)
            excellent_quality = final_eval.get(f'{sim_key.replace(\"similarity\", \"excellent_quality\")}', 0)
            
            print(f'📊 Quality Distribution:')
            print(f'   High Quality (>0.6/0.7): {high_quality*100:.1f}%')
            print(f'   Very High Quality (>0.7/0.8): {very_high_quality*100:.1f}%')
            print(f'   Excellent Quality (>0.8/0.9): {excellent_quality*100:.1f}%')
    
    # WandB link
    wandb_url = summary.get('wandb_url')
    if wandb_url:
        print(f'📊 WandB Dashboard: {wandb_url}')
        
except Exception as e:
    print(f'Could not parse training summary: {e}')
    sys.exit(1)
"
        
        echo ""
        echo "📁 Results saved to: $OUTPUT_DIR"
        echo "📄 Summary: ${OUTPUT_DIR}/ddp_training_summary.json"
        
        # Check for checkpoints
        CHECKPOINT_COUNT=$(find "$OUTPUT_DIR" -name "checkpoint_step_*.pt" | wc -l)
        echo "💾 Checkpoints saved: $CHECKPOINT_COUNT"
        
        if [ $CHECKPOINT_COUNT -gt 0 ]; then
            LATEST_CHECKPOINT=$(find "$OUTPUT_DIR" -name "checkpoint_step_*.pt" | sort -V | tail -1)
            echo "📦 Latest checkpoint: $(basename $LATEST_CHECKPOINT)"
        fi
        
    else
        echo "⚠️ No training summary found"
    fi
    
    echo ""
    echo "🎯 Multi-GPU DDP Training Success!"
    echo "=================================="
    echo "✅ Trained on $MAX_SHARDS shards using $WORLD_SIZE GPUs"
    echo "✅ Memory optimization prevented OOM errors"
    echo "✅ Effective batch size: $EFFECTIVE_BATCH_SIZE"
    echo "✅ Task: $TASK_MODE with spherical flow matching"
    echo ""
    echo "💡 Next Steps:"
    echo "  • Review training curves in WandB (if enabled)"
    echo "  • Run evaluation script for detailed analysis"
    echo "  • Consider fine-tuning hyperparameters if needed"
    echo "  • Scale to more shards or larger model if results are good"
    
else
    echo "❌ FAILED: Multi-GPU DDP training exit code $TRAINING_EXIT_CODE"
    echo ""
    echo "🔍 Debugging Information:"
    echo "========================"
    echo "Exit code: $TRAINING_EXIT_CODE"
    echo "Log files:"
    echo "  - SLURM output: slurm_out/blip3o_ddp_${SLURM_JOB_ID}.out"
    echo "  - SLURM error: slurm_out/blip3o_ddp_${SLURM_JOB_ID}.err"
    
    if [ -d "$OUTPUT_DIR" ]; then
        echo "  - Training logs: $OUTPUT_DIR/training_rank_*.log"
    fi
    
    echo ""
    echo "💡 Troubleshooting:"
    echo "  • Check SLURM logs for OOM or CUDA errors"
    echo "  • Verify embeddings directory and file permissions"
    echo "  • Try reducing batch_size or max_shard_cache if OOM persists"
    echo "  • Check GPU compatibility and CUDA version"
    echo "  • Verify DDP setup and network configuration"
    
    # Final system status
    echo ""
    echo "📊 Final System Status:"
    echo "Memory: $(free -h | awk '/^Mem:/ {print $3 "/" $2}')"
    nvidia-smi --query-gpu=memory.used,memory.total --format=csv,noheader,nounits | awk '{print "GPU Memory: " $1 "/" $2 " MB"}'
fi

echo ""
echo "🏁 Job completed at $(date)"
echo "========================================"

# Final cleanup
if [ -d "$OUTPUT_DIR" ]; then
    echo "📁 Results available in: $OUTPUT_DIR"
fi

exit $TRAINING_EXIT_CODE