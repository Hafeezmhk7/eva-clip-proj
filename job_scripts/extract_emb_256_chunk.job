#!/bin/bash
#SBATCH --partition=gpu_h100
#SBATCH --gpus=1
#SBATCH --job-name=blip3o_extract_chunked
#SBATCH --time=24:00:00
#SBATCH --output=./slurm_out/extract_chunked_%j.out
#SBATCH --error=./slurm_out/extract_chunked_%j.err
#SBATCH --mem=64GB
#SBATCH --cpus-per-task=8
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1

echo "🚀 Starting BLIP3-o CHUNKED Embedding Extraction - 256 TOKENS"
echo "============================================================="
echo "Job ID: $SLURM_JOB_ID"
echo "Node: $SLURMD_NODENAME" 
echo "Date: $(date)"
echo "Working directory: $(pwd)"
echo "User: $(whoami)"
echo "CUDA Visible Devices: $CUDA_VISIBLE_DEVICES"
echo ""
echo "🎯 CHUNKED EXTRACTION APPROACH:"
echo "   • Process each TAR file separately"
echo "   • Save individual pickle files (small chunks)"
echo "   • Avoid disk quota issues completely"
echo "   • Enable scaling to 30+ TAR files (~100k samples)"
echo "   • Perfect for sequential training"

# =============================================================================
# TEMP DIRECTORY SETUP FOR SNELLIUS
# =============================================================================

# Set up temp workspace for large files
if [ -n "$TMPDIR" ]; then
    TEMP_WORKSPACE="$TMPDIR/blip3o_chunked_$SLURM_JOB_ID"
    echo "📁 Using TMPDIR: $TMPDIR"
elif [ -n "$SCRATCH_SHARED" ]; then
    TEMP_WORKSPACE="$SCRATCH_SHARED/$(whoami)/blip3o_chunked_$SLURM_JOB_ID"
    echo "📁 Using SCRATCH_SHARED: $SCRATCH_SHARED"
else:
    # Fallback
    TEMP_WORKSPACE="./temp_chunked_$SLURM_JOB_ID"
    echo "📁 Using local temp: $TEMP_WORKSPACE"
fi

echo "🗂️  TEMP WORKSPACE: $TEMP_WORKSPACE"

# Create directories
mkdir -p slurm_out
mkdir -p "$TEMP_WORKSPACE"
mkdir -p "$TEMP_WORKSPACE/chunked_embeddings"

# Set environment variables to use temp directory
export TORCH_HOME="$TEMP_WORKSPACE/torch_cache"
export HF_HOME="$TEMP_WORKSPACE/huggingface_cache"
export TRANSFORMERS_CACHE="$TEMP_WORKSPACE/transformers_cache"

mkdir -p "$TORCH_HOME" "$HF_HOME" "$TRANSFORMERS_CACHE"

echo "✅ Temp directories created"
echo "   Chunked embeddings: $TEMP_WORKSPACE/chunked_embeddings"
echo "   Model cache: $TORCH_HOME"

# Check available space
echo ""
echo "💾 STORAGE SPACE:"
if command -v df > /dev/null; then
    df -h "$TEMP_WORKSPACE" | tail -1 | awk '{printf "   Total: %s, Used: %s, Available: %s\n", $2, $3, $4}'
fi

# =============================================================================
# REQUIREMENTS CHECK
# =============================================================================

echo ""
echo "🔍 STEP 0: Checking requirements and environment..."
echo "================================================"

# Check Python packages
echo "🐍 Checking Python packages..."
python -c "
import torch
import transformers
import webdataset
import PIL
import numpy as np
print(f'✅ PyTorch: {torch.__version__}')
print(f'✅ Transformers: {transformers.__version__}')
print(f'✅ WebDataset: {webdataset.__version__}')
print(f'✅ PIL: {PIL.__version__}')
print(f'✅ NumPy: {np.__version__}')
" || {
    echo "❌ Missing required packages!"
    echo "Please install requirements:"
    echo "  pip install torch transformers webdataset pillow numpy"
    exit 1
}

# Check CUDA
echo "🎮 Checking CUDA..."
python -c "
import torch
if torch.cuda.is_available():
    print(f'✅ CUDA available: {torch.cuda.get_device_name(0)}')
    print(f'✅ CUDA version: {torch.version.cuda}')
    print(f'✅ GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB')
else:
    print('❌ CUDA not available!')
    exit(1)
" || {
    echo "❌ CUDA check failed!"
    exit 1
}

echo "✅ Requirements check completed"

# =============================================================================
# ENVIRONMENT SETUP
# =============================================================================

# Load modules
module purge
module load 2024
module load Miniconda3/24.7.1-0
module load CUDA/12.6.0

# Activate conda environment
source activate eva_clip_env

echo "✅ Environment activated"

# =============================================================================
# GPU AND SYSTEM INFO
# =============================================================================

echo ""
echo "💾 System Information:"
echo "   GPU: $(nvidia-smi --query-gpu=name --format=csv,noheader,nounits)"
echo "   GPU Memory: $(nvidia-smi --query-gpu=memory.total --format=csv,noheader,nounits) MB"
echo "   CPU Cores: $SLURM_CPUS_PER_TASK"
echo "   System Memory: $SLURM_MEM_PER_NODE MB"

# =============================================================================
# STEP 1: DOWNLOAD TAR FILES (30 SHARDS FOR ~100K SAMPLES)
# =============================================================================

echo ""
echo "📥 STEP 1: Downloading dataset shards (30 shards for ~100k samples)..."
echo "======================================================================"

# Download 30 shards for chunked processing
python src/data_hand/download_data.py --shards $(seq -s ' ' 0 29) --data_dir "$TEMP_WORKSPACE/blip3o_data"

DOWNLOAD_EXIT_CODE=$?

if [ $DOWNLOAD_EXIT_CODE -ne 0 ]; then
    echo "❌ Download failed with exit code: $DOWNLOAD_EXIT_CODE"
    echo "Trying with fewer shards..."
    
    # Fallback: try with 20 shards
    python src/data_hand/download_data.py --shards $(seq -s ' ' 0 19) --data_dir "$TEMP_WORKSPACE/blip3o_data"
    
    DOWNLOAD_EXIT_CODE=$?
    if [ $DOWNLOAD_EXIT_CODE -ne 0 ]; then
        echo "❌ Even reduced download failed. Exiting."
        exit 1
    fi
fi

echo "✅ Dataset download completed"

# =============================================================================
# STEP 2: CHUNKED EMBEDDING EXTRACTION
# =============================================================================

echo ""
echo "🧠 STEP 2: Chunked embedding extraction (one file per TAR)..."
echo "=============================================================="

# Set the environment variable for chunked extraction
export BLIP3O_TEMP_DIR="$TEMP_WORKSPACE"

# Run chunked embedding extraction
python src/modules/extract_embeddings_g.py

EXTRACTION_EXIT_CODE=$?

if [ $EXTRACTION_EXIT_CODE -ne 0 ]; then
    echo "❌ Chunked embedding extraction failed with exit code: $EXTRACTION_EXIT_CODE"
    echo ""
    echo "🔍 Debugging information:"
    echo "   Temp workspace: $TEMP_WORKSPACE"
    echo "   Available space:"
    df -h "$TEMP_WORKSPACE" 2>/dev/null || echo "   Cannot check space"
    echo "   GPU memory:"
    nvidia-smi
    exit 1
fi

echo "✅ Chunked embedding extraction completed successfully!"

# =============================================================================
# STEP 3: VALIDATE CHUNKED EMBEDDINGS
# =============================================================================

echo ""
echo "🧪 STEP 3: Validating chunked embeddings..."
echo "==========================================="

CHUNKED_DIR="$TEMP_WORKSPACE/chunked_embeddings"

if [ ! -d "$CHUNKED_DIR" ]; then
    echo "❌ Chunked embeddings directory not found: $CHUNKED_DIR"
    exit 1
fi

# Check for manifest file
MANIFEST_FILE="$CHUNKED_DIR/embeddings_manifest.json"
if [ ! -f "$MANIFEST_FILE" ]; then
    echo "❌ Manifest file not found: $MANIFEST_FILE"
    exit 1
fi

echo "✅ Found manifest file: $MANIFEST_FILE"

# Count shard files
SHARD_COUNT=$(find "$CHUNKED_DIR" -name "embeddings_shard_*.pkl" | wc -l)
echo "📊 Found $SHARD_COUNT embedding shard files"

# Show total size
TOTAL_SIZE=$(du -sh "$CHUNKED_DIR" | cut -f1)
echo "📊 Total chunked embeddings size: $TOTAL_SIZE"

# Validate manifest
python -c "
import json
import sys

try:
    with open('$MANIFEST_FILE', 'r') as f:
        manifest = json.load(f)
    
    print(f'✅ Manifest validation:')
    print(f'   Total shards: {manifest[\"total_shards\"]}')
    print(f'   Total samples: {manifest[\"total_samples\"]:,}')
    print(f'   Total size: {manifest[\"total_size_mb\"]:.1f} MB')
    print(f'   Format: {manifest[\"format_version\"]}')
    
    if manifest['total_samples'] >= 50000:
        print(f'✅ Excellent! Over 50k samples for robust training')
    elif manifest['total_samples'] >= 20000:
        print(f'✅ Good! Over 20k samples for decent training')
    else:
        print(f'⚠️  Warning: Only {manifest[\"total_samples\"]} samples - consider more shards')
    
except Exception as e:
    print(f'❌ Manifest validation failed: {e}')
    sys.exit(1)
"

VALIDATION_EXIT_CODE=$?

if [ $VALIDATION_EXIT_CODE -ne 0 ]; then
    echo "❌ Manifest validation failed!"
    exit 1
fi

echo "✅ Chunked embeddings validation passed!"

# =============================================================================
# STEP 4: CREATE PROJECT SYMLINKS
# =============================================================================

echo ""
echo "📂 STEP 4: Creating project symlinks (no copying needed)..."
echo "=========================================================="

# Create project embeddings directory
PROJECT_EMBEDDINGS_DIR="./chunked_embeddings"
mkdir -p "$PROJECT_EMBEDDINGS_DIR"

# Create symbolic link to the chunked directory
ln -sf "$CHUNKED_DIR" "$PROJECT_EMBEDDINGS_DIR/temp_chunked"

if [ $? -eq 0 ]; then
    echo "✅ Symbolic link created successfully"
    echo "   Link: $PROJECT_EMBEDDINGS_DIR/temp_chunked"
    echo "   Target: $CHUNKED_DIR"
else
    echo "⚠️  Failed to create symbolic link"
    echo "   Use temp path directly: $CHUNKED_DIR"
fi

# =============================================================================
# CLEANUP AND SUMMARY
# =============================================================================

echo ""
echo "🗑️  CLEANUP: Removing raw data files..."
echo "======================================="

# Remove downloaded tar files to save space (keep embeddings)
if [ -d "$TEMP_WORKSPACE/blip3o_data" ]; then
    TAR_SIZE=$(du -sh "$TEMP_WORKSPACE/blip3o_data" 2>/dev/null | cut -f1 || echo "unknown")
    echo "   Removing downloaded tar files ($TAR_SIZE)..."
    rm -rf "$TEMP_WORKSPACE/blip3o_data"
    echo "   ✅ Tar files removed"
fi

# Keep chunked embeddings in temp for training
echo "   📁 Keeping chunked embeddings in temp: $CHUNKED_DIR"

echo ""
echo "🎉 CHUNKED EXTRACTION COMPLETED SUCCESSFULLY!"
echo "=============================================="
echo ""
echo "📊 SUMMARY:"
echo "   ✅ Downloaded 30 TAR files to temp"
echo "   ✅ Extracted embeddings into $SHARD_COUNT individual chunks"
echo "   ✅ Total size: $TOTAL_SIZE (much smaller individual files)"
echo "   ✅ Validated BLIP3-o compatible format (256 tokens)"
echo "   ✅ Ready for chunked sequential training"
echo ""
echo "📁 LOCATIONS:"
echo "   Temp (for training): $CHUNKED_DIR"
echo "   Project symlink: $PROJECT_EMBEDDINGS_DIR/temp_chunked"
echo ""
echo "🔍 WHAT'S NEXT:"
echo "   1. Start training with chunked dataset"
echo "   2. Model will load one chunk at a time"
echo "   3. Automatic cleanup of processed chunks"
echo "   4. Scale to 100k+ samples without disk issues"
echo ""
echo "📋 CHUNKED FORMAT:"
echo "   - Multiple files: embeddings_shard_00000.pkl, embeddings_shard_00001.pkl, ..."
echo "   - Per shard: [N, 256, 1024] CLIP + [N, 256, 4096] EVA + captions"
echo "   - Sequential loading during training"
echo "   - Automatic cleanup after processing"
echo ""

# =============================================================================
# FINAL STATUS CHECK
# =============================================================================

echo "📊 Final Status Check:"
echo "====================="

# Check chunked embeddings
if [ -d "$CHUNKED_DIR" ]; then
    echo "   ✅ Chunked embeddings: $TOTAL_SIZE"
    echo "   ✅ Shard count: $SHARD_COUNT"
else
    echo "   ❌ Chunked embeddings: NOT FOUND"
fi

# Check project symlink
if [ -L "$PROJECT_EMBEDDINGS_DIR/temp_chunked" ]; then
    echo "   ✅ Project symlink: Created"
else
    echo "   ⚠️  Project symlink: Not created"
fi

# Check remaining temp space
echo "   💾 Remaining temp space:"
df -h "$TEMP_WORKSPACE" 2>/dev/null | tail -1 | awk '{print "      Available: " $4}' || echo "      Cannot check"

# GPU memory status
echo "   🎮 GPU memory:"
nvidia-smi --query-gpu=memory.used,memory.total --format=csv,noheader,nounits | awk '{print "      Used: " $1 "MB / " $2 "MB"}'

echo ""
echo "⏱️  Total runtime: $SECONDS seconds"
echo "🎯 Job completed at: $(date)"

# Final success message
echo ""
echo "🎉 SUCCESS SUMMARY:"
echo "=================="
echo "✅ Multi-shard dataset downloaded (30 shards)"
echo "✅ Chunked embeddings extracted (one file per TAR)"
echo "✅ 256-token format validated"
echo "✅ No disk quota issues (small individual files)"
echo "✅ Ready for sequential chunked training!"
echo ""
echo "Your chunked BLIP3-o embeddings are ready! 🚀"

# Export paths for training
echo ""
echo "📝 TRAINING COMMAND:"
echo "python train_blip3o_dit.py \\"
echo "  --chunked_embeddings_dir \"$CHUNKED_DIR\" \\"
echo "  --output_dir ./checkpoints/blip3o-dit-chunked \\"
echo "  --num_epochs 10 \\"
echo "  --batch_size 32 \\"
echo "  --learning_rate 1e-4 \\"
echo "  --gradient_checkpointing \\"
echo "  --fp16"