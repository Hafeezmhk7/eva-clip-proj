#!/bin/bash
#SBATCH --partition=gpu_h100
#SBATCH --gpus=1
#SBATCH --job-name=blip3o_extract_chunked
#SBATCH --time=24:00:00
#SBATCH --output=./slurm_out/extract_chunked_%j.out
#SBATCH --error=./slurm_out/extract_chunked_%j.err
#SBATCH --mem=64GB
#SBATCH --cpus-per-task=8
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1

echo "🚀 Starting BLIP3-o CHUNKED Embedding Extraction - 256 TOKENS"
echo "============================================================="
echo "Job ID: $SLURM_JOB_ID"
echo "Node: $SLURMD_NODENAME" 
echo "Date: $(date)"
echo "Working directory: $(pwd)"
echo "User: $(whoami)"
echo "CUDA Visible Devices: $CUDA_VISIBLE_DEVICES"
echo ""
echo "🎯 CHUNKED EXTRACTION APPROACH:"
echo "   • Uses structured temp manager"
echo "   • Persistent storage for embeddings (14-day retention)"
echo "   • Automatic cache management"
echo "   • Sequential processing of TAR files"

# =============================================================================
# ENVIRONMENT SETUP
# =============================================================================

echo ""
echo "🗂️ STEP 0: Setting up Snellius environment..."
echo "==============================================="

# Load modules
module purge
module load 2024
module load Miniconda3/24.7.1-0
module load CUDA/12.6.0

# Activate conda environment
source activate eva_clip_env

# CRITICAL: Set up Snellius scratch directories
export SCRATCH_SHARED="/scratch-shared"
export SCRATCH_LOCAL="/scratch-local"

# Set up project directories with proper Snellius paths
export BLIP3O_USER=$(whoami)
export BLIP3O_JOB_ID=${SLURM_JOB_ID}

# Persistent workspace (14-day retention on scratch-shared)
export BLIP3O_WORKSPACE="/scratch-shared/${BLIP3O_USER}/blip3o_workspace"
export BLIP3O_DATASETS="${BLIP3O_WORKSPACE}/datasets"
export BLIP3O_EMBEDDINGS="${BLIP3O_WORKSPACE}/embeddings"
export BLIP3O_CHECKPOINTS="${BLIP3O_WORKSPACE}/checkpoints"
export BLIP3O_LOGS="${BLIP3O_WORKSPACE}/logs"

# Job-specific temp (deleted after job)
export BLIP3O_JOB_TEMP="/scratch-local/${BLIP3O_USER}.${BLIP3O_JOB_ID}/blip3o_job_${BLIP3O_JOB_ID}"
export BLIP3O_CACHE="${BLIP3O_JOB_TEMP}/cache"
export BLIP3O_WORKING="${BLIP3O_JOB_TEMP}/working"

# Create all directories
mkdir -p "${BLIP3O_WORKSPACE}"/{datasets,embeddings,checkpoints,logs,metadata}
mkdir -p "${BLIP3O_JOB_TEMP}"/{cache,working,temp_checkpoints}

# Redirect model caches to job temp (to avoid home directory quota)
export TORCH_HOME="${BLIP3O_CACHE}/torch"
export HF_HOME="${BLIP3O_CACHE}/huggingface"
export TRANSFORMERS_CACHE="${BLIP3O_CACHE}/transformers"
export WANDB_DIR="${BLIP3O_LOGS}/wandb"

# Create cache subdirectories
mkdir -p "${TORCH_HOME}" "${HF_HOME}" "${TRANSFORMERS_CACHE}" "${WANDB_DIR}"

echo "✅ Snellius environment configured:"
echo "   Persistent workspace: ${BLIP3O_WORKSPACE}"
echo "   Job temp: ${BLIP3O_JOB_TEMP}"
echo "   Datasets: ${BLIP3O_DATASETS}"
echo "   Embeddings: ${BLIP3O_EMBEDDINGS}"
echo "   Cache: ${BLIP3O_CACHE}"

# Check disk space
echo ""
echo "💾 Disk space check:"
df -h /scratch-shared 2>/dev/null || echo "   scratch-shared: Not available"
df -h /scratch-local 2>/dev/null || echo "   scratch-local: Not available"
df -h ~ | tail -1 | awk '{print "   Home: " $4 " available (" $5 " used)"}'

# =============================================================================
# STEP 1: DOWNLOAD TAR FILES
# =============================================================================

echo ""
echo "📥 STEP 1: Downloading dataset shards to scratch-shared..."
echo "=========================================================="

# Download 30 shards to the scratch-shared datasets directory
python src/data_hand/download_data.py \
    --shards $(seq -s ' ' 0 29) \
    --data_dir "${BLIP3O_DATASETS}" \
    --force

DOWNLOAD_EXIT_CODE=$?

if [ $DOWNLOAD_EXIT_CODE -ne 0 ]; then
    echo "❌ Download failed with exit code: $DOWNLOAD_EXIT_CODE"
    echo "Trying with fewer shards..."
    
    # Fallback: try with 20 shards
    python src/data_hand/download_data.py \
        --shards $(seq -s ' ' 0 19) \
        --data_dir "${BLIP3O_DATASETS}" \
        --force
    
    DOWNLOAD_EXIT_CODE=$?
    if [ $DOWNLOAD_EXIT_CODE -ne 0 ]; then
        echo "❌ Even reduced download failed. Checking disk space..."
        df -h /scratch-shared /scratch-local ~ 2>/dev/null || true
        echo "Exiting."
        exit 1
    fi
fi

echo "✅ Dataset download completed"

# Check downloaded files
echo "📊 Downloaded files:"
ls -lh "${BLIP3O_DATASETS}"/*.tar 2>/dev/null | head -5
SHARD_COUNT=$(ls "${BLIP3O_DATASETS}"/*.tar 2>/dev/null | wc -l)
echo "Total shards: ${SHARD_COUNT}"

if [ $SHARD_COUNT -eq 0 ]; then
    echo "❌ No TAR files found after download!"
    exit 1
fi

# =============================================================================
# STEP 2: CHUNKED EMBEDDING EXTRACTION
# =============================================================================

echo ""
echo "🧠 STEP 2: Chunked embedding extraction..."
echo "=========================================="

# Set environment variables for the extraction script
export BLIP3O_TEMP_DIR="${BLIP3O_JOB_TEMP}"

# Run chunked embedding extraction
python src/modules/extract_embeddings_g.py

EXTRACTION_EXIT_CODE=$?

if [ $EXTRACTION_EXIT_CODE -ne 0 ]; then
    echo "❌ Chunked embedding extraction failed with exit code: $EXTRACTION_EXIT_CODE"
    echo "Checking disk space and temp directories..."
    df -h /scratch-shared /scratch-local ~ 2>/dev/null || true
    ls -la "${BLIP3O_EMBEDDINGS}" 2>/dev/null || true
    exit 1
fi

echo "✅ Chunked embedding extraction completed successfully!"

# =============================================================================
# STEP 3: VALIDATE EMBEDDINGS
# =============================================================================

echo ""
echo "🧪 STEP 3: Validating embeddings..."
echo "==================================="

# Find embeddings directory (look for chunked_256_tokens subdirectory)
EMBEDDINGS_DIR=$(find "${BLIP3O_EMBEDDINGS}" -name "*chunked*256*" -type d | head -1)

if [ -z "$EMBEDDINGS_DIR" ]; then
    echo "❌ No chunked embeddings directory found in ${BLIP3O_EMBEDDINGS}"
    echo "Available directories:"
    ls -la "${BLIP3O_EMBEDDINGS}" 2>/dev/null || true
    exit 1
fi

echo "✅ Found embeddings directory: $EMBEDDINGS_DIR"

# Check for manifest file
MANIFEST_FILE="$EMBEDDINGS_DIR/embeddings_manifest.json"
if [ ! -f "$MANIFEST_FILE" ]; then
    echo "❌ Manifest file not found: $MANIFEST_FILE"
    echo "Files in embeddings directory:"
    ls -la "$EMBEDDINGS_DIR" 2>/dev/null || true
    exit 1
fi

echo "✅ Found manifest file: $MANIFEST_FILE"

# Count shard files
SHARD_COUNT=$(find "$EMBEDDINGS_DIR" -name "embeddings_shard_*.pkl" | wc -l)
echo "📊 Found $SHARD_COUNT embedding shard files"

# Show total size
TOTAL_SIZE=$(du -sh "$EMBEDDINGS_DIR" | cut -f1)
echo "📊 Total embeddings size: $TOTAL_SIZE"

# Validate manifest
python -c "
import json
import sys

try:
    with open('$MANIFEST_FILE', 'r') as f:
        manifest = json.load(f)
    
    print(f'✅ Manifest validation:')
    print(f'   Total shards: {manifest[\"total_shards\"]}')
    print(f'   Total samples: {manifest[\"total_samples\"]:,}')
    print(f'   Total size: {manifest[\"total_size_mb\"]:.1f} MB')
    print(f'   Format: {manifest[\"format_version\"]}')
    
    if manifest['total_samples'] >= 50000:
        print(f'✅ Excellent! Over 50k samples for robust training')
    elif manifest['total_samples'] >= 20000:
        print(f'✅ Good! Over 20k samples for decent training')
    else:
        print(f'⚠️  Warning: Only {manifest[\"total_samples\"]} samples - consider more shards')
    
except Exception as e:
    print(f'❌ Manifest validation failed: {e}')
    sys.exit(1)
"

VALIDATION_EXIT_CODE=$?

if [ $VALIDATION_EXIT_CODE -ne 0 ]; then
    echo "❌ Manifest validation failed!"
    exit 1
fi

echo "✅ Embeddings validation passed!"

# =============================================================================
# STEP 4: CLEANUP AND FINALIZE
# =============================================================================

echo ""
echo "🧹 STEP 4: Cleaning up job temp files..."
echo "======================================="

# Clean up model cache to save space (keeping only essential files)
if [ -d "${BLIP3O_CACHE}" ]; then
    CACHE_SIZE_BEFORE=$(du -sh "${BLIP3O_CACHE}" | cut -f1)
    echo "Cache size before cleanup: $CACHE_SIZE_BEFORE"
    
    # Remove large cache files but keep small config files
    find "${BLIP3O_CACHE}" -name "*.bin" -size +100M -delete 2>/dev/null || true
    find "${BLIP3O_CACHE}" -name "*.safetensors" -size +100M -delete 2>/dev/null || true
    
    CACHE_SIZE_AFTER=$(du -sh "${BLIP3O_CACHE}" | cut -f1)
    echo "Cache size after cleanup: $CACHE_SIZE_AFTER"
fi

# Create summary file
SUMMARY_FILE="${BLIP3O_EMBEDDINGS}/extraction_summary_${SLURM_JOB_ID}.json"
cat > "$SUMMARY_FILE" << EOF
{
    "job_id": "${SLURM_JOB_ID}",
    "completion_time": "$(date -Iseconds)",
    "user": "${BLIP3O_USER}",
    "embeddings_directory": "$EMBEDDINGS_DIR",
    "datasets_directory": "${BLIP3O_DATASETS}",
    "total_shards_processed": $SHARD_COUNT,
    "storage_location": "scratch-shared",
    "retention_policy": "14_days",
    "next_steps": {
        "training_command": "sbatch job_scripts/train_flow_match_256.job",
        "embeddings_path": "$EMBEDDINGS_DIR"
    }
}
EOF

echo "✅ Summary saved to: $SUMMARY_FILE"

# =============================================================================
# FINAL STEPS
# =============================================================================

echo ""
echo "🎉 CHUNKED EXTRACTION COMPLETED SUCCESSFULLY!"
echo "=============================================="
echo ""
echo "📁 EMBEDDINGS LOCATION:"
echo "   $EMBEDDINGS_DIR"
echo ""
echo "🔍 WHAT'S NEXT:"
echo "   1. Start training: sbatch job_scripts/train_flow_match_256.job"
echo "   2. Embeddings will auto-discover from: $EMBEDDINGS_DIR"
echo ""
echo "📋 STORAGE INFO:"
echo "   • Embeddings stored in scratch-shared (persistent)"
echo "   • 14-day automatic retention"
echo "   • Accessible across different jobs"
echo "   • Job temp cleaned automatically"
echo ""
echo "💾 FINAL DISK USAGE:"
df -h /scratch-shared /scratch-local ~ 2>/dev/null | grep -v "Filesystem" || true
echo ""
echo "⏱️  Job completed at: $(date)"