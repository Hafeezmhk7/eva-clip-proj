#!/bin/bash
#SBATCH --partition=gpu_h100
#SBATCH --gpus=1
#SBATCH --job-name=blip3o_dit_256_chunked_train
#SBATCH --time=24:00:00
#SBATCH --output=./slurm_out/train_chunked_%j.out
#SBATCH --error=./slurm_out/train_chunked_%j.err
#SBATCH --mem=64GB
#SBATCH --cpus-per-task=8
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1

echo "üöÄ Starting BLIP3-o DiT CHUNKED Training Job - 256 TOKENS"
echo "========================================================"
echo "Job ID: $SLURM_JOB_ID"
echo "Node: $SLURMD_NODENAME" 
echo "Date: $(date)"
echo "Working directory: $(pwd)"
echo "User: $(whoami)"
echo "CUDA Visible Devices: $CUDA_VISIBLE_DEVICES"

# =============================================================================
# TEMP MANAGER SETUP
# =============================================================================

echo ""
echo "üóÇÔ∏è STEP 0: Setting up structured temp environment..."
echo "==================================================="

# Load modules
module purge
module load 2024
module load Miniconda3/24.7.1-0
module load CUDA/12.6.0

# Activate conda environment
source activate eva_clip_env

# Run Python to initialize temp manager
python -c "
import sys
sys.path.insert(0, 'src/utils')
from temp_manager import setup_snellius_environment
manager = setup_snellius_environment('blip3o_workspace')
manager.setup_model_cache()
print(f'‚úÖ Temp manager initialized:')
print(f'   Persistent workspace: {manager.persistent_workspace}')
print(f'   Job temp: {manager.job_temp}')
print(f'   Embeddings dir: {manager.get_embeddings_dir()}')
print(f'   Checkpoints dir: {manager.get_checkpoints_dir()}')
"

echo "‚úÖ Environment setup completed"

# =============================================================================
# START TRAINING
# =============================================================================

echo ""
echo "üöÄ STEP 1: Starting training with structured temp management..."
echo "==============================================================="

# Run training with auto-discovery of embeddings
python train_blip3o_dit.py \
  --auto_find_embeddings \
  --output_dir "./checkpoints/blip3o-dit-temp" \
  --batch_size 64 \
  --num_epochs 5 \
  --model_dim 512 \
  --num_heads 8 \
  --num_layers 24 \
  --learning_rate 5e-5 \
  --weight_decay 0.01 \
  --warmup_steps 20 \
  --gradient_accumulation_steps 1 \
  --logging_steps 10 \
  --save_steps 200 \
  --eval_steps 50 \
  --fp16 \
  --gradient_checkpointing \
  --normalize_embeddings \
  --delete_after_use \
  --wandb_project "blip3o-dit-256-tokens-temp" \
  --wandb_run_name "chunked-256-tokens-$(date +%Y%m%d-%H%M%S)"

TRAINING_EXIT_CODE=$?

echo ""
echo "üéØ TRAINING COMPLETED WITH EXIT CODE: $TRAINING_EXIT_CODE"

# =============================================================================
# FINAL STEPS
# =============================================================================

if [ $TRAINING_EXIT_CODE -eq 0 ]; then
    echo ""
    echo "‚úÖ BLIP3-o DiT training completed successfully!"
    echo ""
    echo "üìÅ Model saved to persistent checkpoints directory"
    echo "üìÅ Final model archived in home directory"
else
    echo "‚ùå Training failed with exit code: $TRAINING_EXIT_CODE"
    exit 1
fi

echo ""
echo "üéâ BLIP3-o DiT Chunked Training Job completed at: $(date)"
echo "‚è±Ô∏è Total runtime: $SECONDS seconds"