#!/bin/bash
#SBATCH --job-name=blip3o_enhanced_training
#SBATCH --partition=gpu_h100
#SBATCH --nodes=1
#SBATCH --gpus=3
#SBATCH --cpus-per-gpu=18
#SBATCH --time=12:00:00
#SBATCH --mem=0
#SBATCH --output=./slurm_out/blip3o_enhanced_%j.out
#SBATCH --error=./slurm_out/blip3o_enhanced_%j.err

echo "üöÄ BLIP3-o Enhanced Patch-Level DiT Training"
echo "================================================================"
echo "üéØ ENHANCED TRAINING FEATURES:"
echo "  ‚úÖ 256-token patch-level flow matching"
echo "  ‚úÖ EVA-CLIP conditioning (4096-dim)"
echo "  ‚úÖ CLIP output supervision (1024-dim)"
echo "  ‚ùå Evaluation COMPLETELY DISABLED (pure training)"
echo "  ‚úÖ 3D Rotary Position Embedding"
echo "  ‚úÖ ENHANCED: Cosine LR scheduling with decay"
echo "  ‚úÖ ENHANCED: Convergence monitoring"
echo "  ‚úÖ ENHANCED: Optimized hyperparameters"
echo "  ‚úÖ NO EVALUATION = NO GRADIENT ISSUES"
echo "================================================================"
echo "Job ID: $SLURM_JOB_ID"
echo "Node: $SLURMD_NODENAME"
echo "Date: $(date)"
echo "Working directory: $(pwd)"
echo "User: $(whoami)"
echo "GPUs requested: $SLURM_GPUS"
echo "CPUs per GPU: $SLURM_CPUS_PER_GPU"
echo "================================================================"

# =============================================================================
# ENVIRONMENT SETUP
# =============================================================================

echo ""
echo "üîß Enhanced Environment Setup..."
echo "==============================="

# Change to submit directory
cd $SLURM_SUBMIT_DIR
echo "‚úÖ Working directory: $(pwd)"

# Purge modules and load required ones
module purge
echo "üì¶ Loading modules..."

# Load modules
module load 2024
module load Miniconda3/24.7.1-0
module load CUDA/12.6.0

echo "‚úÖ Modules loaded successfully"

# Activate conda environment
source activate eva_clip_env
echo "‚úÖ Conda environment activated: $CONDA_DEFAULT_ENV"

# =============================================================================
# ENHANCED GPU ENVIRONMENT SETUP
# =============================================================================

echo ""
echo "üéÆ Enhanced GPU Environment Setup..."
echo "==================================="

# Print initial SLURM GPU allocation
echo "üìä SLURM GPU Variables:"
echo "  SLURM_GPUS: $SLURM_GPUS"
echo "  SLURM_GPUS_ON_NODE: $SLURM_GPUS_ON_NODE"
echo "  SLURM_LOCALID: $SLURM_LOCALID"
echo "  SLURM_PROCID: $SLURM_PROCID"
echo "  CUDA_VISIBLE_DEVICES: $CUDA_VISIBLE_DEVICES"

# Apply enhanced GPU fixes
echo ""
echo "üîß Applying enhanced GPU environment fixes..."

# Fix CUDA_VISIBLE_DEVICES if empty
if [ -z "$CUDA_VISIBLE_DEVICES" ] || [ "$CUDA_VISIBLE_DEVICES" = "" ]; then
    echo "‚ö†Ô∏è  CUDA_VISIBLE_DEVICES is empty - applying fix..."
    
    if [ ! -z "$SLURM_LOCALID" ]; then
        export CUDA_VISIBLE_DEVICES=$SLURM_LOCALID
        echo "‚úÖ Set CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES (from SLURM_LOCALID)"
    elif [ ! -z "$SLURM_GPUS" ]; then
        export CUDA_VISIBLE_DEVICES=$(seq -s, 0 $((SLURM_GPUS-1)))
        echo "‚úÖ Set CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES (from SLURM_GPUS)"
    fi
fi

# Enhanced NCCL settings for convergence optimization
echo ""
echo "üîß Optimizing enhanced NCCL settings..."

export NCCL_IB_DISABLE=0
export NCCL_NET_GDR_LEVEL=3
export NCCL_P2P_LEVEL=NVL
export NCCL_ASYNC_ERROR_HANDLING=1
export NCCL_DEBUG=WARN
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512,expandable_segments:True
export OMP_NUM_THREADS=1
export TOKENIZERS_PARALLELISM=false
export CUDA_LAUNCH_BLOCKING=0

echo "‚úÖ Enhanced GPU environment optimized"

# =============================================================================
# ENHANCED WORKSPACE SETUP
# =============================================================================

echo ""
echo "üìÅ Enhanced Workspace Setup..."
echo "=============================="

# Setup directories
export BLIP3O_USER=$(whoami)
export BLIP3O_JOB_ID=${SLURM_JOB_ID}

# Use scratch-shared for persistent storage
export BLIP3O_WORKSPACE="/scratch-shared/${BLIP3O_USER}/blip3o_workspace"
export BLIP3O_EMBEDDINGS="${BLIP3O_WORKSPACE}/embeddings"
export BLIP3O_CHECKPOINTS="${BLIP3O_WORKSPACE}/checkpoints"

# Job-specific temp directory
if [ ! -z "$TMPDIR" ]; then
    export BLIP3O_JOB_TEMP="$TMPDIR/blip3o_job_${BLIP3O_JOB_ID}"
else
    export BLIP3O_JOB_TEMP="/tmp/blip3o_job_${BLIP3O_USER}_${BLIP3O_JOB_ID}"
fi

# Create directories
echo "üìÅ Creating enhanced workspace directories..."
mkdir -p "${BLIP3O_WORKSPACE}"/{embeddings,checkpoints,logs,enhanced}
mkdir -p "${BLIP3O_JOB_TEMP}"/{cache,working,enhanced}

# Redirect model caches to job temp
export TORCH_HOME="${BLIP3O_JOB_TEMP}/cache/torch"
export HF_HOME="${BLIP3O_JOB_TEMP}/cache/huggingface"
export TRANSFORMERS_CACHE="${BLIP3O_JOB_TEMP}/cache/transformers"

mkdir -p "${TORCH_HOME}" "${HF_HOME}" "${TRANSFORMERS_CACHE}"

echo "‚úÖ Enhanced workspace configured:"
echo "  Workspace: ${BLIP3O_WORKSPACE}"
echo "  Job temp: ${BLIP3O_JOB_TEMP}"
echo "  Model cache: ${TORCH_HOME}"

# =============================================================================
# ENHANCED GPU DIAGNOSTICS
# =============================================================================

echo ""
echo "üß™ Enhanced GPU Diagnostics..."
echo "============================="

echo "üîç Python CUDA check:"
python -c "
import torch
print(f'PyTorch version: {torch.__version__}')
print(f'CUDA available: {torch.cuda.is_available()}')
if torch.cuda.is_available():
    print(f'CUDA version: {torch.version.cuda}')
    print(f'GPU count: {torch.cuda.device_count()}')
    for i in range(torch.cuda.device_count()):
        props = torch.cuda.get_device_properties(i)
        memory_gb = props.total_memory / (1024**3)
        print(f'  GPU {i}: {props.name} ({memory_gb:.1f} GB)')
        
        try:
            test_tensor = torch.randn(100, 100, device=f'cuda:{i}')
            print(f'    ‚úÖ GPU {i} accessible and working')
        except Exception as e:
            print(f'    ‚ùå GPU {i} error: {e}')
else:
    print('‚ùå CUDA not available!')
"

echo ""
echo "üîç nvidia-smi check:"
if command -v nvidia-smi &> /dev/null; then
    nvidia-smi --query-gpu=index,name,memory.total,memory.used --format=csv,noheader,nounits
else
    echo "‚ùå nvidia-smi not available"
fi

# =============================================================================
# FIND EMBEDDINGS
# =============================================================================

echo ""
echo "üîç Finding patch-level embeddings..."
echo "==================================="

EMBEDDINGS_DIR=""

SEARCH_LOCATIONS=(
    "${BLIP3O_EMBEDDINGS}/chunked_256_tokens"
    "${BLIP3O_EMBEDDINGS}"
    "./embeddings/chunked_256_tokens"
    "./embeddings"
)

for location in "${SEARCH_LOCATIONS[@]}"; do
    if [ -d "$location" ] && [ -f "$location/embeddings_manifest.json" ]; then
        EMBEDDINGS_DIR="$location"
        echo "‚úÖ Found embeddings: $EMBEDDINGS_DIR"
        break
    fi
done

if [ -z "$EMBEDDINGS_DIR" ]; then
    echo "‚ùå No patch-level embeddings found!"
    echo "üí° Searched locations:"
    for location in "${SEARCH_LOCATIONS[@]}"; do
        echo "    $location"
    done
    echo ""
    echo "üí° Please run embedding extraction first:"
    echo "    python src/modules/extract_embeddings_g.py"
    exit 1
fi

# Verify embeddings
MANIFEST_FILE="${EMBEDDINGS_DIR}/embeddings_manifest.json"
if [ -f "$MANIFEST_FILE" ]; then
    echo "üìä Dataset info:"
    python -c "
import json
try:
    with open('$MANIFEST_FILE') as f:
        manifest = json.load(f)
    print(f'  Total shards: {manifest.get(\"total_shards\", \"unknown\")}')
    print(f'  Total samples: {manifest.get(\"total_samples\", \"unknown\"):,}')
    print(f'  Format: {manifest.get(\"format_version\", \"unknown\")}')
    print(f'  Tokens per image: 256 (patch-level)')
    print(f'  CLIP dimension: 1024')
    print(f'  EVA dimension: 4096')
except Exception as e:
    print(f'  Error reading manifest: {e}')
"
else
    echo "‚ùå Manifest file missing: $MANIFEST_FILE"
    exit 1
fi

# =============================================================================
# CREATE ENHANCED OUTPUT DIRECTORY
# =============================================================================

echo ""
echo "üìÇ Setting up enhanced output directory..."
echo "========================================="

TIMESTAMP=$(date +%Y%m%d_%H%M%S)
OUTPUT_DIR="./checkpoints/blip3o_enhanced_${SLURM_JOB_ID}_${TIMESTAMP}"

mkdir -p "$OUTPUT_DIR"
mkdir -p "./slurm_out"

echo "‚úÖ Enhanced output directory: $OUTPUT_DIR"

# =============================================================================
# ENHANCED TRAINING CONFIGURATION
# =============================================================================

echo ""
echo "‚öôÔ∏è Enhanced Training Configuration..."
echo "===================================="

# Enhanced training configuration (optimized for convergence)
NUM_EPOCHS=10                    # Enhanced from 6 to 10
BATCH_SIZE=24
MODEL_SIZE="base"
HIDDEN_SIZE=768
NUM_LAYERS=12
NUM_HEADS=12
GRADIENT_ACCUMULATION=2          # Enhanced from 4 to 2
WARMUP_STEPS=150                 # Enhanced from 200 to 150
LEARNING_RATE=0.0002             # Enhanced to 2e-4
CONTRASTIVE_WEIGHT=0.15          # Enhanced from 0.1 to 0.15
LR_SCHEDULER_TYPE="cosine"       # Enhanced scheduler
LR_END_RATIO=0.1                 # Enhanced decay ratio
NUM_CYCLES=1.0                   # Enhanced cycles
WARMUP_RATIO=0.02                # Enhanced warmup ratio

# Calculate effective batch size
ACTUAL_GPU_COUNT=$(python -c "import torch; print(torch.cuda.device_count() if torch.cuda.is_available() else 1)")
EFFECTIVE_BATCH_SIZE=$((BATCH_SIZE * ACTUAL_GPU_COUNT * GRADIENT_ACCUMULATION))

echo "Enhanced Training Configuration:"
echo "  üéØ Target GPUs: ${SLURM_GPUS:-1}"
echo "  üîç Detected GPUs: $ACTUAL_GPU_COUNT"
echo "  üèóÔ∏è  Model: ${MODEL_SIZE} (${HIDDEN_SIZE}D, ${NUM_LAYERS}L, ${NUM_HEADS}H)"
echo "  üìö Epochs: $NUM_EPOCHS"
echo "  üì¶ Batch size per GPU: $BATCH_SIZE"
echo "  üîÑ Gradient accumulation: $GRADIENT_ACCUMULATION"
echo "  üìä Effective batch size: $EFFECTIVE_BATCH_SIZE"
echo "  üìà Learning rate: $LEARNING_RATE"
echo "  üî• Warmup steps: $WARMUP_STEPS"
echo "  ‚öñÔ∏è Contrastive weight: $CONTRASTIVE_WEIGHT"
echo "  üìä LR scheduler: $LR_SCHEDULER_TYPE"
echo "  üìâ LR end ratio: $LR_END_RATIO"
echo "  üîÅ Cycles: $NUM_CYCLES"
echo "  üéØ Architecture: 256-token patch-level DiT"
echo "  üìê Input: EVA-CLIP (256√ó4096) ‚Üí CLIP (256√ó1024)"
echo "  ‚ùå Evaluation: COMPLETELY DISABLED"
echo "  ‚ö° Mode: ENHANCED PURE TRAINING"
echo "  üîß Features: Convergence monitoring, optimized scheduling"

# =============================================================================
# ENHANCED FINAL VERIFICATION
# =============================================================================

echo ""
echo "üîç Enhanced final verification..."
echo "==============================="

# Verify enhanced Python imports
echo "üêç Verifying enhanced BLIP3-o modules..."
python -c "
import sys
from pathlib import Path
sys.path.insert(0, str(Path.cwd() / 'src'))

try:
    from src.modules.models.blip3o_patch_dit import BLIP3oPatchDiTModel
    print('‚úÖ Patch DiT model')
    
    from src.modules.losses.blip3o_flow_matching_loss import BLIP3oFlowMatchingLoss
    print('‚úÖ Flow matching loss')
    
    from src.modules.trainers.blip3o_patch_trainer_enhanced import BLIP3oPatchTrainerEnhanced
    print('‚úÖ Enhanced patch trainer')
    
    from src.modules.datasets.blip3o_dataset import BLIP3oEmbeddingDataset
    print('‚úÖ Patch dataset')
    
    print('üéâ All enhanced modules ready!')
    
except ImportError as e:
    print(f'‚ùå Import error: {e}')
    sys.exit(1)
"

if [ $? -ne 0 ]; then
    echo "‚ùå Enhanced module verification failed!"
    exit 1
fi

# =============================================================================
# START ENHANCED TRAINING
# =============================================================================

echo ""
echo "üöÄ Starting Enhanced BLIP3-o Training..."
echo "========================================"
echo "üéØ ENHANCED TRAINING FEATURES:"
echo "  ‚Ä¢ 256-token patch embeddings (16√ó16 grid)"
echo "  ‚Ä¢ EVA-CLIP conditioning (4096-dim)"
echo "  ‚Ä¢ CLIP patch supervision (1024-dim)"
echo "  ‚Ä¢ Enhanced flow matching training objective"
echo "  ‚Ä¢ 3D Rotary Position Embedding"
echo "  ‚Ä¢ Enhanced cosine LR scheduling with decay"
echo "  ‚Ä¢ Advanced convergence monitoring"
echo "  ‚Ä¢ Optimized hyperparameters for convergence"
echo "  ‚Ä¢ Multi-GPU distributed training"
echo "  ‚Ä¢ NO EVALUATION = NO GRADIENT ISSUES"
echo ""
echo "üìà ENHANCED EXPECTED RESULTS:"
echo "  ‚Ä¢ Superior convergence with cosine scheduling"
echo "  ‚Ä¢ Advanced monitoring and progress tracking"
echo "  ‚Ä¢ Optimized hyperparameters for best performance"
echo "  ‚Ä¢ Smooth training completion without interruptions"
echo "  ‚Ä¢ Model checkpoints saved periodically"
echo "  ‚Ä¢ No evaluation-related crashes"
echo ""

TRAIN_START_TIME=$(date +%s)

echo "üî• Launching enhanced training mode..."

# ENHANCED: Use enhanced training script with all optimizations
python train_blip3o_patch_enhanced.py \
    --chunked_embeddings_dir "$EMBEDDINGS_DIR" \
    --output_dir "$OUTPUT_DIR" \
    --model_size "$MODEL_SIZE" \
    --hidden_size $HIDDEN_SIZE \
    --num_layers $NUM_LAYERS \
    --num_heads $NUM_HEADS \
    --num_epochs $NUM_EPOCHS \
    --batch_size $BATCH_SIZE \
    --learning_rate $LEARNING_RATE \
    --lr_scheduler_type "$LR_SCHEDULER_TYPE" \
    --lr_end_ratio $LR_END_RATIO \
    --num_cycles $NUM_CYCLES \
    --warmup_ratio $WARMUP_RATIO \
    --weight_decay 0.01 \
    --warmup_steps $WARMUP_STEPS \
    --gradient_accumulation_steps $GRADIENT_ACCUMULATION \
    --fp16 \
    --dataloader_num_workers 4 \
    --use_contrastive_loss \
    --contrastive_weight $CONTRASTIVE_WEIGHT \
    --enhanced_loss \
    --disable_evaluation \
    --convergence_monitoring \
    --enhanced_logging

TRAINING_EXIT_CODE=$?
TRAIN_END_TIME=$(date +%s)
TRAIN_DURATION=$((TRAIN_END_TIME - TRAIN_START_TIME))

echo ""
echo "üèÅ ENHANCED TRAINING COMPLETED"
echo "============================="
echo "Exit code: $TRAINING_EXIT_CODE"
echo "Duration: $TRAIN_DURATION seconds ($(($TRAIN_DURATION / 60)) minutes)"

# =============================================================================
# ENHANCED RESULTS AND CLEANUP
# =============================================================================

if [ $TRAINING_EXIT_CODE -eq 0 ]; then
    echo ""
    echo "‚úÖ SUCCESS! Enhanced training completed!"
    echo "======================================"
    
    # Copy to persistent storage
    if [ -d "$OUTPUT_DIR" ]; then
        PERSISTENT_DIR="${BLIP3O_CHECKPOINTS}/$(basename $OUTPUT_DIR)"
        mkdir -p "$PERSISTENT_DIR"
        
        echo "üìÅ Copying enhanced model to persistent storage..."
        if cp -r "$OUTPUT_DIR"/* "$PERSISTENT_DIR/" 2>/dev/null; then
            echo "‚úÖ Enhanced model copied to: $PERSISTENT_DIR"
        else
            echo "‚ö†Ô∏è  Copy failed, model remains at: $OUTPUT_DIR"
        fi
    fi
    
    echo ""
    echo "üéâ ENHANCED TRAINING SUCCESS!"
    echo "============================"
    echo "‚úÖ 256-token patch-level flow matching completed"
    echo "‚úÖ EVA-CLIP ‚Üí CLIP patch translation trained"
    echo "‚úÖ Enhanced convergence optimization successful"
    echo "‚úÖ Advanced monitoring and progress tracking"
    echo "‚úÖ No evaluation interruptions or gradient issues"
    echo "‚úÖ BLIP3-o paper alignment achieved"
    echo "‚úÖ Enhanced model ready for inference and evaluation"
    echo ""
    echo "üìÅ Enhanced model locations:"
    echo "   Primary: $OUTPUT_DIR"
    if [ -d "$PERSISTENT_DIR" ]; then
        echo "   Persistent: $PERSISTENT_DIR"
    fi
    echo ""
    echo "üß™ Next steps for enhanced evaluation:"
    echo "   1. Run separate recall evaluation: python eval_blip3o_patch_recall.py"
    echo "   2. Test enhanced inference capabilities"
    echo "   3. Compare with CLIP baseline performance"
    echo "   4. Generate enhanced embeddings for downstream tasks"
    echo "   5. Analyze convergence metrics and training progress"
    
else
    echo ""
    echo "‚ùå ENHANCED TRAINING FAILED"
    echo "=========================="
    echo "Exit code: $TRAINING_EXIT_CODE"
    echo ""
    echo "üîç Enhanced Debugging Information:"
    echo "================================"
    echo "1. Check error log: ${SLURM_JOB_ID}.err"
    echo "2. Check training logs in: $OUTPUT_DIR"
    echo "3. Check GPU diagnostics output above"
    echo "4. Check enhanced training logs"
    echo ""
    echo "üí° Enhanced solutions:"
    echo "  ‚Ä¢ GPU allocation: Check SLURM GPU request"
    echo "  ‚Ä¢ Memory issues: Reduce batch size"
    echo "  ‚Ä¢ Module loading: Check conda environment"
    echo "  ‚Ä¢ Embeddings: Verify patch-level embeddings exist"
    echo "  ‚Ä¢ Enhanced code: Ensure latest enhanced trainer files are used"
    echo "  ‚Ä¢ Dependencies: Check enhanced module imports"
fi

# Enhanced cleanup
echo ""
echo "üßπ Enhanced Cleanup..."
echo "===================="

if [ -d "${BLIP3O_JOB_TEMP}/cache" ]; then
    CACHE_SIZE=$(du -sh "${BLIP3O_JOB_TEMP}/cache" 2>/dev/null | cut -f1)
    rm -rf "${BLIP3O_JOB_TEMP}/cache" 2>/dev/null
    echo "‚úÖ Cleaned up enhanced model cache ($CACHE_SIZE)"
fi

echo ""
echo "üèÅ Enhanced job completed at $(date)"
echo "Total runtime: $SECONDS seconds"
echo "================================================================"

exit $TRAINING_EXIT_CODE