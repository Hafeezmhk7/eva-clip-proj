#!/bin/bash
#SBATCH --job-name=blip3o_evaluation_fair
#SBATCH --partition=gpu_h100
#SBATCH --nodes=1
#SBATCH --gpus=1
#SBATCH --cpus-per-gpu=18
#SBATCH --time=4:00:00
#SBATCH --mem=0
#SBATCH --output=./slurm_out/evaluation_%j.out
#SBATCH --error=./slurm_out/evaluation_%j.err

echo "🔍 BLIP3-o DiT Model Evaluation on MS-COCO - UPDATED WITH FAIR COMPARISON"
echo "=========================================================================="
echo "🎯 IMPROVEMENT: Now uses CLIP's visual projection for fair comparison!"
echo "Both methods compared in CLIP's aligned 768-dimensional embedding space"
echo ""
echo "Tasks:"
echo "  📊 Task 1: Alignment Evaluation (Cosine Similarity in CLIP-aligned space)"
echo "  🎯 Task 2: Recall Evaluation (Image-to-Text Retrieval in CLIP-aligned space)"
echo "=========================================================================="
echo "Job ID: $SLURM_JOB_ID"
echo "Node: $SLURMD_NODENAME"
echo "Date: $(date)"
echo "Working directory: $(pwd)"
echo "User: $(whoami)"
echo "GPUs requested: $SLURM_GPUS"
echo "CPUs per GPU: $SLURM_CPUS_PER_GPU"
echo "=========================================================================="

# =============================================================================
# ENVIRONMENT SETUP (Same as training script)
# =============================================================================

echo ""
echo "🔧 Environment Setup..."
echo "======================="

# Load modules (same as training)
module purge
module load 2024
module load Miniconda3/24.7.1-0
module load CUDA/12.6.0

# Activate conda environment
source activate eva_clip_env

# Environment check
echo "=== Environment Check ==="
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA: {torch.version.cuda}'); print(f'Devices: {torch.cuda.device_count()}')"
echo "========================="

# Check GPU availability
if [ $(python -c "import torch; print(torch.cuda.device_count())") -eq 0 ]; then
    echo "❌ No GPUs available! Check SLURM allocation."
    exit 1
fi

echo "🎮 GPU setup:"
echo "   GPUs available: $(python -c "import torch; print(torch.cuda.device_count())")"
echo "   GPU name: $(python -c "import torch; print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None')")"

# Set up directories (same as training)
export BLIP3O_USER=$(whoami)
export BLIP3O_JOB_ID=${SLURM_JOB_ID}

# Persistent workspace
export BLIP3O_WORKSPACE="/scratch-shared/${BLIP3O_USER}/blip3o_workspace"
export BLIP3O_CHECKPOINTS="${BLIP3O_WORKSPACE}/checkpoints"

# Job-specific temp for evaluation results
export BLIP3O_JOB_TEMP="/scratch-local/${BLIP3O_USER}.${BLIP3O_JOB_ID}/blip3o_eval_${BLIP3O_JOB_ID}"

# Create directories
mkdir -p "${BLIP3O_JOB_TEMP}"/{cache,results}

# Redirect model caches to temp (avoid quota issues)
export TORCH_HOME="${BLIP3O_JOB_TEMP}/cache/torch"
export HF_HOME="${BLIP3O_JOB_TEMP}/cache/huggingface"
export TRANSFORMERS_CACHE="${BLIP3O_JOB_TEMP}/cache/transformers"

# Create cache directories
mkdir -p "${TORCH_HOME}" "${HF_HOME}" "${TRANSFORMERS_CACHE}"

echo "✅ Environment configured"
echo "   Workspace: ${BLIP3O_WORKSPACE}"
echo "   Job temp: ${BLIP3O_JOB_TEMP}"

# =============================================================================
# MODEL AND DATA PATHS
# =============================================================================

echo ""
echo "📍 Setting up paths..."
echo "======================"

# Model path - UPDATE THIS to your specific model
MODEL_PATH="/scratch-shared/scur2711/blip3o_workspace/checkpoints/blip3o_multi_gpu_fixed_cosine_13170504_20250716_034251"

# COCO dataset path
COCO_ROOT="./data/coco"

# Results directory
RESULTS_DIR="${BLIP3O_JOB_TEMP}/results"
PERSISTENT_RESULTS_DIR="${BLIP3O_WORKSPACE}/evaluation_results/eval_fair_${BLIP3O_JOB_ID}_$(date +%Y%m%d_%H%M%S)"

# Create results directories
mkdir -p "$RESULTS_DIR"
mkdir -p "$PERSISTENT_RESULTS_DIR"

# Verify paths
echo "🔍 Verifying paths..."
if [ ! -d "$MODEL_PATH" ]; then
    echo "❌ Model not found: $MODEL_PATH"
    echo "Available models:"
    ls -la "${BLIP3O_CHECKPOINTS}/" | grep blip3o || echo "No models found"
    exit 1
fi

if [ ! -d "$COCO_ROOT" ]; then
    echo "❌ COCO dataset not found: $COCO_ROOT"
    echo "Please ensure COCO is downloaded in ./data/coco/"
    exit 1
fi

echo "✅ Model found: $MODEL_PATH"
echo "✅ COCO found: $COCO_ROOT"
echo "✅ Results will be saved to: $PERSISTENT_RESULTS_DIR"

# =============================================================================
# EVALUATION CONFIGURATION
# =============================================================================

echo ""
echo "⚙️  Evaluation Configuration..."
echo "=============================="

# Evaluation parameters
MAX_SAMPLES=0       # Number of samples to evaluate (set to null for all)
BATCH_SIZE=64           # Batch size for GPU evaluation
DEVICE="cuda"           # Use GPU
VERBOSE_FLAG="--verbose"

# For testing, use smaller values
if [ "${1:-}" = "test" ]; then
    echo "🧪 TEST MODE: Using small sample size"
    MAX_SAMPLES=20
    BATCH_SIZE=4
fi

echo "📊 Evaluation settings:"
echo "   Max samples: $MAX_SAMPLES"
echo "   Batch size: $BATCH_SIZE"
echo "   Device: $DEVICE"
echo "   Model: $(basename $MODEL_PATH)"
echo ""
echo "🎯 Evaluation Improvements:"
echo "   ✅ Uses CLIP's visual projection for fair comparison"
echo "   ✅ Both methods compared in CLIP's aligned 768-dim space"
echo "   ✅ Follows CLIP's standard evaluation methodology"
echo "   ✅ Literature-compliant baseline established"

# =============================================================================
# TASK 1: ALIGNMENT EVALUATION
# =============================================================================

echo ""
echo "📊 Starting Task 1: Alignment Evaluation (Fair Comparison)"
echo "=========================================================="
echo "🎯 UPDATED: Now evaluating alignment in CLIP's aligned embedding space"
echo ""
echo "Method comparison:"
echo "  (a) CLIP text + CLIP vision → visual projection → 768-dim aligned space"
echo "  (b) CLIP text + Generated CLIP (EVA→BLIP3-o) → visual projection → 768-dim aligned space"
echo ""
echo "✅ Fair comparison: Both methods use same embedding space"
echo "✅ Literature compliant: Follows CLIP's evaluation methodology"

ALIGNMENT_START_TIME=$(date +%s)

python evaluate_alignment.py \
    --blip3o_model_path "$MODEL_PATH" \
    --coco_root "$COCO_ROOT" \
    --max_samples $MAX_SAMPLES \
    --batch_size $BATCH_SIZE \
    --device $DEVICE \
    --results_dir "${RESULTS_DIR}/alignment" \
    --save_detailed \
    $VERBOSE_FLAG

ALIGNMENT_EXIT_CODE=$?
ALIGNMENT_END_TIME=$(date +%s)
ALIGNMENT_DURATION=$((ALIGNMENT_END_TIME - ALIGNMENT_START_TIME))

if [ $ALIGNMENT_EXIT_CODE -eq 0 ]; then
    echo "✅ Task 1 (Alignment) completed successfully in ${ALIGNMENT_DURATION}s"
    echo "   Results reflect fair comparison in CLIP's aligned space"
else
    echo "❌ Task 1 (Alignment) failed with exit code: $ALIGNMENT_EXIT_CODE"
fi

# =============================================================================
# TASK 2: RECALL EVALUATION  
# =============================================================================

echo ""
echo "🎯 Starting Task 2: Recall Evaluation (Fair Comparison)"
echo "======================================================="
echo "🎯 UPDATED: Now evaluating retrieval in CLIP's aligned embedding space"
echo ""
echo "Method comparison:"
echo "  (a) Image → CLIP vision → visual projection → retrieval (768-dim aligned)"
echo "  (b) Image → EVA-CLIP → BLIP3-o → visual projection → retrieval (768-dim aligned)"
echo ""
echo "✅ Fair retrieval: Both image methods compete in same space against text"
echo "✅ Meaningful results: Differences reflect actual model performance"

RECALL_START_TIME=$(date +%s)

python evaluate_recall.py \
    --blip3o_model_path "$MODEL_PATH" \
    --coco_root "$COCO_ROOT" \
    --max_samples $MAX_SAMPLES \
    --batch_size $BATCH_SIZE \
    --k_values 1 5 10 \
    --device $DEVICE \
    --results_dir "${RESULTS_DIR}/recall" \
    --save_detailed \
    $VERBOSE_FLAG

RECALL_EXIT_CODE=$?
RECALL_END_TIME=$(date +%s)
RECALL_DURATION=$((RECALL_END_TIME - RECALL_START_TIME))

if [ $RECALL_EXIT_CODE -eq 0 ]; then
    echo "✅ Task 2 (Recall) completed successfully in ${RECALL_DURATION}s"
    echo "   Results reflect fair comparison in CLIP's aligned space"
else
    echo "❌ Task 2 (Recall) failed with exit code: $RECALL_EXIT_CODE"
fi

# =============================================================================
# RESULTS ARCHIVING
# =============================================================================

echo ""
echo "📁 Archiving Results..."
echo "======================"

# Copy results to persistent storage
if [ -d "$RESULTS_DIR" ] && [ "$(ls -A "$RESULTS_DIR")" ]; then
    echo "📁 Copying results to persistent storage..."
    cp -r "$RESULTS_DIR"/* "$PERSISTENT_RESULTS_DIR/"
    
    # Create summary file
    SUMMARY_FILE="$PERSISTENT_RESULTS_DIR/evaluation_summary.txt"
    
    cat > "$SUMMARY_FILE" << EOF
BLIP3-o DiT Evaluation Summary - FAIR COMPARISON VERSION
=========================================================
Job ID: $SLURM_JOB_ID
Date: $(date)
Node: $SLURMD_NODENAME
User: $BLIP3O_USER

🎯 EVALUATION IMPROVEMENTS:
- Uses CLIP's visual projection for fair comparison
- Both methods compared in CLIP's aligned 768-dim space
- Follows CLIP's standard evaluation methodology
- Results reflect actual model performance differences

Model Information:
- Path: $MODEL_PATH
- Model: $(basename $MODEL_PATH)

Evaluation Settings:
- Max samples: $MAX_SAMPLES
- Batch size: $BATCH_SIZE
- Device: $DEVICE
- COCO root: $COCO_ROOT
- Embedding space: CLIP-aligned 768-dimensional
- Uses visual projection: YES

Task Results:
- Task 1 (Alignment): $([ $ALIGNMENT_EXIT_CODE -eq 0 ] && echo "SUCCESS" || echo "FAILED") (${ALIGNMENT_DURATION}s)
- Task 2 (Recall): $([ $RECALL_EXIT_CODE -eq 0 ] && echo "SUCCESS" || echo "FAILED") (${RECALL_DURATION}s)

Results Location: $PERSISTENT_RESULTS_DIR

Files Generated:
$(find "$PERSISTENT_RESULTS_DIR" -type f -name "*.json" | sed 's|^|  - |')
EOF

    echo "✅ Results saved to: $PERSISTENT_RESULTS_DIR"
    echo "📄 Summary saved to: $SUMMARY_FILE"
    
    # Show quick summary
    echo ""
    echo "📈 Quick Results Preview (Fair Comparison):"
    echo "==========================================="
    
    # Show alignment results if available
    if [ -f "$PERSISTENT_RESULTS_DIR/alignment/alignment_summary.json" ]; then
        echo "📊 Alignment Results (768-dim aligned space):"
        python -c "
import json
try:
    with open('$PERSISTENT_RESULTS_DIR/alignment/alignment_summary.json', 'r') as f:
        data = json.load(f)
    print(f'  CLIP Text + CLIP Vision (768-dim):     {data.get(\"clip_text_clip_vision_mean\", 0):.4f}')
    print(f'  CLIP Text + Generated CLIP (768-dim):  {data.get(\"clip_text_generated_mean\", 0):.4f}')
    print(f'  Difference (fair comparison):          {data.get(\"difference_mean\", 0):+.4f}')
    print(f'  Samples evaluated:                     {data.get(\"num_samples\", 0)}')
    print(f'  Embedding space:                       {data.get(\"embedding_space\", \"unknown\")}')
    print(f'  Uses visual projection:                {data.get(\"uses_visual_projection\", False)}')
except Exception as e:
    print(f'  Could not parse alignment results: {e}')
"
    fi
    
    # Show recall results if available
    if [ -f "$PERSISTENT_RESULTS_DIR/recall/recall_summary.json" ]; then
        echo ""
        echo "🎯 Recall Results (768-dim aligned space):"
        python -c "
import json
try:
    with open('$PERSISTENT_RESULTS_DIR/recall/recall_summary.json', 'r') as f:
        data = json.load(f)
    print('  Method (a) - Image→CLIP Vision→Visual Proj→Text:')
    for k in [1, 5, 10]:
        recall = data.get(f'clip_vision_recall@{k}', 0)
        print(f'    Recall@{k:2d}: {recall:.4f} ({recall*100:.2f}%)')
    print('  Method (b) - Image→EVA→BLIP3o→Visual Proj→Text:')
    for k in [1, 5, 10]:
        recall = data.get(f'generated_recall@{k}', 0)
        print(f'    Recall@{k:2d}: {recall:.4f} ({recall*100:.2f}%)')
    print('  Fair comparison improvements:')
    for k in [1, 5, 10]:
        diff = data.get(f'recall@{k}_difference', 0)
        print(f'    Recall@{k:2d}: {diff:+.4f} ({diff*100:+.2f}%)')
    print(f'  Embedding space: {data.get(\"embedding_space\", \"unknown\")}')
    print(f'  Uses visual projection: {data.get(\"uses_visual_projection\", False)}')
except Exception as e:
    print(f'  Could not parse recall results: {e}')
"
    fi
    
else
    echo "⚠️  No results to archive"
fi

# =============================================================================
# FINAL SUMMARY
# =============================================================================

echo ""
echo "🎉 FAIR COMPARISON EVALUATION COMPLETED!"
echo "========================================"

TOTAL_END_TIME=$(date +%s)
TOTAL_DURATION=$SECONDS

echo "📊 Final Summary:"
echo "   Task 1 (Alignment): $([ $ALIGNMENT_EXIT_CODE -eq 0 ] && echo "✅ SUCCESS" || echo "❌ FAILED")"
echo "   Task 2 (Recall): $([ $RECALL_EXIT_CODE -eq 0 ] && echo "✅ SUCCESS" || echo "❌ FAILED")"
echo "   Total samples evaluated: $MAX_SAMPLES"
echo "   Results location: $PERSISTENT_RESULTS_DIR"
echo ""
echo "🎯 Evaluation Improvements Applied:"
echo "   ✅ CLIP visual projection used for both methods"
echo "   ✅ Fair comparison in CLIP's aligned 768-dim space"
echo "   ✅ Literature-compliant evaluation methodology"
echo "   ✅ Meaningful performance differences captured"

if [ $ALIGNMENT_EXIT_CODE -eq 0 ] && [ $RECALL_EXIT_CODE -eq 0 ]; then
    echo ""
    echo "🚀 Both evaluations completed successfully with fair comparison!"
    echo "📊 Check the results in: $PERSISTENT_RESULTS_DIR"
    echo "📈 Summary files:"
    echo "   - alignment_summary.json (Task 1 results with fair comparison)"
    echo "   - recall_summary.json (Task 2 results with fair comparison)"
    echo "   - evaluation_summary.txt (Overall summary with improvements)"
    echo ""
    echo "🔬 Technical Details:"
    echo "   • Both image embeddings projected to CLIP's aligned 768-dim space"
    echo "   • Text embeddings already in CLIP's aligned 768-dim space"
    echo "   • Results now reflect actual model performance differences"
    echo "   • Baseline (CLIP vision) properly established with projection"
    
    OVERALL_EXIT_CODE=0
else
    echo ""
    echo "⚠️  Some evaluations failed. Check the logs for details."
    echo "📁 Logs location: ./slurm_out/evaluation_${SLURM_JOB_ID}.{out,err}"
    
    OVERALL_EXIT_CODE=1
fi

echo ""
echo "🎉 Job completed at: $(date)"
echo "⏱️  Total runtime: $TOTAL_DURATION seconds"
echo "🎯 Fair comparison evaluation ensures meaningful results!"

exit $OVERALL_EXIT_CODE