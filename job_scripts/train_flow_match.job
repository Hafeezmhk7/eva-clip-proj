#!/bin/bash
#SBATCH --partition=gpu_h100
#SBATCH --gpus=1
#SBATCH --job-name=lumina_train
#SBATCH --time=24:00:00          # Increased for training
#SBATCH --output=./slurm_out/train_%j.out
#SBATCH --error=./slurm_out/train_%j.err
#SBATCH --mem=64GB               # Increased memory
#SBATCH --cpus-per-task=8
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1

echo "ðŸš€ Starting Lumina-DiT Training Job"
echo "=============================================="
echo "Job ID: $SLURM_JOB_ID"
echo "Node: $SLURMD_NODENAME"
echo "Date: $(date)"
echo "Working directory: $(pwd)"
echo "CUDA Visible Devices: $CUDA_VISIBLE_DEVICES"

# Create output directories
mkdir -p slurm_out
mkdir -p checkpoints

# Load modules
module purge
module load 2024
module load Miniconda3/24.7.1-0
module load CUDA/12.6.0

# Activate conda environment
source activate eva_clip_env


python train_blip3o_dit.py \
  --embeddings_path path/to/blip3o_grid_embeddings.pkl \
  --output_dir ./checkpoints/blip3o-dit \
  --num_epochs 10 \
  --batch_size 32 \
  --learning_rate 1e-4 \
  --gradient_checkpointing \
  --fp16