#!/bin/bash
#SBATCH --job-name=blip3o_eval
#SBATCH --partition=gpu_h100
#SBATCH --nodes=1
#SBATCH --gpus=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=16
#SBATCH --time=2:00:00
#SBATCH --mem=32G
#SBATCH --output=./slurm_out/blip3o_eval_%j.out
#SBATCH --error=./slurm_out/blip3o_eval_%j.err

# =============================================================================
# BLIP3-o Evaluation - Comprehensive Model Assessment
# Evaluates trained model to verify training/inference alignment
# =============================================================================

echo "üîç BLIP3-o Comprehensive Evaluation"
echo "==================================="
echo "Job ID: ${SLURM_JOB_ID}"
echo "Node: $(hostname)"
echo "Time: $(date)"
echo "GPU: $(nvidia-smi --query-gpu=name --format=csv,noheader,nounits | head -1)"
echo "==================================="

cd $SLURM_SUBMIT_DIR

# Setup environment
module purge
module load 2024
module load Miniconda3/24.7.1-0
module load CUDA/12.6.0
source activate eva_clip_env

# Configuration
EMBEDDINGS_DIR="/scratch-shared/azadaianchuk1/blip3o_workspace/embeddings/patch_only_256_tokens"
OUTPUT_DIR="./eval_results_$(date +%Y%m%d_%H%M%S)"

# Model path - can be provided as argument or auto-detected
if [ -n "$1" ]; then
    MODEL_PATH="$1"
    echo "üìÇ Using provided model path: $MODEL_PATH"
else
    echo "üîç Auto-detecting most recent model..."
    MODEL_PATH=$(find ./checkpoints -name "*blip3o*" -type d | sort | tail -1)
    if [ -z "$MODEL_PATH" ]; then
        echo "‚ùå No model found! Please provide model path as argument."
        echo "Usage: sbatch job_scripts/evaluate_blip3o.job <model_path>"
        exit 1
    fi
    echo "üìÇ Found model: $MODEL_PATH"
fi

# Create output directory
mkdir -p "${OUTPUT_DIR}"
mkdir -p ./slurm_out

echo ""
echo "‚öôÔ∏è Evaluation Configuration:"
echo "============================"
echo "Model: $MODEL_PATH"
echo "Embeddings: $EMBEDDINGS_DIR"
echo "Output: $OUTPUT_DIR"
echo ""

# Verify model exists
if [ ! -d "$MODEL_PATH" ]; then
    echo "‚ùå Model directory not found: $MODEL_PATH"
    echo "Available models:"
    ls -la ./checkpoints/ 2>/dev/null || echo "No checkpoints found"
    exit 1
fi

# Check for required model files
MODEL_CONFIG="$MODEL_PATH/config.json"
if [ ! -f "$MODEL_CONFIG" ]; then
    echo "‚ùå Model config not found: $MODEL_CONFIG"
    echo "Model directory contents:"
    ls -la "$MODEL_PATH"
    exit 1
fi

# Check for model weights
MODEL_WEIGHTS_SAFETENSORS="$MODEL_PATH/model.safetensors"
MODEL_WEIGHTS_BIN="$MODEL_PATH/pytorch_model.bin"

if [ ! -f "$MODEL_WEIGHTS_SAFETENSORS" ] && [ ! -f "$MODEL_WEIGHTS_BIN" ]; then
    echo "‚ùå No model weights found in: $MODEL_PATH"
    echo "Expected: model.safetensors or pytorch_model.bin"
    echo "Model directory contents:"
    ls -la "$MODEL_PATH"
    exit 1
fi

echo "‚úÖ Model verified: $MODEL_PATH"

# Verify embeddings
if [ ! -d "$EMBEDDINGS_DIR" ]; then
    echo "‚ùå Embeddings not found: $EMBEDDINGS_DIR"
    echo "Available embeddings:"
    ls -la "/scratch-shared/azadaianchuk1/blip3o_workspace/embeddings/" 2>/dev/null
    exit 1
fi

echo "‚úÖ Embeddings verified: $EMBEDDINGS_DIR"

# Verify evaluation script
if [ ! -f "eval_blip3o_patch_similarity.py" ]; then
    echo "‚ùå Evaluation script not found!"
    exit 1
fi

echo "‚úÖ Evaluation script found"

# Check if we have training info to compare against
TRAINING_INFO="$MODEL_PATH/training_info.json"
if [ -f "$TRAINING_INFO" ]; then
    echo "‚úÖ Training info found - will compare metrics"
    COMPARE_FLAG="--compare_with_training"
else
    echo "‚ö†Ô∏è No training info found - standalone evaluation"
    COMPARE_FLAG=""
fi

echo ""
echo "üöÄ Starting Comprehensive Evaluation..."
echo "======================================="
echo "Goal: Verify that inference metrics match training metrics"
echo "Expected: Overall cosine similarity should match final training values"
echo ""

# Launch evaluation
python eval_blip3o_patch_similarity.py \
    --model_path "$MODEL_PATH" \
    --chunked_embeddings_dir "$EMBEDDINGS_DIR" \
    --output_dir "$OUTPUT_DIR" \
    --training_mode auto \
    --num_samples 5000 \
    --batch_size 16 \
    --num_inference_steps 50 \
    --same_data_eval \
    --normalize_embeddings \
    --device auto \
    --torch_dtype float32 \
    $COMPARE_FLAG

EVAL_EXIT_CODE=$?

echo ""
echo "========================================"
echo "üìä Evaluation Results"
echo "========================================"

if [ $EVAL_EXIT_CODE -eq 0 ]; then
    echo "‚úÖ Evaluation completed successfully!"
    
    # Find and display results
    RESULTS_FILE=$(find "$OUTPUT_DIR" -name "*results*.json" | head -1)
    
    if [ -f "$RESULTS_FILE" ]; then
        echo ""
        echo "üìã Results Summary:"
        echo "=================="
        
        # Extract key metrics
        python -c "
import json
try:
    with open('$RESULTS_FILE', 'r') as f:
        data = json.load(f)
    
    if 'results_summary' in data:
        r = data['results_summary']
        print(f'üéØ Overall Embedding Similarity: {r.get(\"overall_embedding_similarity\", 0):.4f}')
        print(f'üìä High Quality Images (>0.7): {r.get(\"high_quality_images_percentage\", 0):.1f}%')
        print(f'üìä Very High Quality Images (>0.8): {r.get(\"very_high_quality_images_percentage\", 0):.1f}%')
        print(f'üìä Excellent Quality Images (>0.9): {r.get(\"excellent_quality_images_percentage\", 0):.1f}%')
        print(f'üìà Images Evaluated: {r.get(\"total_images\", 0):,}')
        print(f'‚è±Ô∏è Avg Generation Time: {r.get(\"avg_generation_time_per_batch\", 0):.2f}s per batch')
        
        # Assessment
        sim = r.get('overall_embedding_similarity', 0)
        if sim > 0.3:
            print('üéâ EXCELLENT: Very high quality embeddings!')
        elif sim > 0.2:
            print('‚úÖ VERY GOOD: High quality embeddings!')
        elif sim > 0.1:
            print('üîÑ GOOD: Solid embedding quality!')
        elif sim > 0.05:
            print('üìà LEARNING: Shows clear improvement!')
        else:
            print('‚ö†Ô∏è NEEDS WORK: Low similarity')
    
    # Check training comparison if available
    if 'training_comparison' in data and data['training_comparison']:
        print('')
        print('üîç Training vs Evaluation Comparison:')
        print('====================================')
        training_info = data['training_comparison']
        
        if 'final_results' in training_info:
            final_results = training_info['final_results']
            
            if 'training_summary' in final_results:
                training_summary = final_results['training_summary']
                training_emb_sim = training_summary.get('best_embedding_sim', 0)
                eval_emb_sim = r.get('overall_embedding_similarity', 0)
                
                print(f'üìä Training Best Embedding Sim: {training_emb_sim:.4f}')
                print(f'üìä Evaluation Embedding Sim:    {eval_emb_sim:.4f}')
                print(f'üìä Difference:                   {abs(eval_emb_sim - training_emb_sim):.4f}')
                
                if abs(eval_emb_sim - training_emb_sim) < 0.02:
                    print('üéâ PERFECT: Training and evaluation metrics match!')
                elif abs(eval_emb_sim - training_emb_sim) < 0.05:
                    print('‚úÖ GOOD: Training and evaluation metrics are close')
                else:
                    print('‚ö†Ô∏è CONCERN: Training and evaluation metrics differ significantly')
        
    else:
        print('')
        print('‚ÑπÔ∏è No training comparison available')
            
except Exception as e:
    print(f'Could not parse results: {e}')
"
        
        echo ""
        echo "üìÅ Results saved to: $OUTPUT_DIR"
        echo "üìÑ Full results: $RESULTS_FILE"
    else
        echo "‚ö†Ô∏è No results file found"
    fi
    
    echo ""
    echo "üéØ Interpretation:"
    echo "=================="
    echo "‚Ä¢ If metrics match training: Implementation is correct ‚úÖ"
    echo "‚Ä¢ If metrics differ significantly: Check implementation ‚ö†Ô∏è"
    echo "‚Ä¢ Overall similarity >0.1: Model is learning properly üìà"
    echo "‚Ä¢ Overall similarity >0.3: Excellent performance üéâ"
    
    echo ""
    echo "‚úÖ SUCCESS: Evaluation completed!"
    
else
    echo "‚ùå FAILED: Evaluation exit code $EVAL_EXIT_CODE"
    echo ""
    echo "üí° Troubleshooting:"
    echo "  ‚Ä¢ Check log files in ./slurm_out/"
    echo "  ‚Ä¢ Verify model and embeddings paths"
    echo "  ‚Ä¢ Try smaller batch size: --batch_size 8"
    echo "  ‚Ä¢ Try fewer samples: --num_samples 1000"
fi

echo ""
echo "üèÅ Job completed at $(date)"
echo "========================================"

exit $EVAL_EXIT_CODE