#!/bin/bash
#SBATCH --job-name=blip3o_multi_gpu
#SBATCH --partition=gpu_h100
#SBATCH --nodes=1
#SBATCH --gpus=3
#SBATCH --cpus-per-gpu=18
#SBATCH --time=12:00:00
#SBATCH --mem=0
#SBATCH --output=./slurm_out/multi_gpu_%j.out
#SBATCH --error=./slurm_out/multi_gpu_%j.err

echo "🚀 BLIP3-o DiT Multi-GPU Training (3x H100 GPUs)"
echo "================================================================"
echo "Job ID: $SLURM_JOB_ID"
echo "Node: $SLURMD_NODENAME"
echo "Date: $(date)"
echo "Working directory: $(pwd)"
echo "User: $(whoami)"
echo "GPUs requested: $SLURM_GPUS"
echo "CPUs per GPU: $SLURM_CPUS_PER_GPU"
echo "================================================================"

# =============================================================================
# ENVIRONMENT SETUP
# =============================================================================

echo ""
echo "🔧 Environment Setup..."
echo "======================="

# Load modules
module purge
module load 2024
module load Miniconda3/24.7.1-0
module load CUDA/12.6.0

# Activate conda environment
source activate eva_clip_env

# Environment check
echo "=== Environment Check ==="
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA: {torch.version.cuda}'); print(f'Devices: {torch.cuda.device_count()}')"
echo "========================="

# Set up distributed training environment variables
export MASTER_PORT=12340
master_addr=$(scontrol show hostnames "$SLURM_JOB_NODELIST" | head -n 1)
export MASTER_ADDR=$master_addr

# Get actual number of GPUs
NUM_GPUS=$(python -c "import torch; print(torch.cuda.device_count())")
export WORLD_SIZE=$NUM_GPUS

echo "🌐 Distributed training setup:"
echo "   MASTER_ADDR: $MASTER_ADDR"
echo "   MASTER_PORT: $MASTER_PORT"
echo "   WORLD_SIZE: $WORLD_SIZE"
echo "   ACTUAL GPUS: $NUM_GPUS"

# Set up Snellius scratch directories
export SCRATCH_SHARED="/scratch-shared"
export SCRATCH_LOCAL="/scratch-local"
export BLIP3O_USER=$(whoami)
export BLIP3O_JOB_ID=${SLURM_JOB_ID}

# Persistent workspace
export BLIP3O_WORKSPACE="/scratch-shared/${BLIP3O_USER}/blip3o_workspace"
export BLIP3O_EMBEDDINGS="${BLIP3O_WORKSPACE}/embeddings"
export BLIP3O_CHECKPOINTS="${BLIP3O_WORKSPACE}/checkpoints"
export BLIP3O_LOGS="${BLIP3O_WORKSPACE}/logs"

# Job-specific temp
export BLIP3O_JOB_TEMP="/scratch-local/${BLIP3O_USER}.${BLIP3O_JOB_ID}/blip3o_job_${BLIP3O_JOB_ID}"
export BLIP3O_CACHE="${BLIP3O_JOB_TEMP}/cache"
export BLIP3O_TEMP_CHECKPOINTS="${BLIP3O_JOB_TEMP}/temp_checkpoints"

# Create directories
mkdir -p "${BLIP3O_WORKSPACE}"/{embeddings,checkpoints,logs,metadata}
mkdir -p "${BLIP3O_JOB_TEMP}"/{cache,temp_checkpoints}

# Redirect model caches
export TORCH_HOME="${BLIP3O_CACHE}/torch"
export HF_HOME="${BLIP3O_CACHE}/huggingface"
export TRANSFORMERS_CACHE="${BLIP3O_CACHE}/transformers"
export WANDB_DIR="${BLIP3O_LOGS}/wandb"

# Create cache directories
mkdir -p "${TORCH_HOME}" "${HF_HOME}" "${TRANSFORMERS_CACHE}" "${WANDB_DIR}"

echo "✅ Environment configured"
echo "   Workspace: ${BLIP3O_WORKSPACE}"
echo "   Job temp: ${BLIP3O_JOB_TEMP}"

# Check GPU info
echo ""
echo "🎮 GPU Information:"
nvidia-smi -L
echo ""
nvidia-smi --query-gpu=memory.total,memory.free --format=csv,noheader,nounits

# =============================================================================
# FIND EMBEDDINGS
# =============================================================================

echo ""
echo "🔍 Finding embeddings..."
echo "========================"

# Find embeddings
EMBEDDINGS_DIR=$(find "${BLIP3O_EMBEDDINGS}" -name "*chunked*256*" -type d | head -1)

if [ -z "$EMBEDDINGS_DIR" ]; then
    echo "❌ No 256-token embeddings found in ${BLIP3O_EMBEDDINGS}"
    echo "Available directories:"
    ls -la "${BLIP3O_EMBEDDINGS}" 2>/dev/null || echo "   (none)"
    exit 1
fi

echo "✅ Found embeddings: $EMBEDDINGS_DIR"

# Validate manifest
MANIFEST_FILE="$EMBEDDINGS_DIR/embeddings_manifest.json"
if [ ! -f "$MANIFEST_FILE" ]; then
    echo "❌ Manifest file not found: $MANIFEST_FILE"
    exit 1
fi

echo "✅ Embeddings validated"

# =============================================================================
# TRAINING SETUP
# =============================================================================

echo ""
echo "📁 Training Setup..."
echo "=================="

# Create training directories
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
TRAINING_NAME="blip3o_multi_gpu_${SLURM_JOB_ID}_${TIMESTAMP}"

TEMP_CHECKPOINT_DIR="${BLIP3O_TEMP_CHECKPOINTS}/${TRAINING_NAME}"
PERSISTENT_CHECKPOINT_DIR="${BLIP3O_CHECKPOINTS}/${TRAINING_NAME}"
FINAL_MODEL_DIR="${HOME}/models/blip3o_multi_gpu_${TIMESTAMP}"

mkdir -p "$TEMP_CHECKPOINT_DIR"
mkdir -p "$PERSISTENT_CHECKPOINT_DIR"
mkdir -p "$FINAL_MODEL_DIR"

echo "✅ Training directories created:"
echo "   Temp: $TEMP_CHECKPOINT_DIR"
echo "   Persistent: $PERSISTENT_CHECKPOINT_DIR"
echo "   Final: $FINAL_MODEL_DIR"

# =============================================================================
# MULTI-GPU TRAINING WITH TORCHRUN - FIXED SYNTAX
# =============================================================================

echo ""
echo "🚀 Starting Multi-GPU Training with torchrun..."
echo "==============================================="

echo "🔧 Multi-GPU Configuration:"
echo "   🎯 GPUs: ${NUM_GPUS}x H100"
echo "   📐 Model: Large configuration (768 dim, 16 layers)"
echo "   📦 Batch size: 8 per GPU"
echo "   🔄 Gradient accumulation: 4"
echo "   💾 Memory optimizations: Gradient checkpointing enabled"
echo ""

# Fixed command syntax - removed line breaks between arguments
torchrun --nproc_per_node=$NUM_GPUS --nnodes=1 --node_rank=0 --master_addr=$MASTER_ADDR --master_port=$MASTER_PORT \
    train_blip3o_dit_multi_gpu.py \
    --chunked_embeddings_dir "$EMBEDDINGS_DIR" \
    --output_dir "$TEMP_CHECKPOINT_DIR" \
    --batch_size 8 \
    --eval_batch_size 4 \
    --num_epochs 5 \
    --model_dim 768 \
    --num_heads 12 \
    --num_layers 16 \
    --learning_rate 1e-4 \
    --weight_decay 0.01 \
    --warmup_steps 100 \
    --gradient_accumulation_steps 4 \
    --fp16 \
    --dataloader_num_workers 4

TRAINING_EXIT_CODE=$?

echo ""
echo "🎯 MULTI-GPU TRAINING COMPLETED WITH EXIT CODE: $TRAINING_EXIT_CODE"

# =============================================================================
# MODEL ARCHIVING AND CLEANUP
# =============================================================================

if [ $TRAINING_EXIT_CODE -eq 0 ]; then
    echo ""
    echo "✅ Training completed successfully! Archiving model..."
    echo "==================================================="
    
    # Copy to persistent storage
    if [ -d "$TEMP_CHECKPOINT_DIR" ] && [ "$(ls -A "$TEMP_CHECKPOINT_DIR")" ]; then
        echo "📁 Copying to persistent storage..."
        cp -r "$TEMP_CHECKPOINT_DIR"/* "$PERSISTENT_CHECKPOINT_DIR/"
        echo "✅ Saved to: $PERSISTENT_CHECKPOINT_DIR"
    fi
    
    # Copy to home directory
    if [ -d "$PERSISTENT_CHECKPOINT_DIR" ] && [ "$(ls -A "$PERSISTENT_CHECKPOINT_DIR")" ]; then
        echo "🏠 Archiving to home directory..."
        cp -r "$PERSISTENT_CHECKPOINT_DIR"/* "$FINAL_MODEL_DIR/"
        echo "✅ Final model: $FINAL_MODEL_DIR"
        
        # Create usage instructions
        cat > "$FINAL_MODEL_DIR/README.md" << EOF
# BLIP3-o DiT Model (256 Tokens) - Multi-GPU Training

## Training Information
- **Date**: $(date -Iseconds)
- **Job ID**: ${SLURM_JOB_ID}
- **GPUs Used**: ${NUM_GPUS}x H100
- **Tokens**: 256 (16x16 grid)
- **Training Method**: PyTorch DDP with torchrun

## Model Architecture
- **Hidden Dimension**: 768
- **Layers**: 16
- **Attention Heads**: 12
- **Input Resolution**: 16x16 = 256 tokens
- **CLIP Input**: 1024-dim (ViT-L/14)
- **EVA-CLIP Conditioning**: 4096-dim (EVA-CLIP-8B)

## Loading
\`\`\`python
from src.modules.models.blip3o_dit import create_blip3o_dit_model
from src.modules.config.blip3o_config import BLIP3oDiTConfig

# Load the model configuration and weights
config = BLIP3oDiTConfig(
    input_size=16, dim=768, n_layers=16, n_heads=12
)
model = create_blip3o_dit_model(config)
model.load_state_dict(torch.load('pytorch_model.bin'))
\`\`\`
EOF
    fi
    
    echo ""
    echo "🎉 SUCCESS! Multi-GPU BLIP3-o training completed!"
    echo "Model saved to: $FINAL_MODEL_DIR"
    
else
    echo ""
    echo "❌ Multi-GPU training failed"
    echo "Check logs: ./slurm_out/multi_gpu_${SLURM_JOB_ID}.{out,err}"
    exit 1
fi

echo ""
echo "🎉 Job completed at: $(date)"
echo "⏱️  Total runtime: $SECONDS seconds"
echo ""
echo "📋 MULTI-GPU TRAINING SUMMARY:"
echo "==============================="
echo "✅ Used ${NUM_GPUS}x H100 GPUs"
echo "✅ DDP with NCCL backend"
echo "✅ Memory issues resolved!"