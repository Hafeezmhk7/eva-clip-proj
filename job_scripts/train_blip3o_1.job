#!/bin/bash
#SBATCH --job-name=universal_denoising_wandb
#SBATCH --partition=gpu_h100
#SBATCH --nodes=1
#SBATCH --gpus=3
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=18
#SBATCH --time=15:00:00
#SBATCH --mem=64G
#SBATCH --output=./slurm_out/eva_denoising_1shard_%j.out
#SBATCH --error=./slurm_out/eva_denoising_1shard_%j.err

# =============================================================================
# Universal BLIP3-o Denoising Training with WandB Integration
# Supports both EVA denoising and CLIP denoising with comprehensive monitoring
# =============================================================================

echo "üöÄ Universal BLIP3-o Denoising Training with WandB"
echo "=========================================================="
echo "Job ID: ${SLURM_JOB_ID}"
echo "Node: $(hostname)"
echo "Time: $(date)"
echo "GPUs: $(nvidia-smi --query-gpu=name --format=csv,noheader,nounits | tr '\n' ', ')"
echo "WandB Integration: ENABLED"
echo "=========================================================="

cd $SLURM_SUBMIT_DIR

# Setup environment
module purge
module load 2024
module load Miniconda3/24.7.1-0
module load CUDA/12.6.0
source activate eva_clip_env

# =============================================================================
# WANDB SETUP AND LOGIN
# =============================================================================
echo "üìä Setting up WandB monitoring..."

# Install WandB if not already installed
# pip install wandb --quiet

# WandB login with provided API key
echo "üîë Logging into WandB..."
wandb login 0d9895af249ee18e4fa141e8a2350e0f4adb920f --relogin

# Verify WandB login
if [ $? -eq 0 ]; then
    echo "‚úÖ WandB login successful"
    
    # Set WandB environment variables for better integration
    export WANDB_PROJECT="eva-clip"
    export WANDB_ENTITY="$(whoami)"
    export WANDB_RUN_GROUP="eva_pred_eva_guid_1shard-${SLURM_JOB_ID}"
    export WANDB_JOB_TYPE="training"
    export WANDB_TAGS="slurm,gpu_h100,universal_denoising"
    export WANDB_NOTES="Universal BLIP3-o denoising training with EVA and CLIP support"
    
    # WandB cache directory (avoid home directory quota issues)
    export WANDB_CACHE_DIR="${TMPDIR}/wandb_cache"
    export WANDB_DATA_DIR="${TMPDIR}/wandb_data"
    mkdir -p "${WANDB_CACHE_DIR}" "${WANDB_DATA_DIR}"
    
    echo "üéØ WandB Project: ${WANDB_PROJECT}"
    echo "üë§ WandB Entity: ${WANDB_ENTITY}"
    echo "üè∑Ô∏è WandB Tags: ${WANDB_TAGS}"
    echo "üíæ WandB Cache: ${WANDB_CACHE_DIR}"
else
    echo "‚ùå WandB login failed - continuing without WandB"
    export WANDB_MODE="disabled"
fi

# =============================================================================
# TASK CONFIGURATION - Choose your task mode
# =============================================================================

# TASK_MODE options:
# - "eva_denoising": Input/Output EVA [4096], Conditioning EVA [4096] (original task)
# - "clip_denoising": Input/Output CLIP [1024], Conditioning EVA [4096] (NEW!)

TASK_MODE="clip_denoising"          # NEW: CLIP denoising with EVA conditioning
# TASK_MODE="eva_denoising"         # Original: EVA denoising

# =============================================================================
# CONFIGURATION
# =============================================================================

EMBEDDINGS_DIR="/scratch-shared/azadaianchuk1/blip3o_workspace/embeddings/patch_only_256_tokens"
OUTPUT_DIR="./checkpoints/universal_${TASK_MODE}_wandb_$(date +%Y%m%d_%H%M%S)"
TRAINING_MODE="patch_only"
MODEL_SIZE="base"

# Optimized hyperparameters for universal training
NUM_EPOCHS=10                    # Conservative epochs for initial testing
BATCH_SIZE=8                     # Optimized batch size for stability
LEARNING_RATE=1e-4              # Conservative learning rate for universal training
WEIGHT_DECAY=0.01               # Standard regularization
WARMUP_STEPS=100                # Warmup for stable training
MAX_GRAD_NORM=1.0               # Critical for spherical flow stability

# Evaluation parameters
EVAL_EVERY_N_STEPS=100
EVAL_NUM_SAMPLES=500
EVAL_INFERENCE_STEPS=50

# Testing configuration
OVERFIT_TEST_SIZE=20            # Enable overfitting test to verify architecture
DEBUG_MODE="--debug_mode"      # Enable debug logging
MAX_SHARDS=1                   # Start with fewer shards for testing

# Spherical flow matching parameters
SPHERE_CONSTRAINT_WEIGHT=0.1    # Ensures unit sphere constraints
NOISE_SCHEDULE="uniform"        # Noise sampling schedule
MAX_NOISE_LEVEL=0.9            # Maximum corruption level
MIN_NOISE_LEVEL=0.1            # Minimum corruption level

# WandB specific configuration
WANDB_RUN_NAME="${TASK_MODE}_${MODEL_SIZE}_${SLURM_JOB_ID}"
WANDB_TAGS_EXTRA="${TASK_MODE},${MODEL_SIZE},slurm_${SLURM_JOB_ID}"

# Create output directory
mkdir -p "${OUTPUT_DIR}"
mkdir -p ./slurm_out

echo ""
echo "‚öôÔ∏è Universal Denoising Configuration with WandB:"
echo "========================================"

if [ "$TASK_MODE" = "eva_denoising" ]; then
    echo "üéØ TASK: EVA-CLIP Denoising"
    echo "  üì• Input: Noisy EVA embeddings [B, N, 4096]"
    echo "  üéÆ Conditioning: Clean EVA embeddings [B, N, 4096]"
    echo "  üì§ Output: Clean EVA embeddings [B, N, 4096]"
    echo "  üåä Method: Spherical Flow Matching on 4096D hypersphere"
    echo "  üéØ Target: Cosine similarity >0.7 (excellent), >0.5 (good)"
elif [ "$TASK_MODE" = "clip_denoising" ]; then
    echo "üéØ TASK: CLIP-ViT Denoising with EVA Conditioning"
    echo "  üì• Input: Noisy CLIP embeddings [B, N, 1024]"
    echo "  üéÆ Conditioning: Clean EVA embeddings [B, N, 4096]"
    echo "  üì§ Output: Clean CLIP embeddings [B, N, 1024]"
    echo "  üåä Method: Spherical Flow Matching on 1024D hypersphere"
    echo "  üß† Architecture: Cross-attention between 1024D and 4096D spaces"
    echo "  üéØ Target: Cosine similarity >0.6 (excellent), >0.4 (good)"
else
    echo "‚ùå Unknown task mode: $TASK_MODE"
    exit 1
fi

echo ""
echo "üèóÔ∏è Model: Universal BLIP3-o DiT with task-adaptive dimensions"
echo "üìä WandB: Real-time metrics and monitoring enabled"
echo "Embeddings: $EMBEDDINGS_DIR"
echo "Output: $OUTPUT_DIR"
echo "Training mode: $TRAINING_MODE"
echo "Model size: $MODEL_SIZE"
echo "Max shards: $MAX_SHARDS"
echo ""
echo "üìä Training Hyperparameters:"
echo "  Task mode: $TASK_MODE"
echo "  Epochs: $NUM_EPOCHS"
echo "  Batch size: $BATCH_SIZE"
echo "  Learning rate: $LEARNING_RATE"
echo "  Weight decay: $WEIGHT_DECAY"
echo "  Warmup steps: $WARMUP_STEPS"
echo "  Max grad norm: $MAX_GRAD_NORM"
echo "  Overfitting test: $OVERFIT_TEST_SIZE samples"
echo ""
echo "üåä Spherical Flow Parameters:"
echo "  Sphere constraint weight: $SPHERE_CONSTRAINT_WEIGHT"
echo "  Noise schedule: $NOISE_SCHEDULE"
echo "  Noise range: [$MIN_NOISE_LEVEL, $MAX_NOISE_LEVEL]"
echo ""
echo "üîç Evaluation Configuration:"
echo "  Eval every: $EVAL_EVERY_N_STEPS steps"
echo "  Eval samples: $EVAL_NUM_SAMPLES"
echo "  Inference steps: $EVAL_INFERENCE_STEPS"
echo ""
echo "üìä WandB Configuration:"
echo "  Project: $WANDB_PROJECT"
echo "  Run name: $WANDB_RUN_NAME"
echo "  Tags: $WANDB_TAGS_EXTRA"
echo "  Group: $WANDB_RUN_GROUP"
echo ""

# Verify embeddings exist
if [ ! -d "$EMBEDDINGS_DIR" ]; then
    echo "‚ùå Embeddings directory not found: $EMBEDDINGS_DIR"
    echo "Available embeddings:"
    ls -la "/scratch-shared/azadaianchuk1/blip3o_workspace/embeddings/" 2>/dev/null || echo "No embeddings found"
    exit 1
fi

echo "‚úÖ Embeddings verified: $EMBEDDINGS_DIR"

# Check available shards
SHARD_COUNT=$(find "$EMBEDDINGS_DIR" -name "*.pkl" | wc -l)
echo "‚úÖ Found $SHARD_COUNT embedding shards"

if [ $SHARD_COUNT -eq 0 ]; then
    echo "‚ùå No embedding shards found!"
    exit 1
fi

if [ $SHARD_COUNT -lt $MAX_SHARDS ]; then
    echo "‚ö†Ô∏è Only $SHARD_COUNT shards available (requested $MAX_SHARDS)"
    MAX_SHARDS=$SHARD_COUNT
    echo "   Adjusted to use $MAX_SHARDS shards"
fi

# Verify universal training script exists
if [ ! -f "train_eva_repro.py" ]; then
    echo "‚ùå Universal training script not found: train_eva_repro.py"
    echo "Available Python files:"
    ls -la *.py 2>/dev/null || echo "No Python files found"
    exit 1
fi

echo "‚úÖ Universal training script found"

echo ""
echo "üöÄ Starting Universal Denoising Training with WandB Monitoring..."
echo "=============================================="

echo "  üìä Real-time metrics visible in WandB dashboard"
echo ""
echo "üèóÔ∏è UNIVERSAL ARCHITECTURE FEATURES:"
echo "  ‚Ä¢ Task-adaptive input/output dimensions"
echo "  ‚Ä¢ Flexible cross-attention conditioning"
echo "  ‚Ä¢ Universal BLIP3-o DiT with 3D RoPE and Grouped-Query Attention"
echo "  ‚Ä¢ Sandwich Normalization (RMSNorm)"
echo "  ‚Ä¢ Spherical Flow Matching with SLERP interpolation"
echo "  ‚Ä¢ Proper gradient flow and initialization"
echo "  ‚Ä¢ Task-specific evaluation metrics"
echo "  ‚Ä¢ Comprehensive WandB monitoring and logging"
echo ""

# Set up comprehensive WandB environment
export WANDB_CONFIG_PATHS="${OUTPUT_DIR}/experiment_config.json"
export WANDB_RUN_ID="${SLURM_JOB_ID}_${TASK_MODE}_$(date +%s)"

# Launch universal denoising training with WandB
python train_eva_repro.py \
    --task_mode "$TASK_MODE" \
    --chunked_embeddings_dir "$EMBEDDINGS_DIR" \
    --output_dir "$OUTPUT_DIR" \
    --model_size "$MODEL_SIZE" \
    --training_mode "$TRAINING_MODE" \
    --learning_rate $LEARNING_RATE \
    --batch_size $BATCH_SIZE \
    --num_epochs $NUM_EPOCHS \
    --warmup_steps $WARMUP_STEPS \
    --weight_decay $WEIGHT_DECAY \
    --max_grad_norm $MAX_GRAD_NORM \
    --sphere_constraint_weight $SPHERE_CONSTRAINT_WEIGHT \
    --noise_schedule $NOISE_SCHEDULE \
    --max_noise_level $MAX_NOISE_LEVEL \
    --min_noise_level $MIN_NOISE_LEVEL \
    --eval_every_n_steps $EVAL_EVERY_N_STEPS \
    --eval_num_samples $EVAL_NUM_SAMPLES \
    --eval_inference_steps $EVAL_INFERENCE_STEPS \
    --overfit_test_size $OVERFIT_TEST_SIZE \
    --max_shards $MAX_SHARDS \
    --fp16 \
    --use_wandb \
    --wandb_project "$WANDB_PROJECT" \
    --wandb_run_name "$WANDB_RUN_NAME" \
    --wandb_tags $WANDB_TAGS_EXTRA \
    --wandb_notes "Universal BLIP3-o ${TASK_MODE} training on Snellius H100 cluster" \
    $DEBUG_MODE

TRAINING_EXIT_CODE=$?

echo ""
echo "========================================"
echo "üìä Universal Denoising Results with WandB"
echo "========================================"

if [ $TRAINING_EXIT_CODE -eq 0 ]; then
    echo "‚úÖ Universal denoising training completed successfully!"
    
    echo ""
    echo "üìã Training Summary:"
    echo "==================="
    
    # Check for training results
    SUMMARY_FILE="$OUTPUT_DIR/training_summary.json"
    CONFIG_FILE="$OUTPUT_DIR/experiment_config.json"
    FINAL_SUMMARY_FILE="$OUTPUT_DIR/final_summary.json"
    
    if [ -f "$FINAL_SUMMARY_FILE" ]; then
        echo ""
        echo "üìä Training Results:"
        echo "==================="
        
        # Extract key metrics using Python
        python -c "
import json
import sys
try:
    with open('$FINAL_SUMMARY_FILE', 'r') as f:
        data = json.load(f)
    
    print(f'üéØ Task Mode: {data.get(\"task_mode\", \"unknown\")}')
    print(f'üéØ Best Loss: {data.get(\"best_loss\", float(\"inf\")):.6f}')
    print(f'üéØ Best Similarity: {data.get(\"best_eval_similarity\", 0):.4f}')
    print(f'üìä Total Steps: {data.get(\"total_steps\", 0):,}')
    print(f'‚è±Ô∏è Training Time: {data.get(\"duration_seconds\", 0):.1f} seconds')
    
    # WandB URL if available
    wandb_url = data.get('wandb_url', '')
    if wandb_url:
        print(f'üìä WandB Dashboard: {wandb_url}')
    
    # Task-specific assessment
    task_mode = data.get('task_mode', 'unknown')
    best_sim = data.get('best_eval_similarity', 0)
    
    if task_mode == 'eva_denoising':
        if best_sim > 0.7:
            print(f'üéâ OUTSTANDING: EVA denoising similarity > 0.7!')
        elif best_sim > 0.5:
            print(f'‚úÖ GOOD: EVA denoising similarity > 0.5!')
        elif best_sim > 0.2:
            print(f'üìà FAIR: EVA denoising shows learning!')
        else:
            print(f'‚ö†Ô∏è  NEEDS WORK: EVA similarity = {best_sim:.4f}')
    elif task_mode == 'clip_denoising':
        if best_sim > 0.6:
            print(f'üéâ OUTSTANDING: CLIP denoising similarity > 0.6!')
        elif best_sim > 0.4:
            print(f'‚úÖ GOOD: CLIP denoising similarity > 0.4!')
        elif best_sim > 0.15:
            print(f'üìà FAIR: CLIP denoising shows learning!')
        else:
            print(f'‚ö†Ô∏è  NEEDS WORK: CLIP similarity = {best_sim:.4f}')
    
    # Overfitting test results
    if data.get('overfit_test', False):
        overfit_success = data.get('overfit_success', False)
        print(f'üß™ Overfitting Test: {\"‚úÖ PASSED\" if overfit_success else \"‚ùå FAILED\"}')
        if overfit_success:
            print(f'   ‚úÖ Architecture can learn - implementation is correct!')
        else:
            print(f'   ‚ö†Ô∏è  Architecture struggles to overfit - check implementation')
    
    # Final evaluation details
    final_eval = data.get('final_eval', {})
    if final_eval:
        print(f'')
        print(f'üîç Final Evaluation:')
        
        # Determine metric prefix based on task
        if task_mode == 'eva_denoising':
            metric_prefix = 'eval_eva'
        elif task_mode == 'clip_denoising':
            metric_prefix = 'eval_clip'
        else:
            metric_prefix = 'eval_generic'
        
        main_sim_key = f'{metric_prefix}_similarity'
        if main_sim_key in final_eval:
            final_sim = final_eval[main_sim_key]
            print(f'   Overall Similarity: {final_sim:.4f}')
            
            high_qual_key = f'{metric_prefix}_high_quality'
            very_high_qual_key = f'{metric_prefix}_very_high_quality'
            
            if high_qual_key in final_eval:
                high_qual = final_eval[high_qual_key] * 100
                print(f'   High Quality: {high_qual:.1f}%')
            
            if very_high_qual_key in final_eval:
                very_high_qual = final_eval[very_high_qual_key] * 100
                print(f'   Very High Quality: {very_high_qual:.1f}%')
            
            samples_key = f'{metric_prefix}_samples'
            if samples_key in final_eval:
                samples = final_eval[samples_key]
                print(f'   Samples Evaluated: {samples:,}')
    
except Exception as e:
    print(f'Could not parse final summary: {e}')
    # Try to show any available checkpoints
    import os
    try:
        checkpoints = [f for f in os.listdir('$OUTPUT_DIR') if f.endswith('.pt')]
        if checkpoints:
            print(f'Found {len(checkpoints)} checkpoint files')
            print(f'Latest: {max(checkpoints)}')
    except:
        pass
    sys.exit(1)
"
        
        echo ""
        echo "üìÅ Training artifacts saved to: $OUTPUT_DIR"
    else
        echo "‚ö†Ô∏è No final summary found, checking for other outputs..."
        echo "Directory contents:"
        ls -la "$OUTPUT_DIR" 2>/dev/null || echo "Output directory not found"
    fi
    
    

echo ""
echo "üìä GPU Resource Usage Summary:"
nvidia-smi --query-gpu=name,memory.total,memory.used,utilization.gpu --format=csv,noheader,nounits | \
    awk 'BEGIN{print "GPU | Total Memory | Used Memory | Utilization"} {printf "%s | %s MB | %s MB | %s%%\n", $1, $2, $3, $4}'

echo ""
echo "üèÅ Job completed at $(date)"
echo "Total job time: $(echo "scale=2; ($(date +%s) - $SECONDS) / 3600" | bc -l) hours"
echo ""
echo "üìö UNIVERSAL ARCHITECTURE SUMMARY WITH WANDB:"
echo "This job tests Universal BLIP3-o DiT architecture for both EVA and CLIP denoising."
echo "Task mode: $TASK_MODE"

if [ "$TASK_MODE" = "eva_denoising" ]; then
    echo "EVA Denoising: Validates original spherical flow matching on 4096D space."
elif [ "$TASK_MODE" = "clip_denoising" ]; then
    echo "CLIP Denoising: Validates cross-attention conditioning between 1024D and 4096D spaces."
fi

echo "Success indicates the universal architecture is correctly implemented and ready."
echo "WandB provides comprehensive monitoring and visualization of training progress."

# Final WandB summary
if [ "$WANDB_MODE" != "disabled" ]; then
    echo ""
    echo "üìä WandB Summary:"
    echo "  Project: $WANDB_PROJECT"
    echo "  Run: $WANDB_RUN_NAME"
    echo "  Monitor your experiment at: https://wandb.ai/$WANDB_ENTITY/$WANDB_PROJECT"
    echo "  Look for runs tagged with: $WANDB_TAGS_EXTRA"
fi

echo "========================================"

exit $TRAINING_EXIT_CODE