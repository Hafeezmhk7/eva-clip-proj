#!/bin/bash
#SBATCH --job-name=blip3o_multi_gpu_fixed
#SBATCH --partition=gpu_h100
#SBATCH --nodes=1
#SBATCH --gpus=3
#SBATCH --cpus-per-gpu=18
#SBATCH --time=12:00:00
#SBATCH --mem=0
#SBATCH --output=./slurm_out/multi_gpu_joint_%j.out
#SBATCH --error=./slurm_out/multi_gpu_joint_%j.err

echo "üöÄ FIXED BLIP3-o DiT Multi-GPU Training (3x H100 GPUs) - Cosine Scheduler"
echo "================================================================"
echo "Key Fixes:"
echo "  ‚úÖ Removed invalid flow_matching_loss_weight parameter"
echo "  ‚úÖ Proper cosine learning rate scheduler"
echo "  ‚úÖ DDP parameter usage compatibility"
echo "  ‚úÖ All model parameters guaranteed to be used"
echo "================================================================"
echo "Job ID: $SLURM_JOB_ID"
echo "Node: $SLURMD_NODENAME"
echo "Date: $(date)"
echo "Working directory: $(pwd)"
echo "User: $(whoami)"
echo "GPUs requested: $SLURM_GPUS"
echo "CPUs per GPU: $SLURM_CPUS_PER_GPU"
echo "================================================================"

# =============================================================================
# ENVIRONMENT SETUP
# =============================================================================

echo ""
echo "üîß Environment Setup..."
echo "======================="

# Load modules
module purge
module load 2024
module load Miniconda3/24.7.1-0
module load CUDA/12.6.0

# Activate conda environment
source activate eva_clip_env

# Environment check
echo "=== Environment Check ==="
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA: {torch.version.cuda}'); print(f'Devices: {torch.cuda.device_count()}')"
echo "========================="

# Get actual number of GPUs
NUM_GPUS=$(python -c "import torch; print(torch.cuda.device_count())")

echo "üåê Multi-GPU setup:"
echo "   Actual GPUs available: $NUM_GPUS"
echo "   Will use all available GPUs"

# Set up directories (simplified)
export BLIP3O_USER=$(whoami)
export BLIP3O_JOB_ID=${SLURM_JOB_ID}

# Persistent workspace
export BLIP3O_WORKSPACE="/scratch-shared/${BLIP3O_USER}/blip3o_workspace"
export BLIP3O_EMBEDDINGS="${BLIP3O_WORKSPACE}/embeddings"
export BLIP3O_CHECKPOINTS="${BLIP3O_WORKSPACE}/checkpoints"

# Job-specific temp
export BLIP3O_JOB_TEMP="/scratch-local/${BLIP3O_USER}.${BLIP3O_JOB_ID}/blip3o_job_${BLIP3O_JOB_ID}"

# Create directories
mkdir -p "${BLIP3O_WORKSPACE}"/{embeddings,checkpoints,logs}
mkdir -p "${BLIP3O_JOB_TEMP}"/{cache,temp_checkpoints}

# Redirect model caches to temp
export TORCH_HOME="${BLIP3O_JOB_TEMP}/cache/torch"
export HF_HOME="${BLIP3O_JOB_TEMP}/cache/huggingface"
export TRANSFORMERS_CACHE="${BLIP3O_JOB_TEMP}/cache/transformers"

# Create cache directories
mkdir -p "${TORCH_HOME}" "${HF_HOME}" "${TRANSFORMERS_CACHE}"

echo "‚úÖ Environment configured"
echo "   Workspace: ${BLIP3O_WORKSPACE}"
echo "   Job temp: ${BLIP3O_JOB_TEMP}"

# =============================================================================
# FIND EMBEDDINGS
# =============================================================================

echo ""
echo "üîç Finding embeddings..."
echo "========================"

# Find embeddings
EMBEDDINGS_DIR=$(find "${BLIP3O_EMBEDDINGS}" -name "*chunked*256*" -type d | head -1)

if [ -z "$EMBEDDINGS_DIR" ]; then
    echo "‚ùå No 256-token embeddings found in ${BLIP3O_EMBEDDINGS}"
    echo "Available directories:"
    ls -la "${BLIP3O_EMBEDDINGS}" 2>/dev/null || echo "   (none)"
    exit 1
fi

echo "‚úÖ Found embeddings: $EMBEDDINGS_DIR"

# =============================================================================
# TRAINING SETUP
# =============================================================================

echo ""
echo "üìÅ Training Setup..."
echo "=================="

# Create training directories
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
TRAINING_NAME="blip3o_multi_gpu_fixed_cosine_${SLURM_JOB_ID}_${TIMESTAMP}"

TEMP_CHECKPOINT_DIR="${BLIP3O_JOB_TEMP}/temp_checkpoints/${TRAINING_NAME}"
PERSISTENT_CHECKPOINT_DIR="${BLIP3O_CHECKPOINTS}/${TRAINING_NAME}"

mkdir -p "$TEMP_CHECKPOINT_DIR"
mkdir -p "$PERSISTENT_CHECKPOINT_DIR"

echo "‚úÖ Training directories created:"
echo "   Temp: $TEMP_CHECKPOINT_DIR"
echo "   Persistent: $PERSISTENT_CHECKPOINT_DIR"

# =============================================================================
# FIXED MULTI-GPU TRAINING WITH TORCHRUN
# =============================================================================

echo ""
echo "üöÄ Starting FIXED Multi-GPU Training with Cosine Scheduler..."
echo "============================================================="

echo "üîß Fixed Multi-GPU Configuration:"
echo "   üéØ GPUs: ${NUM_GPUS}x H100"
echo "   üìê Model: Large configuration (768 dim, 16 layers)"
echo "   üì¶ Batch size: 8 per GPU"
echo "   üîÑ Gradient accumulation: 2"
echo "   üíæ Memory optimizations: Gradient checkpointing enabled"
echo "   üìà Scheduler: Cosine annealing with warmup"
echo "   ‚úÖ Key Fix: Removed invalid flow_matching_loss_weight parameter"
echo "   ‚úÖ Key Fix: Proper DDP parameter usage"
echo ""

# FIXED: Removed the invalid --flow_matching_loss_weight parameter
torchrun --nproc_per_node=$NUM_GPUS --nnodes=1 --node_rank=0 train_blip3o_dit_multi_gpu.py \
    --chunked_embeddings_dir "$EMBEDDINGS_DIR" \
    --output_dir "$TEMP_CHECKPOINT_DIR" \
    --batch_size 64 \
    --eval_batch_size 8 \
    --num_epochs 15 \
    --model_dim 768 \
    --num_heads 12 \
    --num_layers 16 \
    --mlp_hidden_dim 2048 \
    --mlp_num_layers 3 \
    --mlp_dropout 0.1 \
    --learning_rate 5e-4 \
    --lr_scheduler_type cosine \
    --warmup_ratio 0.05 \
    --weight_decay 0.01 \
    --gradient_accumulation_steps 6 \
    --patch_loss_weight 1.0 \
    --global_loss_weight 2.0 \
    --patch_flow_weight 1.0 \
    --global_flow_weight 3.0 \
    --dataloader_num_workers 2 \
    --fp16

TRAINING_EXIT_CODE=$?

echo ""
echo "üéØ FIXED MULTI-GPU TRAINING COMPLETED WITH EXIT CODE: $TRAINING_EXIT_CODE"

# =============================================================================
# MODEL ARCHIVING
# =============================================================================

if [ $TRAINING_EXIT_CODE -eq 0 ]; then
    echo ""
    echo "‚úÖ Training completed successfully! Archiving model..."
    echo "==================================================="
    
    # Copy to persistent storage
    if [ -d "$TEMP_CHECKPOINT_DIR" ] && [ "$(ls -A "$TEMP_CHECKPOINT_DIR")" ]; then
        echo "üìÅ Copying to persistent storage..."
        cp -r "$TEMP_CHECKPOINT_DIR"/* "$PERSISTENT_CHECKPOINT_DIR/"
        echo "‚úÖ Saved to: $PERSISTENT_CHECKPOINT_DIR"
    fi
    
    echo ""
    echo "üéâ SUCCESS! Fixed Multi-GPU BLIP3-o training with cosine scheduler completed!"
    echo "Model saved to: $PERSISTENT_CHECKPOINT_DIR"
    echo ""
    echo "üîß Applied Fixes:"
    echo "   ‚úÖ Removed invalid flow_matching_loss_weight parameter"
    echo "   ‚úÖ Proper cosine learning rate scheduler implementation"
    echo "   ‚úÖ DDP parameter usage compatibility"
    echo "   ‚úÖ All model parameters guaranteed to be used"
    
else
    echo ""
    echo "‚ùå Multi-GPU training failed"
    echo "Check logs: ./slurm_out/multi_gpu_fixed_${SLURM_JOB_ID}.{out,err}"
    exit 1
fi

echo ""
echo "üéâ Job completed at: $(date)"
echo "‚è±Ô∏è  Total runtime: $SECONDS seconds"