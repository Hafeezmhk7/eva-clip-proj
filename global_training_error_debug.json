{
  "error": "DataLoader with IterableDataset: expected unspecified shuffle option, but got shuffle=True",
  "traceback": "Traceback (most recent call last):\n  File \"/gpfs/home1/scur2711/eva-clip-flow-matching/eva-clip-v3/train_global_blip3o_multi_gpu.py\", line 465, in main\n    train_dataloader, eval_dataloader = create_dataloaders(\n                                        ~~~~~~~~~~~~~~~~~~^\n        chunked_embeddings_dir=args.chunked_embeddings_dir,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<7 lines>...\n        drop_last=True,\n        ^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/gpfs/home1/scur2711/eva-clip-flow-matching/eva-clip-v3/src/modules/datasets/__init__.py\", line 127, in create_enhanced_ddp_dataloaders\n    train_dataloader = create_enhanced_ddp_dataloader(\n        train_dataset,\n    ...<5 lines>...\n        **kwargs\n    )\n  File \"/gpfs/home1/scur2711/eva-clip-flow-matching/eva-clip-v3/src/modules/datasets/__init__.py\", line 81, in create_enhanced_ddp_dataloader\n    dataloader = DataLoader(\n        dataset,\n    ...<8 lines>...\n        **kwargs\n    )\n  File \"/home/scur2711/.conda/envs/eva_clip_env/lib/python3.13/site-packages/torch/utils/data/dataloader.py\", line 339, in __init__\n    raise ValueError(\n        f\"DataLoader with IterableDataset: expected unspecified shuffle option, but got shuffle={shuffle}\"\n    )\nValueError: DataLoader with IterableDataset: expected unspecified shuffle option, but got shuffle=True\n",
  "gpu_info": {
    "cuda_available": true,
    "gpu_count": 3,
    "gpu_names": [
      "NVIDIA H100",
      "NVIDIA H100",
      "NVIDIA H100"
    ],
    "memory_total": [
      93.1114501953125,
      93.1114501953125,
      93.1114501953125
    ],
    "slurm_allocation": "3",
    "cuda_visible_devices": "0,1,2",
    "issues": [],
    "recommendations": []
  },
  "environment": {
    "CUDA_VISIBLE_DEVICES": "0,1,2",
    "SLURM_GPUS": "3",
    "WORLD_SIZE": null,
    "LOCAL_RANK": null
  },
  "timestamp": "2025-07-22T12:28:15.807720",
  "training_type": "global"
}