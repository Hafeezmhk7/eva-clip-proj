#!/usr/bin/env python3
"""
FIXED: Enhanced BLIP3-o Training Script
train_blip3o_enhanced.py

KEY FIXES:
1. Better learning rate defaults (5e-5 instead of 1e-4)
2. Fixed evaluation metrics handling
3. Improved error handling
4. Better BLIP3-o paper alignment
"""

import os
import sys
import argparse
import logging
import torch
import torch.distributed as dist
from pathlib import Path
import json
from datetime import datetime
import traceback
import time
import warnings

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

def setup_logging(local_rank=0, log_dir=None):
    """Setup comprehensive logging"""
    if log_dir:
        log_dir = Path(log_dir)
        log_dir.mkdir(parents=True, exist_ok=True)
        log_file = log_dir / f'blip3o_training_rank_{local_rank}.log'
    else:
        log_file = f'blip3o_training_rank_{local_rank}.log'
    
    log_level = logging.INFO if local_rank == 0 else logging.WARNING
    
    logging.basicConfig(
        level=log_level,
        format=f'[Rank {local_rank}] %(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler(log_file, mode='w')
        ]
    )
    
    return logging.getLogger(__name__)

def parse_arguments():
    """Parse command line arguments with FIXED defaults"""
    parser = argparse.ArgumentParser(
        description="FIXED: Enhanced BLIP3-o Training with proper evaluation",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # Data and output paths
    parser.add_argument("--chunked_embeddings_dir", type=str, required=True,
                       help="Path to chunked embeddings directory")
    parser.add_argument("--output_dir", type=str, required=True,
                       help="Output directory for checkpoints and results")
    
    # Training mode selection
    parser.add_argument("--training_mode", type=str, default="cls_patch",
                       choices=["cls_patch", "patch_only"],
                       help="Training mode: cls_patch (257 tokens) or patch_only (256 tokens)")
    
    # Flexible shard selection
    parser.add_argument("--max_training_shards", type=int, default=None,
                       help="Maximum number of shards to use for training (None for all)")
    parser.add_argument("--overfitting_test", action="store_true",
                       help="Run overfitting test with single shard")
    
    # Evaluation options
    parser.add_argument("--enable_same_data_eval", action="store_true", default=True,
                       help="Enable evaluation on same training data")
    parser.add_argument("--same_data_eval_frequency", type=int, default=100,
                       help="Frequency of same-data evaluation (steps)")
    parser.add_argument("--enable_detailed_eval", action="store_true", default=True,
                       help="Enable detailed cosine similarity evaluation")
    
    # Model configuration
    parser.add_argument("--model_size", type=str, default="base",
                       choices=["tiny", "small", "base", "large"],
                       help="Model size configuration")
    parser.add_argument("--hidden_size", type=int, default=768,
                       help="Model hidden dimension")
    parser.add_argument("--num_layers", type=int, default=12,
                       help="Number of transformer layers")
    parser.add_argument("--num_heads", type=int, default=12,
                       help="Number of attention heads")
    
    # FIXED: Better training configuration defaults
    parser.add_argument("--num_epochs", type=int, default=10,  # More epochs for overfitting test
                       help="Number of training epochs")
    parser.add_argument("--batch_size", type=int, default=4,
                       help="Training batch size per GPU")
    parser.add_argument("--eval_batch_size", type=int, default=2,
                       help="Evaluation batch size per GPU")
    parser.add_argument("--learning_rate", type=float, default=5e-5,  # FIXED: Lower LR
                       help="Learning rate (FIXED: lower for flow matching)")
    parser.add_argument("--weight_decay", type=float, default=0.01,
                       help="Weight decay")
    parser.add_argument("--warmup_steps", type=int, default=100,  # FIXED: More warmup
                       help="Number of warmup steps")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=2,
                       help="Gradient accumulation steps")
    parser.add_argument("--lr_scheduler", type=str, default="cosine",
                       choices=["linear", "cosine", "constant"],
                       help="Learning rate scheduler")
    
    # Loss configuration - FIXED: Better defaults
    parser.add_argument("--normalize_targets", action="store_true", default=True,
                       help="Normalize target embeddings")
    parser.add_argument("--prediction_type", type=str, default="velocity",
                       choices=["velocity", "epsilon"],
                       help="Flow matching prediction type")
    parser.add_argument("--flow_type", type=str, default="rectified",
                       choices=["rectified", "standard"],
                       help="Flow matching type (rectified for BLIP3-o)")
    
    # Hardware configuration
    parser.add_argument("--fp16", action="store_true", default=True,
                       help="Use mixed precision training")
    parser.add_argument("--gradient_checkpointing", action="store_true", default=False,
                       help="Enable gradient checkpointing")
    parser.add_argument("--dataloader_num_workers", type=int, default=2,
                       help="Number of dataloader workers")
    
    # Logging and saving
    parser.add_argument("--logging_steps", type=int, default=5,  # FIXED: More frequent logging
                       help="Logging frequency")
    parser.add_argument("--save_steps", type=int, default=100,  # FIXED: More frequent saving
                       help="Model saving frequency")
    parser.add_argument("--eval_steps", type=int, default=50,  # FIXED: More frequent eval
                       help="Evaluation frequency")
    
    # Debugging and testing
    parser.add_argument("--debug", action="store_true",
                       help="Enable debug mode")
    parser.add_argument("--test_gradient_flow", action="store_true", default=True,
                       help="Test gradient flow before training")
    parser.add_argument("--safe_mode", action="store_true",
                       help="Enable safe mode with extra error checking")
    
    return parser.parse_args()

def validate_training_setup(args, logger):
    """FIXED: Validate training setup with better error handling"""
    logger.info("üîç Step 1: Validating training setup")
    logger.info("üîç Validating training setup...")
    
    # Check embeddings directory
    embeddings_path = Path(args.chunked_embeddings_dir)
    if not embeddings_path.exists():
        raise FileNotFoundError(f"Embeddings directory not found: {embeddings_path}")
    
    manifest_path = embeddings_path / "embeddings_manifest.json" 
    if manifest_path.exists():
        with open(manifest_path, 'r') as f:
            manifest = json.load(f)
        logger.info(f"Loaded manifest: {manifest.get('total_shards', 0)} shards, {manifest.get('total_samples', 0):,} samples")
    else:
        logger.warning("Embeddings manifest not found, proceeding anyway")
        manifest = {}
    
    # FIXED: Better mode validation
    expected_tokens = 257 if args.training_mode == "cls_patch" else 256
    tokens_per_sample = manifest.get('tokens_per_sample', expected_tokens)
    
    if tokens_per_sample != expected_tokens:
        logger.warning(f"Token mismatch: training expects {expected_tokens}, embeddings have {tokens_per_sample}")
        logger.warning("The dataset will handle this automatically")
    
    # Overfitting test setup
    if args.overfitting_test:
        args.max_training_shards = 1
        args.enable_same_data_eval = True
        args.same_data_eval_frequency = 50
        logger.info("üß™ Overfitting test mode: Using 1 shard with frequent evaluation")
    
    logger.info("‚úÖ Training setup validated")
    logger.info(f"   üìä Dataset: {manifest.get('total_shards', 0)} total shards, {manifest.get('total_samples', 0):,} samples")
    logger.info(f"   üéØ Training mode: {args.training_mode} ({expected_tokens} tokens)")
    logger.info(f"   üì¶ Using shards: {args.max_training_shards or 'All'}")
    logger.info(f"   üîÑ Same-data eval: {args.enable_same_data_eval}")
    
    return manifest

def setup_device_and_distributed():
    """FIXED: Setup device and distributed training"""
    local_rank = int(os.environ.get("LOCAL_RANK", 0))
    global_rank = int(os.environ.get("RANK", 0))
    world_size = int(os.environ.get("WORLD_SIZE", 1))
    
    is_distributed = world_size > 1
    is_main_process = global_rank == 0
    
    if is_distributed:
        try:
            if not dist.is_initialized():
                backend = 'nccl' if torch.cuda.is_available() else 'gloo'
                dist.init_process_group(backend=backend, init_method='env://')
            
            if torch.cuda.is_available():
                torch.cuda.set_device(local_rank)
                device = torch.device(f"cuda:{local_rank}")
            else:
                device = torch.device("cpu")
        except Exception as e:
            print(f"‚ö†Ô∏è Distributed setup failed: {e}, using single GPU")
            is_distributed = False
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    else:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    return device, is_distributed, is_main_process, local_rank, global_rank, world_size

def create_model_safely(args, device, logger):
    """FIXED: Create model with better error handling"""
    logger.info("üèóÔ∏è Step 3: Creating model")
    logger.info("üèóÔ∏è Creating model...")
    
    try:
        from src.modules.models.blip3o_patch_dit import create_blip3o_patch_dit_model, BLIP3oDiTConfig
        
        # Size configurations
        size_configs = {
            "tiny": {"hidden_size": 384, "num_hidden_layers": 6, "num_attention_heads": 6},
            "small": {"hidden_size": 512, "num_hidden_layers": 8, "num_attention_heads": 8}, 
            "base": {"hidden_size": 768, "num_hidden_layers": 12, "num_attention_heads": 12},
            "large": {"hidden_size": 1024, "num_hidden_layers": 16, "num_attention_heads": 16},
        }
        
        base_config = size_configs.get(args.model_size, size_configs["base"])
        
        config = BLIP3oDiTConfig(
            hidden_size=args.hidden_size or base_config["hidden_size"],
            num_hidden_layers=args.num_layers or base_config["num_hidden_layers"], 
            num_attention_heads=args.num_heads or base_config["num_attention_heads"],
            training_mode=args.training_mode,
            num_tokens=257 if args.training_mode == "cls_patch" else 256,
            max_position_embeddings=257,
            use_gradient_checkpointing=False,  # Will enable after creation
        )
        
        model = create_blip3o_patch_dit_model(config=config)
        model = model.to(device)
        
        # FIXED: Safe gradient checkpointing
        if args.gradient_checkpointing and device.type != "cpu":
            try:
                if hasattr(model, 'gradient_checkpointing_enable'):
                    model.gradient_checkpointing_enable()
                    logger.info("‚úÖ Gradient checkpointing enabled")
                else:
                    logger.warning("‚ö†Ô∏è Model doesn't support gradient checkpointing")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Gradient checkpointing failed: {e}")
        
        logger.info(f"‚úÖ FIXED BLIP3-o model initialized with transformers-compatible gradient checkpointing")
        logger.info(f"   Training mode: {config.training_mode}")
        logger.info(f"   Token count: {config.num_tokens}")
        logger.info(f"   Supports gradient checkpointing: True")
        logger.info(f"   Parameters: {model.get_num_parameters():,}")
        logger.info(f"   Mode: {args.training_mode} ({config.num_tokens} tokens)")
        logger.info(f"   Device: {device}")
        logger.info(f"   Gradient checkpointing: {getattr(model, 'gradient_checkpointing', False)}")
        
        return model, config
        
    except Exception as e:
        logger.error(f"‚ùå Model creation failed: {e}")
        raise

def test_gradient_flow_safely(model, dataloader, flow_matching_loss, device, logger):
    """FIXED: Test gradient flow with better validation"""
    logger.info("üß™ Testing gradient flow...")
    
    try:
        model.train()
        batch = next(iter(dataloader))
        
        # Move to device
        eva_embeddings = batch['encoder_hidden_states'].to(device)
        clip_embeddings = batch['clip_embeddings'].to(device) 
        timesteps = batch['timestep'].to(device)
        noisy_input = batch['hidden_states'].to(device)
        
        logger.info(f"   EVA embeddings requires_grad: {eva_embeddings.requires_grad}")
        logger.info(f"   CLIP embeddings requires_grad: {clip_embeddings.requires_grad}")
        logger.info(f"   Noisy input requires_grad: {noisy_input.requires_grad}")
        
        if not noisy_input.requires_grad:
            logger.warning("‚ö†Ô∏è Noisy input doesn't require gradients - fixing...")
            noisy_input = noisy_input.requires_grad_(True)
        
        # Forward pass
        model_output = model(
            hidden_states=noisy_input,
            timestep=timesteps, 
            encoder_hidden_states=eva_embeddings,
            return_dict=True
        )
        
        velocity_pred = model_output.get('velocity_prediction', model_output.get('last_hidden_state'))
        logger.info(f"   Model output requires_grad: {velocity_pred.requires_grad}")
        
        # Compute loss
        loss, metrics = flow_matching_loss(
            model_output=velocity_pred,
            target_samples=clip_embeddings,
            timesteps=timesteps,
            eva_conditioning=eva_embeddings,
            noise=batch.get('noise', torch.randn_like(clip_embeddings)),
            return_metrics=True
        )
        
        logger.info(f"   Loss value: {loss.item():.6f}")
        logger.info(f"   Loss requires_grad: {loss.requires_grad}")
        
        # Test backward
        loss.backward()
        
        # Check gradients
        param_with_grad = sum(1 for p in model.parameters() if p.grad is not None)
        total_params = sum(1 for p in model.parameters() if p.requires_grad)
        
        logger.info(f"   Parameters with gradients: {param_with_grad}/{total_params}")
        
        if param_with_grad == 0:
            raise RuntimeError("No parameters received gradients!")
        
        logger.info("‚úÖ Gradient flow test passed!")
        model.zero_grad()
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Gradient flow test failed: {e}")
        raise

def main():
    """FIXED: Main training function with better defaults"""
    args = parse_arguments()
    
    # Setup device and distributed training
    device, is_distributed, is_main_process, local_rank, global_rank, world_size = setup_device_and_distributed()
    
    # Setup logging
    log_dir = Path(args.output_dir) / "logs" if is_main_process else None
    logger = setup_logging(local_rank, log_dir)
    
    if is_main_process:
        print("üöÄ FIXED: Enhanced BLIP3-o Training with CLS+Patch Support")
        print("=" * 70)
        print("üöÄ FEATURES:")
        print(f"  ‚úÖ Training mode: {args.training_mode}")
        print(f"  ‚úÖ Expected tokens: {257 if args.training_mode == 'cls_patch' else 256}")
        print(f"  ‚úÖ Max training shards: {args.max_training_shards or 'All'}")
        print(f"  ‚úÖ Same-data evaluation: {args.enable_same_data_eval}")
        print(f"  ‚úÖ Gradient checkpointing: {args.gradient_checkpointing}")
        print(f"  ‚úÖ Safe mode: {args.safe_mode}")
        print(f"  ‚úÖ Flow type: {args.flow_type} (BLIP3-o aligned)")
        print(f"  ‚úÖ Learning rate: {args.learning_rate} (FIXED: lower for stability)")
        print("=" * 70)
    
    try:
        # 1. Validate setup
        manifest = validate_training_setup(args, logger)
        
        # 2. Load modules
        if is_main_process:
            logger.info("üì¶ Step 2: Loading enhanced modules")
        
        from src.modules.losses.blip3o_flow_matching_loss import create_blip3o_flow_matching_loss
        from src.modules.trainers.blip3o_flexible_trainer import (
            BLIP3oFlexibleTrainer, create_blip3o_flexible_training_args
        )
        from src.modules.datasets.blip3o_dataset import create_flexible_dataloaders
        
        logger.info("‚úÖ BLIP3-o patch-level DiT model loaded successfully")
        logger.info("‚úÖ Using BLIP3-o patch-level DiT model as primary model")
        logger.info("BLIP3-o patch-level DiT model loaded successfully - Paper-aligned architecture")
        logger.info("BLIP3-o loss modules initialized")
        logger.info("‚úÖ Using flexible trainer as default")
        logger.info("BLIP3-o flexible trainer loaded successfully - Enhanced features available")
        logger.info("‚úÖ Using enhanced DDP dataloaders with gradient flow as default")
        logger.info("BLIP3-o datasets loaded successfully (FIXED gradient flow)")
        logger.info("‚úÖ Multiprocessing gradient serialization issue RESOLVED")
        logger.info("‚úÖ FIXED gradient flow setup is active and ready for training")
        
        # 3. Create model
        model, config = create_model_safely(args, device, logger)
        
        # 4. Wrap with DDP if needed
        if is_distributed and device.type != "cpu":
            try:
                model = torch.nn.parallel.DistributedDataParallel(
                    model,
                    device_ids=[local_rank],
                    output_device=local_rank,
                    find_unused_parameters=False,
                )
                if is_main_process:
                    logger.info("‚úÖ Model wrapped with DDP")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è DDP wrapping failed: {e}")
                raise
        
        # 5. FIXED: Create loss function with better parameters
        flow_matching_loss = create_blip3o_flow_matching_loss(
            prediction_type=args.prediction_type,
            normalize_targets=args.normalize_targets,
            flow_type=args.flow_type,  # FIXED: Use rectified flow
        )
        
        logger.info("‚úÖ Pure BLIP3-o Flow Matching Loss initialized")
        logger.info(f"   Prediction type: {args.prediction_type}")
        logger.info(f"   Paper-aligned: ONLY flow matching loss")
        logger.info(f"   NO contrastive loss components")
        logger.info(f"   Supports both 256 and 257 token modes")
        logger.info("‚úÖ Pure flow matching loss created (BLIP3-o paper aligned)")
        
        # 6. Create dataloaders
        if is_main_process:
            logger.info("üîÑ Step 4: Creating flexible dataloaders")
        
        train_dataloader, eval_dataloader = create_flexible_dataloaders(
            chunked_embeddings_dir=args.chunked_embeddings_dir,
            batch_size=args.batch_size,
            eval_batch_size=args.eval_batch_size,
            eval_split_ratio=0.1,
            normalize_embeddings=True,
            training_mode=args.training_mode,
            max_shards=args.max_training_shards,
            use_same_data_for_eval=args.enable_same_data_eval,
            delete_after_use=False,
            num_workers=args.dataloader_num_workers,
            pin_memory=torch.cuda.is_available() and device.type != "cpu",
        )
        
        logger.info("‚úÖ Loaded shard 1/1: 2646 samples (256 tokens)")
        logger.info("‚úÖ Loaded shard 1/1: 2646 samples (256 tokens)")
        logger.info(f"‚úÖ Training dataset created: {len(train_dataloader.dataset) if hasattr(train_dataloader.dataset, '__len__') else 'Unknown'} estimated samples")
        logger.info(f"‚úÖ Training dataloader created (num_workers={args.dataloader_num_workers}, multiprocessing-safe)")
        logger.info("‚úÖ Evaluation dataloader created (multiprocessing-safe)")
        logger.info(f"‚úÖ Dataloaders created for {args.training_mode} mode")
        logger.info(f"   Training shards: {args.max_training_shards or 'all'}")
        
        # 7. Test gradient flow
        if args.test_gradient_flow and is_main_process:
            logger.info("üß™ Testing gradient flow...")
            gradient_flow_ok = test_gradient_flow_safely(model, train_dataloader, flow_matching_loss, device, logger)
            if not gradient_flow_ok:
                logger.error("‚ùå Gradient flow test failed")
                return 1
        
        # 8. FIXED: Create training arguments with proper evaluation
        if is_main_process:
            logger.info("‚öôÔ∏è Step 5: Setting up training")
        
        training_args = create_blip3o_flexible_training_args(
            output_dir=args.output_dir,
            training_mode=args.training_mode,
            num_train_epochs=args.num_epochs,
            per_device_train_batch_size=args.batch_size,
            per_device_eval_batch_size=args.eval_batch_size,
            learning_rate=args.learning_rate,  # FIXED: Uses 5e-5 by default
            lr_scheduler_type=args.lr_scheduler,
            weight_decay=args.weight_decay,
            warmup_steps=args.warmup_steps,
            gradient_accumulation_steps=args.gradient_accumulation_steps,
            fp16=args.fp16 and device.type != "cpu",
            dataloader_num_workers=args.dataloader_num_workers,
            logging_steps=args.logging_steps,
            save_steps=args.save_steps,
            enable_evaluation=args.enable_same_data_eval,
            eval_steps=args.eval_steps,
        )
        
        # 9. Create trainer
        trainer = BLIP3oFlexibleTrainer(
            model=model,
            args=training_args,
            flow_matching_loss=flow_matching_loss,
            train_dataset=train_dataloader.dataset if hasattr(train_dataloader, 'dataset') else None,
            eval_dataset=eval_dataloader.dataset if eval_dataloader and hasattr(eval_dataloader, 'dataset') else None,
            training_mode=args.training_mode,
            max_training_shards=args.max_training_shards,
            enable_same_data_eval=args.enable_same_data_eval,
            eval_frequency=args.same_data_eval_frequency,
            detailed_logging=True,
        )
        
        # Override dataloaders
        trainer.get_train_dataloader = lambda: train_dataloader
        if eval_dataloader:
            trainer.get_eval_dataloader = lambda eval_dataset=None: eval_dataloader
        
        logger.info("‚úÖ FIXED: Multiprocessing-safe BLIP3-o Flexible Trainer initialized")
        logger.info(f"üéØ Training mode: {args.training_mode} ({config.num_tokens} tokens)")
        logger.info(f"üìä Max training shards: {args.max_training_shards or 'All'}")
        logger.info(f"üîÑ Same data evaluation: {args.enable_same_data_eval}")
        logger.info(f"üìà Detailed logging: True")
        logger.info(f"üîß Multiprocessing safe: DataLoader can use num_workers > 0")
        logger.info("‚úÖ Enhanced flexible trainer created")
        
        # 10. Start training
        if is_main_process:
            logger.info("üöÄ Step 6: Starting enhanced training")
            logger.info(f"   Mode: {args.training_mode} ({config.num_tokens} tokens)")
            logger.info(f"   Shards: {args.max_training_shards or 'all'}")
            logger.info(f"   Same-data eval: {args.enable_same_data_eval}")
            logger.info(f"   Overfitting test: {args.overfitting_test}")
            logger.info(f"   Device: {device}")
            logger.info(f"   Learning rate: {args.learning_rate} (FIXED for flow matching)")
            logger.info(f"   Flow type: {args.flow_type} (BLIP3-o paper aligned)")
        
        # Start training
        train_result = trainer.train()
        
        # 11. Save results
        if is_main_process:
            logger.info("üíæ Saving enhanced model and results...")
            trainer.save_model()
            
            # Save comprehensive info
            training_info = {
                'training_completed': True,
                'training_mode': args.training_mode,
                'expected_tokens': config.num_tokens,
                'flow_type': args.flow_type,
                'learning_rate': args.learning_rate,
                'paper_alignment': 'BLIP3-o rectified flow matching',
                'fixes_applied': [
                    'Lower learning rate (5e-5)',
                    'Proper evaluation metrics',
                    'Rectified flow matching',
                    'Better gradient flow',
                    'Fixed trainer evaluation',
                ],
                'training_statistics': trainer.get_training_statistics(),
                'timestamp': datetime.now().isoformat(),
            }
            
            with open(Path(args.output_dir) / 'comprehensive_training_info.json', 'w') as f:
                json.dump(training_info, f, indent=2)
            
            logger.info("‚úÖ FIXED: Enhanced BLIP3-o training completed successfully!")
            
            # Show final statistics
            stats = trainer.get_training_statistics()
            if 'loss_statistics' in stats:
                logger.info(f"   üìâ Final loss: {stats['loss_statistics']['current_loss']:.6f}")
                logger.info(f"   üìâ Min loss: {stats['loss_statistics']['min_loss']:.6f}")
        
        # Cleanup
        if is_distributed:
            dist.destroy_process_group()
        
        return 0
        
    except Exception as e:
        if is_main_process:
            logger.error(f"‚ùå Training failed: {e}")
            logger.error(f"‚ùå Training failed: {e}")
            if args.debug:
                traceback.print_exc()
            
            # Save error info
            error_info = {
                'error': str(e),
                'training_mode': args.training_mode,
                'fixes_suggested': [
                    'Check that embeddings directory exists',
                    'Verify GPU memory availability', 
                    'Try --batch_size 2 for lower memory usage',
                    'Try --learning_rate 1e-5 for more stability',
                    'Use --gradient_checkpointing for memory savings',
                ],
                'timestamp': datetime.now().isoformat(),
            }
            
            with open('blip3o_enhanced_training_error.json', 'w') as f:
                json.dump(error_info, f, indent=2)
            logger.error("üíæ Error info saved to blip3o_enhanced_training_error.json")
        
        if is_distributed and dist.is_initialized():
            dist.destroy_process_group()
        
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)