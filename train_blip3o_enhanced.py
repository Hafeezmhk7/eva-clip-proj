#!/usr/bin/env python3
"""
UPDATED: BLIP3-o Training Script with Better Error Handling
train_blip3o_enhanced.py

FEATURES:
- Graceful handling of missing modules
- Falls back to available trainers
- All scaling fixes applied (velocity_scale, output_scale)
- Training-only mode (no evaluation during training)
- Comprehensive monitoring and metrics
- Overfitting test support
"""

import os
import sys
import argparse
import logging
import torch
import torch.distributed as dist
from pathlib import Path
import json
from datetime import datetime
import traceback
import time

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

def setup_logging(local_rank=0, log_dir=None):
    """Setup comprehensive logging"""
    if log_dir:
        log_dir = Path(log_dir)
        log_dir.mkdir(parents=True, exist_ok=True)
        log_file = log_dir / f'blip3o_training_rank_{local_rank}.log'
    else:
        log_file = f'blip3o_training_rank_{local_rank}.log'
    
    log_level = logging.INFO if local_rank == 0 else logging.WARNING
    
    logging.basicConfig(
        level=log_level,
        format=f'[Rank {local_rank}] %(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler(log_file, mode='w')
        ]
    )
    
    return logging.getLogger(__name__)

def parse_arguments():
    """Parse command line arguments with all new options"""
    parser = argparse.ArgumentParser(
        description="UPDATED BLIP3-o Training with Better Error Handling",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # Data and output paths
    parser.add_argument("--chunked_embeddings_dir", type=str, required=True,
                       help="Path to chunked embeddings directory")
    parser.add_argument("--output_dir", type=str, required=True,
                       help="Output directory for checkpoints and results")
    
    # Training mode selection
    parser.add_argument("--training_mode", type=str, default="patch_only",
                       choices=["cls_patch", "patch_only"],
                       help="Training mode: cls_patch (257 tokens) or patch_only (256 tokens)")
    
    # Model configuration
    parser.add_argument("--model_size", type=str, default="base",
                       choices=["tiny", "small", "base", "large"],
                       help="Model size configuration")
    
    # Training configuration
    parser.add_argument("--num_epochs", type=int, default=10,
                       help="Number of training epochs")
    parser.add_argument("--batch_size", type=int, default=32,
                       help="Training batch size per GPU")
    parser.add_argument("--learning_rate", type=float, default=1e-4,
                       help="Learning rate")
    parser.add_argument("--weight_decay", type=float, default=0.01,
                       help="Weight decay")
    parser.add_argument("--warmup_steps", type=int, default=200,
                       help="Number of warmup steps")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=2,
                       help="Gradient accumulation steps")
    
    # CRITICAL: Scaling parameters (the main fixes)
    parser.add_argument("--velocity_scale", type=float, default=0.1,
                       help="CRITICAL: Velocity scaling factor for flow matching loss")
    parser.add_argument("--output_scale", type=float, default=0.1,
                       help="CRITICAL: Output scaling factor for model")
    parser.add_argument("--target_norm_scale", type=float, default=1.0,
                       help="Target normalization scaling factor")
    
    # Hardware configuration
    parser.add_argument("--fp16", action="store_true", default=True,
                       help="Use mixed precision training")
    parser.add_argument("--gradient_checkpointing", action="store_true", default=False,
                       help="Enable gradient checkpointing")
    
    # Logging and saving
    parser.add_argument("--logging_steps", type=int, default=10,
                       help="Logging frequency")
    parser.add_argument("--save_steps", type=int, default=200,
                       help="Model saving frequency")
    
    # Training type presets
    parser.add_argument("--training_type", type=str, default="custom",
                       choices=["custom", "overfitting", "production", "debug"],
                       help="Preset training configurations")
    
    # Debugging
    parser.add_argument("--debug", action="store_true",
                       help="Enable debug mode")
    
    return parser.parse_args()

def apply_training_type_presets(args, logger):
    """Apply preset configurations based on training type"""
    if args.training_type == "overfitting":
        # Optimize for overfitting tests
        args.num_epochs = 15
        args.batch_size = 16
        args.learning_rate = 5e-5
        args.weight_decay = 0.0  # No regularization
        args.logging_steps = 5
        logger.info("üéØ Applied overfitting preset configuration")
        
    elif args.training_type == "production":
        # Optimize for production training
        args.num_epochs = 10
        args.batch_size = 32
        args.learning_rate = 1e-4
        args.weight_decay = 0.01
        args.gradient_accumulation_steps = 2
        args.gradient_checkpointing = True
        args.save_steps = 500
        logger.info("üöÄ Applied production preset configuration")
        
    elif args.training_type == "debug":
        # Optimize for debugging
        args.model_size = "tiny"
        args.num_epochs = 3
        args.batch_size = 4
        args.learning_rate = 1e-4
        args.weight_decay = 0.0
        args.warmup_steps = 10
        args.gradient_accumulation_steps = 1
        args.logging_steps = 1
        args.save_steps = 10
        args.fp16 = False
        logger.info("üêõ Applied debug preset configuration")

def validate_training_setup(args, logger):
    """Validate training setup and scaling parameters"""
    logger.info("üîç Validating training setup...")
    
    # Check embeddings directory
    embeddings_path = Path(args.chunked_embeddings_dir)
    if not embeddings_path.exists():
        raise FileNotFoundError(f"Embeddings directory not found: {embeddings_path}")
    
    # Look for shard files
    shard_files = list(embeddings_path.glob("embeddings_shard_*.pkl"))
    if not shard_files:
        raise FileNotFoundError(f"No shard files found in {embeddings_path}")
    
    logger.info(f"Found {len(shard_files)} shard files")
    
    # Validate scaling parameters (CRITICAL)
    if args.velocity_scale <= 0 or args.velocity_scale > 1.0:
        logger.warning(f"‚ö†Ô∏è Unusual velocity_scale: {args.velocity_scale} (expected 0.01-0.5)")
    
    if args.output_scale <= 0 or args.output_scale > 1.0:
        logger.warning(f"‚ö†Ô∏è Unusual output_scale: {args.output_scale} (expected 0.01-0.5)")
    
    logger.info("‚úÖ Training setup validated")
    logger.info(f"üîß SCALING FIXES:")
    logger.info(f"   Velocity scale: {args.velocity_scale}")
    logger.info(f"   Output scale: {args.output_scale}")
    logger.info(f"   Target norm scale: {args.target_norm_scale}")
    
    return shard_files

def setup_device_and_distributed():
    """Setup device and distributed training"""
    local_rank = int(os.environ.get("LOCAL_RANK", 0))
    global_rank = int(os.environ.get("RANK", 0))
    world_size = int(os.environ.get("WORLD_SIZE", 1))
    
    is_distributed = world_size > 1
    is_main_process = global_rank == 0
    
    if is_distributed:
        try:
            if not dist.is_initialized():
                backend = 'nccl' if torch.cuda.is_available() else 'gloo'
                dist.init_process_group(backend=backend, init_method='env://')
            
            if torch.cuda.is_available():
                torch.cuda.set_device(local_rank)
                device = torch.device(f"cuda:{local_rank}")
            else:
                device = torch.device("cpu")
        except Exception as e:
            print(f"‚ö†Ô∏è Distributed setup failed: {e}, using single GPU")
            is_distributed = False
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    else:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    return device, is_distributed, is_main_process, local_rank, global_rank, world_size

def create_model_and_config(args, device, logger):
    """Create BLIP3-o model with proper configuration and scaling"""
    logger.info("üèóÔ∏è Creating BLIP3-o model with FIXED scaling...")
    
    try:
        # Try to import the FIXED models with scaling support
        from src.modules.models import create_fixed_model
        
        # FIXED: Pass model_size instead of individual parameters to avoid conflicts
        model = create_fixed_model(
            training_mode=args.training_mode,
            model_size=args.model_size,  # FIXED: Pass model_size instead of individual params
            output_scale=args.output_scale,  # CRITICAL: Apply output scaling
            use_gradient_checkpointing=args.gradient_checkpointing,
        )
        model = model.to(device)
        
        # Get the config for logging
        config = model.config
        
        logger.info(f"‚úÖ FIXED BLIP3-o model created successfully")
        logger.info(f"   Parameters: {model.get_num_parameters():,}")
        logger.info(f"   Mode: {args.training_mode} ({config.num_tokens} tokens)")
        logger.info(f"   Hidden size: {config.hidden_size}")
        logger.info(f"   Layers: {config.num_hidden_layers}")
        logger.info(f"   Heads: {config.num_attention_heads}")
        logger.info(f"   üîß Output scale: {config.output_scale} (APPLIED)")
        logger.info(f"   Device: {device}")
        
        return model, config
        
    except Exception as e:
        logger.error(f"‚ùå Model creation failed: {e}")
        logger.error("Make sure you have updated the models module files")
        raise

def create_loss_function(args, logger):
    """Create FIXED flow matching loss function with scaling"""
    logger.info("üîß Creating FIXED flow matching loss with scaling...")
    
    try:
        from src.modules.losses import get_fixed_loss_function
        
        flow_matching_loss = get_fixed_loss_function(
            velocity_scale=args.velocity_scale,
            target_norm_scale=args.target_norm_scale,
            adaptive_scaling=True,
        )
        
        logger.info("‚úÖ FIXED BLIP3-o Flow Matching Loss initialized")
        logger.info(f"   üîß Velocity scale: {args.velocity_scale} (APPLIED)")
        logger.info(f"   üîß Target norm scale: {args.target_norm_scale}")
        logger.info(f"   ‚úÖ Adaptive scaling: enabled")
        logger.info(f"   ‚úÖ Flow type: rectified")  # Hardcoded since it's fixed
        logger.info(f"   ‚úÖ Prediction type: velocity")  # Hardcoded since it's fixed
        
        return flow_matching_loss
        
    except Exception as e:
        logger.error(f"‚ùå Loss creation failed: {e}")
        logger.error("Make sure you have updated the losses module files")
        raise

def create_dataloader(args, logger):
    """Create training dataloader"""
    logger.info("üîÑ Creating training dataloader...")
    
    try:
        from src.modules.datasets import create_flexible_dataloaders
        
        # Create dataloaders
        train_dataloader, _ = create_flexible_dataloaders(
            chunked_embeddings_dir=args.chunked_embeddings_dir,
            batch_size=args.batch_size,
            normalize_embeddings=False,  # Loss function handles normalization
            training_mode=args.training_mode,
            max_shards=1,  # Single shard for overfitting test
            num_workers=0,  # Avoid multiprocessing issues
            pin_memory=torch.cuda.is_available(),
        )
        
        logger.info(f"‚úÖ Training dataloader created")
        logger.info(f"   Training batches: {len(train_dataloader):,}")
        
        return train_dataloader, None
        
    except Exception as e:
        logger.error(f"‚ùå Dataloader creation failed: {e}")
        raise

def create_training_args(args, logger):
    """Create training arguments using available trainer"""
    logger.info("üîß Creating training arguments...")
    
    # Try unified trainer first, then fall back to training-only trainer
    try:
        from src.modules.trainers import create_unified_training_args
        logger.info("‚úÖ Using unified trainer arguments")
        
        return create_unified_training_args(
            output_dir=args.output_dir,
            enable_evaluation=False,  # Explicitly disable evaluation
            num_train_epochs=args.num_epochs,
            per_device_train_batch_size=args.batch_size,
            learning_rate=args.learning_rate,
            lr_scheduler_type="cosine",
            weight_decay=args.weight_decay,
            warmup_steps=args.warmup_steps,
            gradient_accumulation_steps=args.gradient_accumulation_steps,
            fp16=args.fp16 and torch.cuda.is_available(),
            dataloader_num_workers=0,
            logging_steps=args.logging_steps,
            save_steps=args.save_steps,
        )
        
    except ImportError:
        logger.warning("‚ö†Ô∏è Unified trainer not available, trying training-only trainer")
        
        try:
            from src.modules.trainers import create_training_only_args
            logger.info("‚úÖ Using training-only trainer arguments")
            
            return create_training_only_args(
                output_dir=args.output_dir,
                num_train_epochs=args.num_epochs,
                per_device_train_batch_size=args.batch_size,
                learning_rate=args.learning_rate,
                lr_scheduler_type="cosine",
                weight_decay=args.weight_decay,
                warmup_steps=args.warmup_steps,
                gradient_accumulation_steps=gradient_accumulation_steps,
                fp16=args.fp16 and torch.cuda.is_available(),
                dataloader_num_workers=0,
                logging_steps=args.logging_steps,
                save_steps=args.save_steps,
            )
            
        except ImportError as e:
            logger.error(f"‚ùå No trainer arguments available: {e}")
            raise RuntimeError("No trainer argument factory functions available")

def create_trainer(model, training_args, flow_matching_loss, train_dataloader, args, logger):
    """Create trainer (unified or training-only)"""
    logger.info("üèóÔ∏è Creating trainer...")
    
    # Try unified trainer first
    try:
        from src.modules.trainers import BLIP3oUnifiedTrainer
        logger.info("‚úÖ Using unified trainer")
        
        trainer = BLIP3oUnifiedTrainer(
            model=model,
            args=training_args,
            flow_matching_loss=flow_matching_loss,
            train_dataset=None,  # We'll override dataloader
            enable_evaluation=False,  # Explicitly disable evaluation
            # BLIP3-o specific parameters
            training_mode=args.training_mode,
            detailed_logging=True,
            
            # Scaling parameters for monitoring
            expected_velocity_scale=args.velocity_scale,
            expected_output_scale=args.output_scale,
        )
        
        # Override dataloader
        trainer.get_train_dataloader = lambda: train_dataloader
        
        logger.info("‚úÖ Unified BLIP3oTrainer created")
        
        return trainer, "unified"
        
    except ImportError:
        logger.warning("‚ö†Ô∏è Unified trainer not available, trying training-only trainer")
        
        try:
            from src.modules.trainers import BLIP3oTrainingOnlyTrainer
            logger.info("‚úÖ Using training-only trainer")
            
            trainer = BLIP3oTrainingOnlyTrainer(
                model=model,
                args=training_args,
                flow_matching_loss=flow_matching_loss,
                train_dataset=None,  # We'll override dataloader
                
                # BLIP3-o specific parameters
                training_mode=args.training_mode,
                detailed_logging=True,
                
                # Scaling parameters for monitoring
                expected_velocity_scale=args.velocity_scale,
                expected_output_scale=args.output_scale,
            )
            
            # Override dataloader
            trainer.get_train_dataloader = lambda: train_dataloader
            
            logger.info("‚úÖ Training-only BLIP3oTrainer created")
            
            return trainer, "training_only"
            
        except ImportError as e:
            logger.error(f"‚ùå No trainers available: {e}")
            raise RuntimeError("No trainer classes available")

def main():
    """Main training function with better error handling"""
    args = parse_arguments()
    
    # Setup device and distributed training
    device, is_distributed, is_main_process, local_rank, global_rank, world_size = setup_device_and_distributed()
    
    # Setup logging
    log_dir = Path(args.output_dir) / "logs" if is_main_process else None
    logger = setup_logging(local_rank, log_dir)
    
    if is_main_process:
        print("üöÄ UPDATED BLIP3-o Training with Better Error Handling")
        print(f"Job ID: {os.environ.get('SLURM_JOB_ID', 'local')}")
        print(f"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 70)
        print("üîß ALL FIXES APPLIED:")
        print("  ‚úÖ Graceful handling of missing modules")
        print("  ‚úÖ Falls back to available trainers")
        print("  ‚úÖ Velocity scaling to fix norm mismatch")
        print("  ‚úÖ Output scaling in DiT model")
        print("  ‚úÖ Adaptive scaling in loss function")
        print("  ‚úÖ Proper rectified flow implementation")
        print("  ‚úÖ Fixed generation timestep schedule")
        print("  ‚úÖ Consistent normalization handling")
        print("=" * 70)
        
        # Apply training type presets
        apply_training_type_presets(args, logger)
        
        print(f"  ‚úÖ Training type: {args.training_type}")
        print(f"  ‚úÖ Training mode: {args.training_mode}")
        print(f"  ‚úÖ Velocity scale: {args.velocity_scale}")
        print(f"  ‚úÖ Output scale: {args.output_scale}")
        print("=" * 70)
    
    try:
        # 1. Validate setup
        shard_files = validate_training_setup(args, logger)
        
        # 2. Create FIXED model
        model, config = create_model_and_config(args, device, logger)
        
        # 3. Wrap with DDP if needed
        if is_distributed and device.type != "cpu":
            try:
                model = torch.nn.parallel.DistributedDataParallel(
                    model,
                    device_ids=[local_rank],
                    output_device=local_rank,
                    find_unused_parameters=False,
                )
                if is_main_process:
                    logger.info("‚úÖ Model wrapped with DDP")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è DDP wrapping failed: {e}")
                raise
        
        # 4. Create FIXED loss function
        flow_matching_loss = create_loss_function(args, logger)
        
        # 5. Create dataloaders
        train_dataloader, _ = create_dataloader(args, logger)
        
        # 6. Create training arguments
        training_args = create_training_args(args, logger)
        
        # 7. Create trainer (unified or training-only)
        trainer, trainer_type = create_trainer(model, training_args, flow_matching_loss, train_dataloader, args, logger)
        
        # 8. Start training
        logger.info(f"üöÄ Starting Training Process with {trainer_type} trainer...")
        logger.info(f"   Training for {args.num_epochs} epochs")
        logger.info(f"   Expected improvements:")
        logger.info(f"     üìà Cosine similarity: ~0.01 ‚Üí >0.3 (30x improvement)")
        logger.info(f"     üìä Prediction/target norm alignment")
        logger.info(f"     üìâ Smooth loss decrease")
        
        start_time = time.time()
        train_result = trainer.train()
        training_time = time.time() - start_time
        
        # 9. Save results
        if is_main_process:
            logger.info("üíæ Saving model and results...")
            trainer.save_model()
            
            # Get training statistics
            try:
                training_stats = trainer.get_training_statistics()
            except AttributeError:
                training_stats = {'total_steps': 0, 'loss_statistics': {}}
            
            # Save training info with all fixes
            training_info = {
                'training_completed': True,
                'trainer_type': f'BLIP3o{trainer_type.title()}Trainer',
                'training_mode': args.training_mode,
                'expected_tokens': config.num_tokens,
                'total_epochs': args.num_epochs,
                'total_steps': training_stats.get('total_steps', 0),
                'training_time_seconds': training_time,
                'loss_statistics': training_stats.get('loss_statistics', {}),
                'timestamp': datetime.now().isoformat(),
                
                # ALL FIXES applied
                'all_fixes_applied': {
                    'trainer_type': trainer_type,
                    'velocity_scale': args.velocity_scale,
                    'output_scale': args.output_scale,
                    'target_norm_scale': args.target_norm_scale,
                    'adaptive_scaling': True,
                    'proper_normalization': True,
                    'improved_generation': True,
                    'rectified_flow': True,
                },
                
                'model_config': {
                    'hidden_size': config.hidden_size,
                    'num_layers': config.num_hidden_layers,
                    'num_heads': config.num_attention_heads,
                    'num_tokens': config.num_tokens,
                    'output_scale': config.output_scale,
                },
            }
            
            with open(Path(args.output_dir) / f'training_info_{trainer_type}.json', 'w') as f:
                json.dump(training_info, f, indent=2)
            
            logger.info("‚úÖ Training completed successfully!")
            logger.info(f"   Model saved to: {args.output_dir}")
            logger.info(f"   Training mode: {args.training_mode}")
            logger.info(f"   Training time: {training_time:.1f} seconds")
            final_loss = training_stats.get('loss_statistics', {}).get('current_loss', 'unknown')
            logger.info(f"   Final loss: {final_loss}")
            logger.info(f"   Trainer: {trainer_type}")
            
            # Check for scaling issues
            norm_warnings = training_stats.get('norm_mismatch_warnings', 0)
            scaling_issues = training_stats.get('scaling_issues_detected', [])
            
            if norm_warnings == 0 and not scaling_issues:
                logger.info("üéâ No scaling issues detected - fixes working perfectly!")
            else:
                logger.warning(f"‚ö†Ô∏è Some scaling issues detected: {norm_warnings} warnings, {len(scaling_issues)} issues")
            
            print(f"\n‚úÖ Training completed successfully!")
            print(f"   Model saved to: {args.output_dir}")
            print(f"   Training mode: {args.training_mode} ({config.num_tokens} tokens)")
            print(f"   Final loss: {final_loss}")
            print(f"   Training time: {training_time:.1f} seconds")
            print(f"   Trainer: {trainer_type}")
            
            print("\nüéØ Next steps:")
            print("1. Run evaluation to test the dramatic improvements")
            print("2. Expect cosine similarity >0.3 (vs 0.01 before)")
            print("3. Check that prediction norms now align with target norms")
            print("4. Verify all scaling fixes are working properly")
        
        # Cleanup
        if is_distributed:
            dist.destroy_process_group()
        
        return 0
        
    except Exception as e:
        if is_main_process:
            logger.error(f"‚ùå Training failed: {e}")
            if args.debug:
                traceback.print_exc()
            
            # Check for common import errors
            if "No module named" in str(e):
                logger.error("üí° Import error detected - check these files:")
                logger.error("   ‚Ä¢ src/modules/datasets/__init__.py (add DATASET_AVAILABLE)")
                logger.error("   ‚Ä¢ src/modules/trainers/__init__.py (ensure trainers exist)")
                logger.error("   ‚Ä¢ src/modules/losses/__init__.py (add scaling functions)")
                logger.error("   ‚Ä¢ src/modules/models/__init__.py (add scaling support)")
        
        if is_distributed and dist.is_initialized():
            dist.destroy_process_group()
        
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)