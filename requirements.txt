Here's the updated README with a comprehensive overview of the BLIP3-o Enhanced Patch-Level DiT implementation:

```markdown
# BLIP3-o Enhanced Patch-Level DiT: Image-to-Text Translation via Flow Matching

## 🏗️ Architecture Overview

```mermaid
graph TD
    A[Input Images] --> B[EVA-CLIP Encoder]
    A --> C[CLIP ViT Encoder]
    
    B --> D["EVA Features<br>(B, 257, 4096)"]
    C --> E["CLIP Features<br>(B, 257, 1024)"]
    
    D --> F["Cross-Attention<br>Conditioning"]
    E --> G["Flow Matching Target"]
    
    H["Noise<br>(B, 257, 1024)"] --> I["Linear Interpolation<br>xₜ = (1-α)x₀ + αx₁"]
    G --> I
    
    I --> J["BLIP3-o DiT Model<br>12 Layers, 768 Hidden"]
    F --> J
    
    J --> K["Velocity Prediction<br>(B, 257, 1024)"]
    
    K --> L["Flow Matching Loss<br>MSE(v_pred, v_target)"]
    G --> L
    
    style J fill:#e1f5fe
    style L fill:#ffebee
    style F fill:#f3e5f5
```

### Core Components:
1. **EVA-CLIP Encoder**: Extracts 4096-dimensional features (257 tokens per image)
2. **CLIP ViT Encoder**: Extracts 1024-dimensional patch embeddings (257 tokens)
3. **Linear Interpolation**: Creates noisy inputs for flow matching
4. **DiT Model**: 12-layer transformer with:
   - Rotary position embeddings
   - Cross-attention conditioning
   - Gradient checkpointing support
5. **Flow Matching Loss**: Pure velocity prediction (BLIP3-o paper aligned)

## 📥 Embedding Extraction

### Extract CLS+Patch Embeddings (257 tokens):
```bash
python src/modules/extract_embeddings_g.py \
    --output_dir "./embeddings/cls_patch_257" \
    --training_mode "cls_patch" \
    --batch_size 32
```

### Extract Patch-Only Embeddings (256 tokens):
```bash
python src/modules/extract_embeddings_g.py \
    --output_dir "./embeddings/patch_only_256" \
    --training_mode "patch_only" \
    --batch_size 64
```

## 🚀 Training Commands

### CLS+Patch Training (257 tokens):
```bash
python train_blip3o_enhanced.py \
    --chunked_embeddings_dir "./embeddings/cls_patch_257" \
    --output_dir "./checkpoints/cls_patch" \
    --training_mode "cls_patch" \
    --num_epochs 10 \
    --batch_size 8 \
    --learning_rate 1e-4 \
    --gradient_accumulation_steps 2 \
    --enable_detailed_eval
```

### Patch-Only Training (256 tokens):
```bash
python train_blip3o_enhanced.py \
    --chunked_embeddings_dir "./embeddings/patch_only_256" \
    --output_dir "./checkpoints/patch_only" \
    --training_mode "patch_only" \
    --num_epochs 10 \
    --batch_size 16 \
    --learning_rate 2e-4 \
    --gradient_accumulation_steps 4 \
    --enable_same_data_eval
```

### Overfitting Test (Single Shard):
```bash
python train_blip3o_enhanced.py \
    --max_training_shards 1 \
    --overfitting_test \
    --same_data_eval_frequency 50
```

## 📊 Evaluation & Analysis

```bash
python eval_blip3o_patch_similarity.py \
    --model_path "./checkpoints/cls_patch" \
    --embeddings_dir "./embeddings/cls_patch_257" \
    --output_dir "./results" \
    --num_samples 5000 \
    --save_plots \
    --compute_recall
```

## ⚙️ Model Configuration

### BLIP3oDiTConfig Parameters:
```python
config = {
    "hidden_size": 768,           # Model dimension
    "num_hidden_layers": 12,      # Transformer layers
    "num_attention_heads": 12,    # Attention heads
    "eva_embedding_size": 4096,   # EVA feature dimension
    "clip_embedding_size": 1024,  # CLIP feature dimension
    "num_tokens": 257,            # 256 or 257 tokens
    "prediction_type": "velocity" # Flow matching target
}
```

### Flow Matching Loss:
```python
loss_fn = BLIP3oFlowMatchingLoss(
    sigma_min=1e-4,               # Minimum noise level
    sigma_max=1.0,                # Maximum noise level
    normalize_targets=True,        # Normalize CLIP targets
    prediction_type="velocity"     # Velocity prediction
)
```

## 📁 Project Structure
```
blip3o-enhanced/
├── src/
│   ├── models/
│   │   └── blip3o_patch_dit.py           # DiT model implementation
│   ├── losses/
│   │   └── blip3o_flow_matching_loss.py  # Pure flow matching loss
│   ├── datasets/
│   │   └── blip3o_dataset.py             # Token-mode aware dataloader
│   ├── trainers/
│   │   └── blip3o_flexible_trainer.py    # Enhanced training loop
│   └── extraction/
│       └── extract_embeddings_g.py       # Embedding extraction
├── scripts/
│   ├── train_blip3o_enhanced.py          # Main training script
│   └── eval_patch_similarity.py          # Evaluation script
├── configs/
│   └── blip3o_config.py                  # Model configurations
└── README.md
```

## 📄 License
This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## 🔗 Related Work
- [BLIP3-o Paper](https://arxiv.org/abs/your-paper-id)
- [EVA-CLIP](https://github.com/baaivision/EVA)
- [Flow Matching](https://arxiv.org/abs/2210.02747)
```

